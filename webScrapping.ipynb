{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import math\n",
    "from dateutil import parser\n",
    "import html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait,Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException,NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanText(text):\n",
    "    infos = [item.replace(\"\\n\",\" \").strip() for item in text]\n",
    "    infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "    infos = \" \".join([item for item in infos if item != ''])\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapCorpus(sources,keyword,nb_docs):\n",
    "\n",
    "    source = \"\"\n",
    "    position = \"\"\n",
    "    company = \"\"\n",
    "    workplace = \"\"\n",
    "    published_date = \"\"\n",
    "    contract_type = \"\"\n",
    "    long_infos = \"\"\n",
    "\n",
    "\n",
    "    if  \"emploi-territorial\" in sources:\n",
    "        pages = math.ceil(nb_docs/20)\n",
    "        source = \"emploi-territorial\"\n",
    "        rootLink = \"https://www.emploi-territorial.fr\"\n",
    "        # 20 offres par pages\n",
    "        # pages = math.ceil(nb_docs/20)\n",
    "        url = f\"{rootLink}/emploi-mobilite/?adv-search={keyword}&page={pages}\"\n",
    "        response = requests.get(url) \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        root = soup.find(\"body\")\n",
    "        offresLinkElems = root.select(\"div[class*='bloc-lien-offre'] > a[class*='lien-details-offre']\")[:nb_docs]    \n",
    "        links = [rootLink+offresLinkElem.get(\"href\") for offresLinkElem in offresLinkElems]\n",
    "\n",
    "        corpus = list(dict())\n",
    "\n",
    "        for link in links:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            root = soup.find(\"body\")\n",
    "\n",
    "            position = root.select(\"h2[class*='set-line-emploi']\")[0].text.strip()\n",
    "            company = root.select(\"div[class*='offre-item-value'] > strong > a\")[0].text.strip() if root.select(\"div[class*='offre-item-value'] > strong > a\") else \"NULL\"\n",
    "            workplace = root.select_one(\"div[class*='offre-item-label']:contains('Lieu de travail') + .offre-item-value\").text.strip() if root.select_one(\"div[class*='offre-item-label']:contains('Lieu de travail') + .offre-item-value\") else \"NULL\"\n",
    "            published_date = root.select_one(\"div[class*='px-3']:contains('Publiée le') > .set-color-green\").text.strip() if root.select_one(\"div[class*='px-3']:contains('Publiée le') > .set-color-green\") else \"NULL\"\n",
    "            contract_type = root.select_one('div[class*=\"offre-item-label\"]:contains(\"Type d\\'emploi\") + .offre-item-value').text.strip() if root.select_one('div[class*=\"offre-item-label\"]:contains(\"Type d\\'emploi\") + .offre-item-value') else \"NULL\"\n",
    "            position_type = root.select_one('div[class*=\"offre-item-label\"]:contains(\"Famille de métiers\") + .offre-item-value').text.split(\">\")[0].strip()\n",
    "            \n",
    "            long_infos = root.select('div[class*=\"offre-item-text\"]')\n",
    "\n",
    "            long_infos = getCleanText([item.text for item in long_infos])\n",
    "            long_infos = getOccurences(long_infos)\n",
    "            current_offer = {\"source\": source, \"link\": rootLink, \"position\": position, \"position_type\": position_type,\n",
    "                \"company\": company, \"workplace\": workplace, \"published_date\": published_date,\n",
    "                \"contract_type\": contract_type, \"description\": long_infos}\n",
    "\n",
    "            # Check if the offer is already in the corpus based on a frozenset comparison\n",
    "            corpus.append(current_offer)\n",
    "    \n",
    "    if \"apec\" in sources:\n",
    "        pages = math.ceil(nb_docs/20)\n",
    "        source = \"apec\"\n",
    "        service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        rootLink = \"https://www.apec.fr\"\n",
    "        try:\n",
    "            for page in range(pages):    \n",
    "                try:\n",
    "                    driver.get(f'{rootLink}/candidat/recherche-emploi.html/emploi?motsCles={keyword}&page={pages}')\n",
    "                    # Initial find of elements\n",
    "                    try:\n",
    "                        deny_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#onetrust-reject-all-handler'))\n",
    "                        )\n",
    "\n",
    "                        deny_cookies_button.click()\n",
    "                    except TimeoutException:\n",
    "                        pass\n",
    "\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.card-offer\"))\n",
    "                    )\n",
    "\n",
    "                    # Getting docs left if last page to query\n",
    "                    if nb_docs%20 != 0 and page==pages-1: \n",
    "                        limit = nb_docs%20\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, 'div.card-offer')[:limit]\n",
    "                    else :\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, 'div.card-offer')\n",
    "                        \n",
    "                    for index in range(len(div_elements_to_click_list)):\n",
    "                        try:\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.card-offer\"))\n",
    "                            )\n",
    "                            # Re-find elements after navigating back\n",
    "                            div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, 'div.card-offer')\n",
    "\n",
    "                            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.card-offer\"))\n",
    "                            )\n",
    "                            # Check if the index is within the valid range\n",
    "\n",
    "                            div_elements_to_click = div_elements_to_click_list[index]\n",
    "                            company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                            contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                            workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                            published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                            # Scroll into view\n",
    "\n",
    "                            actions = ActionChains(driver)\n",
    "                            actions.move_to_element(div_elements_to_click).click().perform()\n",
    "\n",
    "                            # Wait for the child element to become present in the DOM\n",
    "                            WebDriverWait(driver, 20).until(\n",
    "                                EC.presence_of_element_located((By.XPATH, f\"//h4[text()='Descriptif du poste']\"))\n",
    "                            )\n",
    "                            # Get the page source after the click\n",
    "                            page_source = driver.page_source\n",
    "\n",
    "                            # Use Beautiful Soup to parse the page source\n",
    "                            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                            # Example: Retrieve the text of a specific element\n",
    "                            desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                            descText = [\" \".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                            profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                            profileText = [\" \".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                            position = soup.select_one(\"h4:contains('Métier') + span\").text\n",
    "                            position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                            long_infos = \" \".join([descText,profileText])\n",
    "                            long_infos = getCleanText(long_infos)\n",
    "                            long_infos = getOccurences(long_infos)\n",
    "                            corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":position_type,\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "                        except Exception as e:\n",
    "                            # Print the exception for debugging purposes\n",
    "                            print(f\"Error: {e}\")\n",
    "\n",
    "                        finally:\n",
    "                            # Navigate back to the main page\n",
    "                            driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    if \"hellowork\" in sources:\n",
    "        pages = math.ceil(nb_docs/30)\n",
    "        source = \"hellowork\"\n",
    "        rootLink = \"https://www.hellowork.com\"\n",
    "        service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "        driver = webdriver.Chrome(service=service) \n",
    "        try:\n",
    "            for page in range(pages):    \n",
    "                try:\n",
    "                    print(page)\n",
    "                    driver.get(f'{rootLink}/fr-fr/emploi/recherche.html?k={keyword}&k_autocomplete=&l=France&l_autocomplete=&p={page+1}')\n",
    "                    # Initial find of elements\n",
    "                    try:\n",
    "                        accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#hw-cc-notice-accept-btn'))\n",
    "                        )\n",
    "\n",
    "                        accept_cookies_button.click()\n",
    "\n",
    "                        combobox = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, \"select[name='country']\"))\n",
    "                        )\n",
    "\n",
    "                        select = Select(combobox)\n",
    "                        # Select a specific item by visible text\n",
    "                        select.select_by_value(\"FR\")\n",
    "\n",
    "                        form = driver.find_element(By.CSS_SELECTOR,\"form[data-action*='service-adaptation']\")\n",
    "                        next_button = form.find_element(By.CSS_SELECTOR,\"button\")\n",
    "                        next_button.click()\n",
    "                    except TimeoutException:\n",
    "                        pass\n",
    "\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"button[data-cy='seeOffer']\"))\n",
    "                    )\n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    # Getting docs left if last page to query\n",
    "                    if nb_docs%20 != 0 and page==pages-1: \n",
    "                        limit = nb_docs%20\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, \"button[data-cy='seeOffer']\")[:limit]\n",
    "                    else :\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, \"button[data-cy='seeOffer']\")\n",
    "                        \n",
    "                    for index in range(len(div_elements_to_click_list)):\n",
    "                        try:\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"button[data-cy='seeOffer']\"))\n",
    "                            )\n",
    "\n",
    "                            # Re-find elements after navigating back\n",
    "                            div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, \"button[data-cy='seeOffer']\")\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"button[data-cy='seeOffer']\"))\n",
    "                            )\n",
    "                            # Check if the index is within the valid range\n",
    "\n",
    "                            div_elements_to_click = div_elements_to_click_list[index]\n",
    "                            # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "                            # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                            # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                            # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                            # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                            # Scroll into view\n",
    "\n",
    "                            # actions = ActionChains(driver)\n",
    "                            # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "                            #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "                            div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "                            company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "                            company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "                            workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "                            published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "                            driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "                            # Wait for the child element to become present in the DOM\n",
    "                            WebDriverWait(driver, 20).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"span[data-cy='jobTitle']\"))\n",
    "                            )\n",
    "                            # Get the page source after the click\n",
    "                            page_source = driver.page_source\n",
    "\n",
    "                            # Use Beautiful Soup to parse the page source\n",
    "                            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                            # Example: Retrieve the text of a specific element\n",
    "                            # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                            # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                            # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                            # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                            position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "                            # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                            long_infos = soup.select(\"section\")\n",
    "                            long_infos = getCleanText([item.text for item in long_infos])\n",
    "                            long_infos = getOccurences(long_infos)\n",
    "                            # infos = [item for item in infos if '' not in item]\n",
    "                            corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "                        except Exception as e:\n",
    "                            # Print the exception for debugging purposes\n",
    "                            print(f\"Error: {e}\")\n",
    "\n",
    "                        finally:\n",
    "                            # Navigate back to the main page\n",
    "                            driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    " \n",
    "    if \"welcometothejungle\" in sources:\n",
    "        source = \"welcometothejungle\"\n",
    "        rootLink = \"https://www.welcometothejungle.com\"\n",
    "        service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        try:\n",
    "            for page in range(1,pages+1):    \n",
    "                try:\n",
    "                    driver.get(f'{rootLink}/fr/jobs?query=data&aroundQuery=France&page={page}')\n",
    "                    # Initial find of elements\n",
    "                    try:\n",
    "                        accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#axeptio_btn_acceptAll'))\n",
    "                        )\n",
    "\n",
    "                        accept_cookies_button.click()\n",
    "                    except TimeoutException:\n",
    "                        pass\n",
    "                        \n",
    "                    offers = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "                    )\n",
    "                    \n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    # Getting docs left if last page to query\n",
    "                    if nb_docs%30 != 0 and page==pages: \n",
    "                        limit = nb_docs%30\n",
    "                        div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")[:limit]\n",
    "                    else :\n",
    "                        div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "\n",
    "                    for index in range(len(div_elements_to_click_list)):\n",
    "                        try:\n",
    "                            offers = WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "                            )\n",
    "                            \n",
    "                            div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "                            div_elements_to_click = div_elements_to_click_list[index]\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "                            # Check if the index is within the valid range\n",
    "                            div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "                            position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "                            position = position_elem.text\n",
    "                            workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "                            contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "                            main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "                            contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "                            published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "                            published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "                            published_date = published_date.strftime('%d/%m/%Y')\n",
    "                            # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "                            # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                            # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                            # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                            # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                            # Scroll into view\n",
    "                            # actions = ActionChains(driver)\n",
    "                            # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "                            #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "                            # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "                            # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "                            # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "                            # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "                            # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "                            driver.execute_script(\"arguments[0].click();\", div_offer)\n",
    "                            WebDriverWait(driver, 20).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-testid='job-section-description']\"))\n",
    "                            )\n",
    "                            description_elem = driver.find_element(By.CSS_SELECTOR,\"div[data-testid='job-section-description']\")\n",
    "\n",
    "                            # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                            \n",
    "                            WebDriverWait(driver, 3)\n",
    "\n",
    "                            try:\n",
    "                                profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "                                profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                                profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "                            except NoSuchElementException:\n",
    "                                # If profile element is not found, set it to \"NULL\"\n",
    "                                profile = \"NULL\"\n",
    "                            # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "                            description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                            # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                            \n",
    "                            #parsing tags\n",
    "                            description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "                            company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "                            company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "                            long_infos = \"\".join([description,profile]) if profile != \"NULL\" else description\n",
    "                            long_infos = getCleanText([item for item in long_infos.split()])\n",
    "                            long_infos = getOccurences(long_infos)\n",
    "\n",
    "                            # print(long_infos)\n",
    "                            # Wait for the child element to become present in the DOM\n",
    "\n",
    "                            # # Get the page source after the click\n",
    "                            # page_source = driver.page_source\n",
    "\n",
    "                            # Use Beautiful Soup to parse the page source\n",
    "                            # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                            # Example: Retrieve the text of a specific element\n",
    "                            # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                            # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                            # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                            # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                            # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "                            # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                            # long_infos = soup.select(\"section\")\n",
    "                            # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "                            # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "                            # infos = \" \".join([item for item in infos if item != ''])\n",
    "                            # infos = [item for item in infos if '' not in item]\n",
    "                            corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "                        except Exception as e:\n",
    "                            # Print the exception for debugging purposes\n",
    "                            print(f\"Error: {e}\")\n",
    "\n",
    "                        finally:\n",
    "                            # Navigate back to the main page\n",
    "                            driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    return(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "corpus = scrapCorpus(sources=[\"emploi-territorial\",\"hellowork\",\"welcometothejungle\"],keyword=\"data\",nb_docs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item[\"description\"] for item in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "def getOccurences(input_text):\n",
    "    french_stop_words = list(spacy.lang.fr.stop_words.STOP_WORDS)\n",
    "    # Create CountVectorizer instance\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=french_stop_words,  # Remove English stop words\n",
    "        token_pattern=r'\\b[^\\d\\W]+\\b'  # Remove tokens with only digits\n",
    "    )\n",
    "\n",
    "    # Tokenize and transform the input text\n",
    "    X = vectorizer.fit_transform([input_text])\n",
    "\n",
    "    # Get feature names (lemmatized tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Convert the sparse matrix to a dense array\n",
    "    token_occurrences = dict(zip(feature_names, map(int, X.toarray()[0])))\n",
    "\n",
    "    # Convert token occurrences to a dictionary\n",
    "    result_dict = {token: count for token, count in token_occurrences.items()}\n",
    "\n",
    "    # Convert the dictionary to a stringified JSON\n",
    "    json_string = json.dumps(result_dict,separators=(',', ':'),ensure_ascii=False)\n",
    "    \n",
    "    return json_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': 'Ingénieur data F/H',\n",
       "  'position_type': \"Informatique et système d'information\",\n",
       "  'company': 'CONSEIL DEPARTEMENTAL DU NORD',\n",
       "  'workplace': 'Lille cedex',\n",
       "  'published_date': '19/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Le Département du Nord place les systèmes d'information au cœur du projet de transformation digitale au service de l'usager et de ses partenaires. La Direction des Systèmes d'Information (environ 110 collaborateurs, 7000 postes de travail, 200 sites) poursuit le développement de projets innovants :\\n-\\tLa sécurisation de ses infrastructures, le développement du haut débit pour ses établissements, des plateformes téléphoniques, des systèmes de GED et de numérisation.\\n-\\tDe nombreux chantiers autour du poste de travail comme la mobilité en autres\\n-\\tLa mise en œuvre d'applications au service des compétences départementales, la solidarité, la mobilité, la jeunesse, le sport, la culture, le développement des territoires.\\n\\nAu sein de la DSI, vous êtes rattaché-e au service Pilotage et Sécurisation SI, qui anime les Consultants Internes SI, les ingénieur-es Sécurité et une cellule Data (Données SI). La cellule DATA initie la structuration de la donnée dans ses missions de définition et de pilotage d'un écosystème Data de la collectivité.\\nLes missions de la cellule Data (Données SI) :\\n*\\tSTRUCTURER : définir et piloter la mise en œuvre de l'écosystème Data de la collectivité, autour de la collecte, du stockage, du référencement, de la qualité, de la visualisation et la diffusion des données.\\n*\\tACCOMPAGNER l'usage : aider à faire parler les données et être un support aux équipes de pilotage des directions métiers. \\n*\\tCAPITALISER notre patrimoine de données : œuvrer au référencement et à la collecte des données à des fins de réemploi ; en premier lieu pour l'analyse des données (dont décisionnel) puis pour la circulation des données entre les systèmes.\\n\\nEn qualité d'ingénieur-e data, vous êtes le/la référent-e technique de la collectivité pour les projets d'intégration et de stockage des données et vous apportez l'expertise technique nécessaire pour le développement des solutions data appropriées. Pour ce faire, vous contribuez activement à :\\n*\\tDéfinir l'architecture décisionnelle cible et les besoins d'infrastructures nécessaires au stockage et à la valorisation des données.\\n*\\tConcevoir les projets d'industrialisation de traitement des données et superviser les équipes opérationnelles dans les projets d'intégration (process ETL, entrepôts de données...). \\n*\\tParticiper aux chantiers data structurants (qualité, référentiels et cartographie des données, urbanisation...).\\n*\\tGarantir la cohérence et la sécurité de l'écosystème décisionnel départemental.\\n*\\tAccompagner les directions métiers dans les projets décisionnels (recueil des besoins, modélisation des données...) et fournir aux équipes (data analystes) un appui technique à l'exploration complexe des données. \\nRelations professionnelles : \\nInterne : Relations directes avec l'équipe de direction et les services de la DSI, les secrétariats généraux et les autres directions de la collectivité (responsables et collaborateurs des dites directions).\\nExterne : Prestataires, éditeurs, autres collectivités, ministères. Savoir faire \\n Assistance à la maîtrise d'ouvrage opérationnelle \\n*\\tParticiper à la conduite du changement \\n*\\tDéfinir des spécifications fonctionnelles à partir de l'expression des besoins \\n\\nPilotage et conduite de projet d'informatisation \\n*\\tÉvaluer les enjeux et les risques (techniques, financiers, organisationnels) d'un projet informatique \\n*\\tÉlaborer le cahier des charges et le calendrier de réalisation \\n*\\tOpérer des choix techniques en matière de logiciels \\n*\\tOrganiser le déroulement du projet et planifier les travaux de développement \\n\\nMaintien en conditions opérationnelles des applications et plateformes (MCO) \\n*\\tAssurer l'assistance de niveau 3 (expertise, problèmes complexes, etc.) \\n*\\tAssurer la maintenance corrective \\n*\\tAssurer la maintenance évolutive et la gestion des changements (au sens ITIL)\\n\\nConception et intégration d'applications \\n*\\tRéaliser les spécifications fonctionnelles et techniques \\n*\\tMettre en œuvre des progiciels (paramétrage, reprise de données, interfaces, développements spécifiques, etc.) \\n*\\tRéaliser des tests des programmes et des prototypes \\n*\\tRédiger la documentation (guides, modes opératoires, etc.) \\n\\nAssistance et appui technique auprès des services de la collectivité \\n*\\tSensibiliser les services et diffuser des supports d'information \\n*\\tConduire une action de formation en interne \\n\\nVeille et observation sectorielle \\n*\\tEnrichir des bases documentaires et d'information \\n\\nGestion de la commande publique \\n*\\tÉlaborer les cahiers des charges et pièces du marché public * Définir des critères de sélection des offres \\n*\\tPréparer les dossiers des commissions d'appels d'offres \\n*\\tAnalyser les propositions techniques et financières des fournisseurs et entreprises \\n*\\tNégocier avec les fournisseurs et les entreprises \\n*\\tAttester le service fait \\n\\nElaboration et suivi du budget \\n*\\tPlanifier les besoins budgétaires et élaborer un budget prévisionnel \\n*\\tSuivre et contrôler l'exécution du budget \\n*\\tRenseigner des outils de pilotage et de suivi (tableaux de bord) \\n\\nContrôle de la qualité des services rendus \\n*\\tVérifier la conformité des prestations des entreprises avec les clauses techniques définies dans les pièces du marché \\n*\\tÉtablir des rapports et bilans d'activités \\n\\nContrôle et suivi des prestations effectuées par des tiers \\n*\\tRéceptionner et contrôler les projets, travaux et prestations fournis par des tiers \\n\\nConduite de projet \\n*\\tOrganiser et animer des groupes projet et des comités de pilotage \\n*\\tIdentifier et mobiliser les acteurs et les compétences nécessaires à la conduite d'un projet Savoirs  \\n*\\t Méthodes et outils de la planification \\n*\\tArchitecture et fonctionnalités des SI \\n*\\tRègles et aspects légaux des SI \\n*\\tTechniques de conception, modélisation et architecture d'applications \\n*\\tMarché de l'offre informatique \\n*\\tNormes et procédures de sécurité \\nFonctionnement de la collectivité et des services \\n*\\tTechniques d'élaboration de cahier des charges et de planification d'études  \\n*\\tTechniques d'analyses comparatives (benchmarking) \\n*\\tCode des marchés publics et modalités d'application \\n*\\tRègles et techniques d'expression écrite et rédactionnelles (notes, compte-rendu, rapports, etc.) \\n*\\tRègles relatives à l'accès aux documents administratifs \\n*\\tTechniques rédactionnelles de rapports et bilans d'activité \\n*\\tMéthodes et techniques de réception des travaux et prestations \\n\\n   Savoir - être \\n*\\tDisposer d'une bonne aisance relationnelle \\n*\\tDisposer de bonnes capacités d'analyse et de synthèse \\n*\\tSavoir travailler en transversalité \\n*\\tFaire preuve d'esprit d'équipe et de capacité à coopérer  \\n*\\tFaire preuve d'autonomie \\n*\\tAvoir de bonnes capacités d'écoute  \\n\\n Obligations du poste : \\n- Formation supérieure, idéalement bac+5 en informatique ou école d'ingénieurs, \\n- Une ou plusieurs expériences dans le développement de projets d'intégration de données\\n\\nConditions particulières : \\n - Déplacements possibles sur les sites \\n-  Fonction Télétravaillable\"},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': 'Ingénieur DATA H/F',\n",
       "  'position_type': 'Pilotage',\n",
       "  'company': 'CONSEIL DEPARTEMENTAL DE HAUTE-SAVOIE',\n",
       "  'workplace': 'Annecy',\n",
       "  'published_date': '15/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': 'Sous l\\'autorité du responsable de l\\'Unité Urbanisation et Intelligence de la donnée, l\\'Ingénieur data assure la disponibilité des différentes briques DATA de notre système d\\'information. En étroite collaboration avec les différents services de la collectivité, vous contribuez à la collecte, nettoyage et organisation des données provenant de différentes sources. Vous utilisez des outils et des techniques d\\'analyse de données pour extraire des informations pertinentes, identifier des tendances et des modèles, et effectuer des analyses approfondies. L\\'Ingénieur data travaille en étroite collaboration avec les équipes métier et les autres services de la DSI pour comprendre leurs besoins et proposer des solutions basées sur les données. Il surveille également les performances des modèles et des analyses, en effectuant des ajustements et des améliorations régulières.\\n\\nL\\'objectif global de l\\'Ingénieur data sera de s\\'assurer en priorité de la disponibilité des infrastructure et moteur de base de données, puis ensuite d\\'exploiter les données pour générer des connaissances et des informations exploitables, afin de soutenir la prise de décisions éclairées et d\\'améliorer la performance globale de la collectivité. Construire et faire évoluer le socle DATA\\nParticiper activement, en tant qu\\'expert, à la construction des entrepôts de donnée du CD74 qui contiendra une plateforme d\\'intégration, des conteneurs de données, un moteur d\\'indexation et de recherche, un bus de message, un gestionnaire de workflow, un API manager et des ETL (Talend) ;\\nParticiper activement à la modélisation de l\\'architecture Data global du CD74 (modélisation des flux et des ressources techniques associées) visant à rendre interopérables des volumes importants et hétérogènes de données de manière sécurisée ;\\nMaintenir en condition opérationnelle ces mêmes briques ;\\nFaire évoluer ce socle d\\'application orienté DATA.\\nGarantir la disponibilité des données du SI\\nAdministrer les bases de données du SI (Oracle, PostgreSQL, MySQL) ;\\nSauvegarder, restaurer ;\\nSuivre le versioning et le licensing des moteurs de bases de données ;\\nÊtre force de proposition d\\'axes d\\'amélioration (processus et technique) visant à professionnaliser/industrialiser la gestion des données du SI.\\nTransformer la performance en excellence, en contribuant aux projets d\\'efficience\\nComprendre les besoins métiers en s\\'appuyant sur les équipes et utilisateurs clés ;\\nParticiper à la réalisation des cahiers des charges ;\\nÊtre le point de contact privilégié des interlocuteurs métiers sur les sujets data ;\\nMener les tests en interactions avec les équipes de la DSI et les utilisateurs clés ;\\nTenir le bon niveau de discours apte à rendre les sujets Data compréhensibles par les différentes populations d\\'utilisateurs clés.\\nMesurer la performance, avec un état d\\'esprit \" data \" avec les compétences et connaissances appréciés suivantes\\nMaîtrise des solutions de bases de données (SQL, NoSQL...) ;\\nConnaissances en langages structurés (Javascript, Java, Python...) ;\\nAppétence pour l\\'IA et capacité à construire ou participer à construire des modèles de machine learning, Deep Learning et l\\'analyse de texte ;\\nForte expertise le stockage de données et les outils ETL tels que Talend, Business Objects ;\\nConnaissance en technologies du Big Data permettant le traitement et la manipulation de données (Hadoop, Spark, Kafka...) ;\\nConnaissance en Master Data Management, DataWarehouses et Data Lakes. Maitrise des moteurs de bases de données\\nUne bonne connaissance d\\'Oracle, de PostgreSQL et du langage SQL (requête et Scripting)\\nRédaction de la documentation sur les architectures déployées et les procédures techniques associées pour le MCO\\nCapacité à résoudre les problèmes techniques et à diagnostiquer les pannes liées aux bases de données\\nCapacité à documenter les procédures et les meilleures pratiques\\nNous recherchons une personne ayant le sens du travail en équipe, le sens du service et de l\\'organisation, et le sens du respect des procédures.\\n\\nVous êtes curieux(se), autonome et rigoureux(se) ? Vous savez relever les incohérences et faites attention au détail ?Vous avez su faire preuve d\\'organisation et de fiabilité au cours de vos précédentes expériences ? Vous êtes motivé(e) pour apprendre et acquérir de nouvelles compétences ? Alors ce poste est fait pour vous et le Département attend votre candidature avant le 15/01/2024'},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': 'Chef(fe) de projet DATA',\n",
       "  'position_type': \"Informatique et système d'information\",\n",
       "  'company': \"Conseil Départemental d'Eure-et-Loir\",\n",
       "  'workplace': 'Chartres cedex',\n",
       "  'published_date': '19/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Rémunération statutaire, régime indemnitaire, collectivité affiliée au cnas, tickets restaurant. Le Conseil départemental, c'est un collectif d'environ 2 000 collaborateurs qui œuvrent quotidiennement au service des Euréliens.\\n\\nLa Direction du numérique rassemble une quarantaine de collaborateurs et a pour objectif d'impulser et d'accompagner les changements professionnels, organisationnels et managériaux nécessaires à la transition numérique de la collectivité et du territoire.\\n\\nDans ce cadre, vous serez rattaché(e) au Service ingénierie des projets et encadrez l'ensemble  des projets de Big data, Data Intelligence et/ou Data Gouvernance.\\n\\nEn tant que responsable de tous les projets innovants dans le domaine de la donnée et de l'intelligence artificielle et de leur bonne réalisation, vous jouerez un rôle d'interface entre les toutes les Directions métiers mais aussi de toutes les parties prenantes du projet. En tant que Chef de projet DATA, vos principales missions seront de :\\n\\n    Déployer la stratégie de la Collectivité en matière d'analyse et de traitement de données ;\\n    Assurer la cohérence des actions des différents intervenants dans le cadre des plans d'actions liés à la gestion de la donnée ;\\n    Veiller à la conformité et à la bonne organisation des données dans les systèmes d'information, piloter et accompagner le déploiement de la solution auprès des utilisateurs ;\\n     Assurer une veille technologique ;\\n    Apporter des conseils au sujet de la restitution de données,\\n    Participer activement à l'activité Opendata\\n    Définir les rôles et responsabilités autour de la donnée IT et des référentiels attenants\\n    Mettre en place un contrôle qualité et une gouvernance sur ces données\\n    Valoriser ces données, faire en sorte qu'elles soient compréhensibles et exploitables\\n    Faciliter l'accès à ces données en respectant les exigences et standards de sécurité et conformité.\\n    Assurer le pilotage, la coordination et le suivi de projets numériques (état des lieux, définition des objectifs et suivi des délais, plan d'actions, coordination des acteurs, évaluation...),\\n    Contribuer à la traduction technique des besoins fonctionnels (analyse fonctionnelle, identification des opportunités et de la faisabilité technologique, évaluation des risques, sécurité du SI)\\n    Assurer une mission de conception et de développement (architecture logicielle, réalisation d'applications, cartographie de flux, structuration et documentation des bases de données)\"},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': \"UN(E) CHEF(FE) DE PROJET RESPONSABLE D'APPLICATIONS SI DATA ET DOCUMENTATION\",\n",
       "  'position_type': 'Mobilité, déplacements et transports',\n",
       "  'company': 'SYNDICAT MIXTE (SM) SYTRAL',\n",
       "  'workplace': 'LYON CEDEX 03',\n",
       "  'published_date': '15/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Au sein de la Direction des Expertises Techniques et du Patrimoine et du service systèmes transverses\\nPlacé(e) sous la responsabilité hiérarchique de la responsable du pôle informatique\\n\\nMISSIONS\\n* Piloter les activités de mise en œuvre, de maintenance et d'évolution des systèmes de collecte et d'exploitation des données des réseaux de transport en réponse aux besoins fonctionnels identifiés.\\n* Chargé(e) du bon fonctionnement quotidien des outils mis à la disposition des utilisateurs SYTRAL Mobilités, opérateurs et partenaires, ainsi que du suivi des usages de la plateforme Cloud hébergeant les données.\\n* Être le/la référent(e) technique sur les projets data et documentation en lien avec les métiers. 1- Mise en œuvre des solutions Data et Documentation,\\n- Piloter la mise en œuvre des projets sur son périmètre : cadrage des besoins, chiffrage, rédaction des cahiers des charges, pilotage des prestations, suivi des risques, validation des livrables et recettes en lien avec les acteurs métier,\\n- Veiller au respect des règles d'urbanisation et de sécurité SI sur les solutions mises en place,\\n- Garantir la qualité de la documentation des solutions et assurer la mise à jour de la cartographie des données et des flux d'alimentation sur son périmètre,\\n- Garantir le respect du budget des solutions Cloud par une surveillance régulière des consommations et le suivi d'alertes.\\n2- Fonctionnement quotidien des outils mis à disposition,\\n- Assurer la gestion des contrats de maintenance des applications du périmètre\\n- Être l'interface avec les éditeurs, intégrateurs ou équipe en charge d'une tierce maintenance applicative (TMA),\\n- Gérer les authentifications et droits sur les données et sur les infrastructures de la plateforme data (création de ressources, stockages, machines...),\\n- Gérer l'administration fonctionnelle et la maintenance évolutive des outils de restitution (PowerBi, BO etc...),\\n- Suivre les anomalies et prendre en charge des adaptations sur les applications du périmètre,\\n- Animer un plan d'amélioration continue et identifier des pistes d'optimisation,\\n- Suivre la qualité de service,\\n- Rédiger des procédures et animer les communautés d'utilisateurs.\\n\\n3- Contact technique sur les projets Data en lien avec les métiers,\\n- Contribuer aux études sur les nouvelles données à collecter, en lien avec les opérateurs et partenaires,\\n- Réaliser les analyses d'impacts (faisabilité, coûts, délais...) sur les demandes relatives à des fonctionnalités ou à de nouveaux algorithmes de calculs, en s'appuyant au besoin sur des expertises techniques spécifiques data/IA. Les savoirs\\n\\tIngénieur, Bac + 5 avec une expérience dans les domaines de la conduite de projets informatiques ou en tant que responsable d'applications, \\n\\tExpertise technico-fonctionnelle de solutions de Gestion Electronique Documentaire, de modélisation de workflow, de reporting et d'outils de manipulation des données (ETL),\\n\\tConnaissances des systèmes de gestion de bases de données, modèles relationnels ou objets et formats de stockages,\\n\\tConnaissances du fonctionnement des clouds publics,\\n\\tConnaissances en matière de marchés publics appréciées.\\n\\nLes savoir-être\\n\\tPragmatisme,\\n\\tRigueur,\\n\\tEsprit d'analyse et de synthèse,\\n\\tAisance relationnelle, facultés de communication, \\n\\tSens du service client.\\n\\nCONDITIONS DE TRAVAIL\\n\\n- Lieu : 21 Boulevard Vivier Merle 69003 LYON - à proximité de la gare Part-Dieu,\\n- Temps de travail : 35h (7h/j), 37h30 (7h30/j + 15 jours de RTT) ou 38h45 (7h45/j + 22 jours de RTT),\\n- Poste ouvert au télétravail jusqu'à 2 jours par semaine sans conditions d'ancienneté,\\n- Rémunération statutaire, RIFSEEP (IFSE et CIA), prime de fin d'année, tickets-restaurant, adhésion au Comité Social de la Métropole de Lyon (voyages, billetterie, culture, loisirs, aides...),\\n- Poste ouvert aux contractuels : il est possible de recruter sur ce poste une personne qui n'est pas lauréate d'un concours de la fonction publique.\"},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': \"Un(e) chef(fe) de projet responsable d'applications SI Data et documentation\",\n",
       "  'position_type': \"Informatique et système d'information\",\n",
       "  'company': 'SYNDICAT MIXTE (SM) SYTRAL',\n",
       "  'workplace': 'LYON CEDEX 03',\n",
       "  'published_date': '15/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Au sein de la Direction des Expertises Techniques et du Patrimoine et du service systèmes transverses\\nPlacé(e) sous la responsabilité hiérarchique de la responsable du pôle informatique\\n\\nMISSIONS\\n* Piloter les activités de mise en œuvre, de maintenance et d'évolution des systèmes de collecte et d'exploitation des données des réseaux de transport en réponse aux besoins fonctionnels identifiés.\\n* Chargé(e) du bon fonctionnement quotidien des outils mis à la disposition des utilisateurs SYTRAL Mobilités, opérateurs et partenaires, ainsi que du suivi des usages de la plateforme Cloud hébergeant les données.\\n* Être le/la référent(e) technique sur les projets data et documentation en lien avec les métiers. ACTIVITES PRINCIPALES\\n1- Mise en œuvre des solutions Data et Documentation,\\n- Piloter la mise en œuvre des projets sur son périmètre : cadrage des besoins, chiffrage, rédaction des cahiers des charges, pilotage des prestations, suivi des risques, validation des livrables et recettes en lien avec les acteurs métier,\\n- Veiller au respect des règles d'urbanisation et de sécurité SI sur les solutions mises en place,\\n- Garantir la qualité de la documentation des solutions et assurer la mise à jour de la cartographie des données et des flux d'alimentation sur son périmètre,\\n- Garantir le respect du budget des solutions Cloud par une surveillance régulière des consommations et le suivi d'alertes.\\n2- Fonctionnement quotidien des outils mis à disposition,\\n- Assurer la gestion des contrats de maintenance des applications du périmètre\\n- Être l'interface avec les éditeurs, intégrateurs ou équipe en charge d'une tierce maintenance applicative (TMA),\\n- Gérer les authentifications et droits sur les données et sur les infrastructures de la plateforme data (création de ressources, stockages, machines...),\\n- Gérer l'administration fonctionnelle et la maintenance évolutive des outils de restitution (PowerBi, BO etc...),\\n- Suivre les anomalies et prendre en charge des adaptations sur les applications du périmètre,\\n- Animer un plan d'amélioration continue et identifier des pistes d'optimisation,\\n- Suivre la qualité de service,\\n- Rédiger des procédures et animer les communautés d'utilisateurs.\\n3- Contact technique sur les projets Data en lien avec les métiers,\\n- Contribuer aux études sur les nouvelles données à collecter, en lien avec les opérateurs et partenaires,\\n- Réaliser les analyses d'impacts (faisabilité, coûts, délais...) sur les demandes relatives à des fonctionnalités ou à de nouveaux algorithmes de calculs, en s'appuyant au besoin sur des expertises techniques spécifiques data/IA. Les savoirs\\n\\tIngénieur, Bac + 5 avec une expérience dans les domaines de la conduite de projets informatiques ou en tant que responsable d'applications, \\n\\tExpertise technico-fonctionnelle de solutions de Gestion Electronique Documentaire, de modélisation de workflow, de reporting et d'outils de manipulation des données (ETL),\\n\\tConnaissances des systèmes de gestion de bases de données, modèles relationnels ou objets et formats de stockages,\\n\\tConnaissances du fonctionnement des clouds publics,\\n\\tConnaissances en matière de marchés publics appréciées.\\n\\nLes savoir-être\\n\\tPragmatisme,\\n\\tRigueur,\\n\\tEsprit d'analyse et de synthèse,\\n\\tAisance relationnelle, facultés de communication, \\n\\tSens du service client.\\nCONDITIONS DE TRAVAIL\\n\\n- Lieu : 21 Boulevard Vivier Merle 69003 LYON - à proximité de la gare Part-Dieu,\\n- Temps de travail : 35h (7h/j), 37h30 (7h30/j + 15 jours de RTT) ou 38h45 (7h45/j + 22 jours de RTT),\\n- Poste ouvert au télétravail jusqu'à 2 jours par semaine sans conditions d'ancienneté,\\n- Rémunération statutaire, RIFSEEP (IFSE et CIA), prime de fin d'année, tickets-restaurant, adhésion au Comité Social de la Métropole de Lyon (voyages, billetterie, culture, loisirs, aides...),\\n- Poste ouvert aux contractuels : il est possible de recruter sur ce poste une personne qui n'est pas lauréate d'un concours de la fonction publique.\"}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in corpus if item[\"source\"]==\"emploi-territorial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, Date, ForeignKey,text,String\n",
    "from sqlalchemy.orm import declarative_base, Session, relationship\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Modele de bdd\n",
    "class HPositionType(Base):\n",
    "    __tablename__ = 'h_position_type'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    position_type = Column(String)\n",
    "    d_positions = relationship('DPosition', back_populates='h_position_type')\n",
    "\n",
    "class DPosition(Base):\n",
    "    __tablename__ = 'd_position'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    position = Column(String)\n",
    "    position_type_id = Column(Integer, ForeignKey('h_position_type.id'))\n",
    "    h_position_type = relationship('HPositionType', back_populates='d_positions')\n",
    "\n",
    "class DWebsite(Base):\n",
    "    __tablename__ = 'd_website'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    label = Column(String)\n",
    "    link = Column(String)\n",
    "\n",
    "class DCompany(Base):\n",
    "    __tablename__ = 'd_company'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    label = Column(String)\n",
    "\n",
    "class DCity(Base):\n",
    "    __tablename__ = 'd_city'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    city = Column(String)\n",
    "\n",
    "class DContractType(Base):\n",
    "    __tablename__ = 'd_contract_type'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    contract_type = Column(String)\n",
    "\n",
    "class DCalendar(Base):\n",
    "    __tablename__ = 'd_calendar'\n",
    "    date = Column(Date, primary_key=True)\n",
    "    day = Column(Integer)\n",
    "    month = Column(Integer)\n",
    "    year = Column(Integer)\n",
    "\n",
    "\n",
    "class FJobAdvertisements(Base):\n",
    "    __tablename__ = 'f_job_advertisements'\n",
    "    nb_occurences = Column(String)\n",
    "    contract_type_id = Column(Integer, ForeignKey('d_contract_type.id'),primary_key=True)\n",
    "    position_id = Column(Integer, ForeignKey('d_position.id'),primary_key=True)\n",
    "    website_id = Column(Integer, ForeignKey('d_website.id'),primary_key=True)\n",
    "    city_id = Column(Integer, ForeignKey('d_city.id'),primary_key=True)\n",
    "    company_id = Column(Integer, ForeignKey('d_company.id'),primary_key=True)\n",
    "    published_date = Column(Date, ForeignKey('d_calendar.date'),primary_key=True)\n",
    "\n",
    "    position = relationship('DPosition', back_populates='job_advertisements')\n",
    "    website = relationship('DWebsite', back_populates='job_advertisements')\n",
    "    company = relationship('DCompany', back_populates='job_advertisements')\n",
    "    city = relationship('DCity', back_populates='job_advertisements')\n",
    "    contract_type = relationship('DContractType', back_populates='job_advertisements')\n",
    "\n",
    "# Définition des relations\n",
    "DPosition.job_advertisements = relationship('FJobAdvertisements', back_populates='position')\n",
    "DWebsite.job_advertisements = relationship('FJobAdvertisements', back_populates='website')\n",
    "DCompany.job_advertisements = relationship('FJobAdvertisements', back_populates='company')\n",
    "DCity.job_advertisements = relationship('FJobAdvertisements', back_populates='city')\n",
    "DContractType.job_advertisements = relationship('FJobAdvertisements', back_populates='contract_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "#Seulement à executer une fois (contrainte avec la table de fait)\n",
    "def insertCalendar(yearsRange):\n",
    "    engine = create_engine('mysql+mysqlconnector://root:@localhost/job_scrapping')\n",
    "    Base.metadata.create_all(engine)\n",
    "    session = Session(bind=engine)\n",
    "    session.commit()\n",
    "    for year in yearsRange:\n",
    "        for month in range(1, 13):\n",
    "            num_days_in_month = (datetime(year, month % 12 + 1, 1) - timedelta(days=1)).day\n",
    "            for day in range(1, num_days_in_month + 1):\n",
    "                date_details = datetime(year, month, day)\n",
    "                date = DCalendar(date=date_details.strftime('%Y-%m-%d'),day=date_details.day,month=date_details.month,year=date_details.year)\n",
    "                session.add(date)\n",
    "                session.commit()\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertCalendar(range(2015,2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alimentation du DW\n",
    "def fillDW(corpus):\n",
    "    engine = create_engine('mysql+mysqlconnector://root:@localhost/job_scrapping')\n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    duplicates = False\n",
    "    #Vidage du DW\n",
    "    session = Session()\n",
    "    session.execute(text(\"CALL pTruncateDW()\"))\n",
    "    session.commit()\n",
    "    i=0\n",
    "    #Itération sur les éléments du corpus récupérés\n",
    "    for item in corpus:\n",
    "        corpus_position = item[\"position\"]\n",
    "        corpus_website = item[\"source\"]\n",
    "        corpus_link = item[\"link\"]\n",
    "        corpus_company = item[\"company\"]\n",
    "        corpus_city = item[\"workplace\"]\n",
    "        corpus_contract_type = item[\"contract_type\"]\n",
    "        corpus_position_type = item['position_type']\n",
    "        corpus_published_date = item['published_date']\n",
    "        corpus_nb_occurence = item['description']\n",
    "        try:\n",
    "                date_object = datetime.strptime(corpus_published_date, '%d/%m/%Y')\n",
    "                # If successful, use the formatted date\n",
    "                corpus_published_date = date_object.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            corpus_published_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        # print(corpus_position)\n",
    "        # print(corpus_position_type)\n",
    "        \n",
    "        position_type = HPositionType(position_type=corpus_position_type)\n",
    "        position = DPosition(position=corpus_position)\n",
    "        website = DWebsite(label=corpus_website,link=corpus_link)\n",
    "        company = DCompany(label=corpus_company)\n",
    "        city = DCity(city=corpus_city)\n",
    "        contract_type = DContractType(contract_type=corpus_contract_type)\n",
    "        session = Session()\n",
    "\n",
    "        # Recherche des élements de chaque dimensions pour éviter les doublons\n",
    "        existing_position_type = session.query(HPositionType).filter_by(position_type=corpus_position_type).first()\n",
    "        existing_position = session.query(DPosition).filter_by(position=corpus_position).first()\n",
    "        existing_website = session.query(DWebsite).filter_by(label=corpus_website).first()\n",
    "        existing_company = session.query(DCompany).filter_by(label=corpus_company).first()\n",
    "        existing_city = session.query(DCity).filter_by(city=corpus_city).first()\n",
    "        existing_contract_type = session.query(DContractType).filter_by(contract_type=corpus_contract_type).first()\n",
    "\n",
    "        #Si elle existe je récupère la ligne existante, sinon j'insert la nouvelle ligne\n",
    "        if existing_position_type:\n",
    "            position_type = existing_position_type\n",
    "        else:\n",
    "            session.add(position_type)\n",
    "            session.commit()\n",
    "        if existing_position:\n",
    "            position = existing_position\n",
    "        else:\n",
    "            position.position_type_id = position_type.id\n",
    "            session.add(position)\n",
    "\n",
    "        if existing_website:\n",
    "            website = existing_website\n",
    "        else:\n",
    "            session.add(website)\n",
    "\n",
    "        if existing_company:\n",
    "            company = existing_company\n",
    "        else:\n",
    "            session.add(company)\n",
    "\n",
    "        if existing_city:\n",
    "            city = existing_city\n",
    "        else:\n",
    "            session.add(city)\n",
    "\n",
    "        if existing_contract_type:\n",
    "            contract_type = existing_contract_type\n",
    "        else:\n",
    "            session.add(contract_type)\n",
    "        session.commit()\n",
    "        print(corpus_nb_occurence)\n",
    "        # J'insert les données dans la table de fait (id des dimensions + KPI)\n",
    "        job_advertisement = FJobAdvertisements(\n",
    "            nb_occurences=corpus_nb_occurence,\n",
    "            position=position,\n",
    "            website=website,\n",
    "            company=company,\n",
    "            city=city,\n",
    "            contract_type=contract_type,\n",
    "            published_date = corpus_published_date\n",
    "        )\n",
    "\n",
    "        session.add(job_advertisement)\n",
    "        try:\n",
    "            session.add(job_advertisement)\n",
    "            session.commit()\n",
    "            i = i + 1\n",
    "        except IntegrityError as e:\n",
    "            duplicates = True\n",
    "            session.rollback()\n",
    "        session.commit()\n",
    "    session.close()\n",
    "    if duplicates:\n",
    "        print(f\"Some exact duplicates have been detected in the job scrapping process, less documents have been saved then asked : {i} documents saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"accompagner\":2,\"accès\":2,\"accéder\":1,\"acteurs\":1,\"action\":1,\"activement\":1,\"activité\":1,\"activités\":1,\"administratifs\":1,\"aider\":1,\"aisance\":1,\"analyse\":2,\"analyser\":1,\"analyses\":1,\"analystes\":1,\"anime\":1,\"animer\":1,\"appels\":1,\"application\":1,\"applications\":4,\"apply\":1,\"apportez\":1,\"appropriées\":1,\"appui\":2,\"architecture\":3,\"aspects\":1,\"assistance\":3,\"assurer\":3,\"attester\":1,\"auprès\":1,\"autonomie\":1,\"autour\":2,\"bac\":1,\"bases\":1,\"benchmarking\":1,\"besoins\":4,\"bien\":1,\"bilans\":2,\"bonne\":1,\"bonnes\":2,\"bord\":1,\"budget\":3,\"budgétaires\":1,\"bénéficiant\":1,\"cahier\":2,\"cahiers\":1,\"calendrier\":1,\"candidats\":2,\"candidature\":1,\"capacité\":1,\"capacités\":2,\"capitaliser\":1,\"cartographie\":1,\"cellule\":3,\"changement\":1,\"changements\":1,\"chantiers\":2,\"charges\":3,\"choix\":1,\"cible\":1,\"circulation\":1,\"clauses\":1,\"code\":2,\"cohérence\":1,\"collaborateurs\":2,\"collecte\":2,\"collectivité\":6,\"collectivités\":1,\"comités\":1,\"commande\":1,\"commissions\":1,\"comparatives\":1,\"complexe\":1,\"complexes\":1,\"compte\":1,\"compétences\":2,\"conception\":2,\"concevoir\":1,\"conditions\":3,\"conduire\":1,\"conduite\":4,\"conformité\":1,\"conformément\":1,\"consultants\":1,\"contractuelle\":1,\"contribuez\":1,\"contrôle\":2,\"contrôler\":2,\"coopérer\":1,\"corrective\":1,\"critères\":1,\"culture\":1,\"cœur\":1,\"d\":37,\"data\":10,\"diffuser\":1,\"diffusion\":1,\"digitale\":1,\"directes\":1,\"direction\":2,\"directions\":4,\"disposer\":2,\"dites\":1,\"documentaires\":1,\"documentation\":1,\"documents\":1,\"donnée\":1,\"données\":17,\"dossiers\":1,\"dsi\":2,\"débit\":1,\"décisionnel\":2,\"décisionnelle\":1,\"décisionnels\":1,\"définies\":2,\"définir\":4,\"définition\":1,\"département\":1,\"départemental\":1,\"départementales\":1,\"déplacements\":1,\"dérogatoire\":1,\"déroulement\":1,\"développement\":6,\"développements\":1,\"e\":3,\"effectuées\":1,\"elaboration\":1,\"emploi\":3,\"enjeux\":1,\"enrichir\":1,\"entreprises\":3,\"entrepôts\":1,\"esprit\":1,\"etl\":1,\"expertise\":2,\"exploration\":1,\"expression\":2,\"expériences\":1,\"externe\":1,\"exécution\":1,\"f\":1,\"faire\":5,\"financiers\":1,\"financières\":1,\"fins\":1,\"fonction\":2,\"fonctionnalités\":1,\"fonctionnelles\":2,\"fonctionnement\":1,\"formation\":2,\"fournir\":1,\"fournis\":1,\"fournisseurs\":2,\"fr\":2,\"garantir\":1,\"ged\":1,\"gestion\":2,\"gestmax\":2,\"groupes\":1,\"guides\":1,\"général\":1,\"généraux\":1,\"h\":1,\"handicapé\":1,\"haut\":1,\"https\":2,\"identifier\":1,\"idéalement\":1,\"industrialisation\":1,\"information\":4,\"informatique\":3,\"informatisation\":1,\"infrastructures\":2,\"ingenieur\":1,\"ingénieur\":2,\"ingénieurs\":1,\"initie\":1,\"innovants\":1,\"interfaces\":1,\"interne\":2,\"internes\":1,\"intégration\":4,\"itil\":1,\"jeunesse\":1,\"l\":15,\"lenord\":2,\"liens\":1,\"lieu\":1,\"logiciels\":1,\"légaux\":1,\"maintenance\":2,\"maintien\":1,\"marché\":3,\"marchés\":1,\"matière\":1,\"maîtrise\":1,\"mco\":1,\"mettre\":1,\"ministères\":1,\"mise\":2,\"missions\":2,\"mobiliser\":1,\"mobilité\":2,\"modalités\":1,\"modes\":1,\"modélisation\":2,\"méthodes\":2,\"métiers\":2,\"niveau\":1,\"nord\":1,\"normes\":1,\"notes\":1,\"numérisation\":1,\"nécessaire\":1,\"nécessaires\":2,\"négocier\":1,\"obligations\":1,\"observation\":1,\"offre\":1,\"offres\":2,\"opérationnelle\":1,\"opérationnelles\":2,\"opératoires\":1,\"opérer\":1,\"organisationnels\":1,\"organiser\":2,\"outils\":2,\"ouvrage\":1,\"paramétrage\":1,\"partenaires\":1,\"participer\":2,\"particulières\":1,\"partir\":1,\"patrimoine\":1,\"pilotage\":6,\"piloter\":1,\"pièces\":2,\"place\":1,\"planification\":2,\"planifier\":2,\"plateformes\":2,\"poste\":2,\"postes\":1,\"poursuit\":1,\"prestataires\":1,\"prestations\":4,\"preuve\":2,\"principe\":1,\"problèmes\":1,\"process\":1,\"procédures\":1,\"professionnelles\":1,\"progiciels\":1,\"programmes\":1,\"projet\":7,\"projets\":7,\"propositions\":1,\"prototypes\":1,\"préparer\":1,\"présenter\":1,\"prévisionnel\":1,\"public\":2,\"publics\":1,\"publique\":2,\"qualité\":5,\"rapports\":3,\"rattaché\":1,\"reconnaissance\":1,\"recrutement\":2,\"recueil\":1,\"relationnelle\":1,\"relations\":2,\"relatives\":1,\"remplissant\":1,\"rendu\":1,\"rendus\":1,\"renseigner\":1,\"reprise\":1,\"requises\":1,\"responsables\":1,\"risques\":1,\"règles\":3,\"réalisation\":1,\"réaliser\":2,\"réception\":1,\"réceptionner\":1,\"rédactionnelles\":2,\"rédiger\":1,\"réemploi\":1,\"référencement\":2,\"référent\":1,\"référentiels\":1,\"savoir\":3,\"savoirs\":1,\"secrétariats\":1,\"sectorielle\":1,\"sein\":1,\"sens\":1,\"sensibiliser\":1,\"service\":4,\"services\":5,\"sites\":2,\"solidarité\":1,\"solutions\":1,\"sport\":1,\"spécifications\":2,\"statutaires\":1,\"stockage\":3,\"structurants\":1,\"structuration\":1,\"structurer\":1,\"suivi\":3,\"superviser\":1,\"support\":1,\"supports\":1,\"supérieure\":1,\"synthèse\":1,\"systèmes\":4,\"sécurisation\":2,\"sécurité\":3,\"sélection\":1,\"tableaux\":1,\"technique\":4,\"techniques\":11,\"territoires\":1,\"tests\":1,\"tiers\":2,\"titre\":1,\"traitement\":1,\"transformation\":1,\"transversalité\":1,\"travail\":2,\"travailler\":1,\"travailleur\":1,\"travaux\":3,\"téléphoniques\":1,\"télétravaillable\":1,\"urbanisation\":1,\"usage\":1,\"usager\":1,\"valorisation\":1,\"veille\":1,\"visualisation\":1,\"voie\":1,\"vouloir\":1,\"vérifier\":1,\"école\":1,\"écosystème\":3,\"écoute\":1,\"écrite\":1,\"éditeurs\":1,\"égalité\":1,\"élaboration\":1,\"élaborer\":3,\"équipe\":2,\"équipes\":3,\"établir\":1,\"établissements\":1,\"études\":1,\"évaluer\":1,\"évolutive\":1,\"êtes\":2,\"œuvre\":3,\"œuvrer\":1}\n",
      "{\"accès\":1,\"accéder\":1,\"acquérir\":1,\"activement\":2,\"administrer\":1,\"ajustements\":1,\"amélioration\":1,\"améliorations\":1,\"améliorer\":1,\"analyse\":2,\"analyses\":2,\"api\":1,\"application\":1,\"apprendre\":1,\"approfondies\":1,\"appréciés\":1,\"appuyant\":1,\"appétence\":1,\"apte\":1,\"architecture\":1,\"architectures\":1,\"associées\":2,\"assure\":1,\"assurer\":1,\"attend\":1,\"attention\":1,\"autonome\":1,\"autorité\":1,\"avez\":1,\"axes\":1,\"base\":1,\"bases\":5,\"basées\":1,\"besoins\":2,\"bien\":1,\"big\":1,\"bon\":1,\"bonne\":1,\"briques\":2,\"bus\":1,\"business\":1,\"bénéficiant\":1,\"cahiers\":1,\"candidats\":2,\"candidature\":2,\"capacité\":3,\"charges\":1,\"clés\":3,\"code\":1,\"collaboration\":2,\"collecte\":1,\"collectivité\":2,\"comprendre\":2,\"compréhensibles\":1,\"compétences\":2,\"condition\":1,\"conditions\":1,\"conformément\":1,\"connaissance\":3,\"connaissances\":3,\"construction\":1,\"construire\":3,\"contact\":1,\"conteneurs\":1,\"contiendra\":1,\"contractuelle\":1,\"contribuant\":1,\"contribuez\":1,\"cours\":1,\"curieux\":1,\"d\":17,\"data\":13,\"datawarehouses\":1,\"deep\":1,\"diagnostiquer\":1,\"discours\":1,\"disponibilité\":3,\"documentation\":1,\"documenter\":1,\"donnée\":2,\"données\":16,\"dsi\":2,\"décisions\":1,\"définies\":1,\"département\":1,\"déployées\":1,\"déposer\":1,\"dérogatoire\":1,\"détail\":1,\"e\":1,\"effectuant\":1,\"effectuer\":1,\"efficience\":1,\"emploi\":4,\"ensuite\":1,\"entrepôts\":1,\"esprit\":1,\"etl\":2,\"excellence\":1,\"expert\":1,\"expertise\":1,\"exploitables\":1,\"exploiter\":1,\"expériences\":1,\"extraire\":1,\"faire\":3,\"faites\":1,\"fiabilité\":1,\"flux\":1,\"fonction\":1,\"force\":1,\"forte\":1,\"garantir\":1,\"gestion\":1,\"gestionnaire\":1,\"global\":2,\"globale\":1,\"général\":1,\"générer\":1,\"hadoop\":1,\"handicapé\":1,\"hétérogènes\":1,\"ia\":1,\"identifier\":1,\"importants\":1,\"incohérences\":1,\"indexation\":1,\"industrialiser\":1,\"information\":1,\"informations\":2,\"infrastructure\":1,\"ingénieur\":3,\"intelligence\":1,\"interactions\":1,\"interlocuteurs\":1,\"interopérables\":1,\"intégration\":1,\"java\":1,\"javascript\":1,\"kafka\":1,\"l\":11,\"lakes\":1,\"langage\":1,\"langages\":1,\"learning\":2,\"licensing\":1,\"liées\":1,\"machine\":1,\"maintenir\":1,\"maitrise\":1,\"management\":1,\"manager\":1,\"manipulation\":1,\"manière\":1,\"master\":1,\"maîtrise\":1,\"mco\":1,\"meilleures\":1,\"mener\":1,\"message\":1,\"mesurer\":1,\"modèles\":3,\"modélisation\":2,\"moteur\":2,\"moteurs\":2,\"motivé\":1,\"mysql\":1,\"métier\":1,\"métiers\":2,\"nettoyage\":1,\"niveau\":1,\"nosql\":1,\"nouvelles\":1,\"objectif\":1,\"objects\":1,\"opérationnelle\":1,\"oracle\":2,\"organisation\":3,\"orienté\":1,\"outils\":2,\"pannes\":1,\"participer\":4,\"performance\":3,\"performances\":1,\"permettant\":1,\"pertinentes\":1,\"plateforme\":1,\"point\":1,\"populations\":1,\"poste\":1,\"postgresql\":2,\"pratiques\":1,\"preuve\":1,\"principe\":1,\"priorité\":1,\"prise\":1,\"privilégié\":1,\"problèmes\":1,\"processus\":1,\"procédures\":3,\"professionnaliser\":1,\"projets\":1,\"proposer\":1,\"proposition\":1,\"provenant\":1,\"précédentes\":1,\"public\":1,\"publique\":1,\"python\":1,\"qu\":1,\"qualité\":1,\"recherche\":1,\"recherchons\":1,\"reconnaissance\":1,\"relever\":1,\"remplissant\":1,\"requises\":1,\"requête\":1,\"respect\":1,\"responsable\":1,\"ressources\":1,\"restaurer\":1,\"rigoureux\":1,\"réalisation\":1,\"rédaction\":1,\"régulières\":1,\"résoudre\":1,\"s\":2,\"sauvegarder\":1,\"savez\":1,\"scripting\":1,\"sens\":3,\"service\":1,\"services\":2,\"site\":1,\"socle\":2,\"solutions\":2,\"sources\":1,\"soutenir\":1,\"spark\":1,\"sql\":2,\"statutaires\":1,\"stockage\":1,\"structurés\":1,\"su\":1,\"sujets\":2,\"surveille\":1,\"système\":1,\"sécurisée\":1,\"talend\":2,\"technique\":1,\"techniques\":4,\"technologies\":1,\"tendances\":1,\"tests\":1,\"texte\":1,\"titre\":1,\"traitement\":1,\"transformer\":1,\"travail\":1,\"travaille\":1,\"travailleur\":1,\"unité\":1,\"urbanisation\":1,\"utilisateurs\":3,\"utilisez\":1,\"versioning\":1,\"visant\":2,\"voie\":1,\"volumes\":1,\"vouloir\":1,\"workflow\":1,\"éclairées\":1,\"égalité\":1,\"équipe\":1,\"équipes\":3,\"état\":1,\"étroite\":2,\"évoluer\":2,\"êtes\":2}\n",
      "{\"accompagner\":2,\"accès\":2,\"accéder\":1,\"acteurs\":1,\"actions\":3,\"activement\":1,\"activité\":1,\"adaptabilité\":1,\"administration\":1,\"affiliée\":1,\"agiles\":1,\"analyse\":2,\"applications\":2,\"apporter\":1,\"architecture\":1,\"architectures\":1,\"artificielle\":1,\"assurer\":4,\"attenants\":1,\"auprès\":1,\"autour\":1,\"bac\":1,\"bases\":2,\"besoins\":1,\"big\":1,\"bigdata\":1,\"bonne\":2,\"budget\":1,\"business\":1,\"bénéficiant\":1,\"c\":1,\"cadre\":2,\"candidats\":2,\"capacité\":2,\"cartographie\":1,\"changements\":1,\"chef\":1,\"cnas\":1,\"code\":1,\"cohérence\":1,\"collaborateurs\":2,\"collectif\":1,\"collectivité\":4,\"compréhensibles\":1,\"compte\":1,\"conception\":2,\"conceptuels\":1,\"conditions\":1,\"conformité\":3,\"conformément\":1,\"connaissance\":3,\"connaître\":3,\"conseil\":1,\"conseils\":1,\"contractuelle\":1,\"contribuer\":1,\"contrôle\":1,\"coordination\":2,\"crm\":1,\"création\":1,\"créativité\":1,\"d\":15,\"data\":4,\"datalake\":1,\"datamarts\":1,\"dataviz\":1,\"datawarehouses\":1,\"direction\":1,\"directions\":1,\"documentation\":1,\"domaine\":2,\"donnée\":4,\"données\":8,\"définies\":1,\"définir\":1,\"définition\":1,\"délais\":1,\"départemental\":1,\"déploiement\":1,\"déployer\":1,\"dérogatoire\":1,\"développement\":3,\"e\":1,\"emploi\":3,\"encadrez\":1,\"ensemble\":1,\"erp\":1,\"esprit\":1,\"etl\":1,\"eurelien\":1,\"euréliens\":1,\"exigences\":1,\"exploitables\":1,\"expérience\":1,\"faciliter\":1,\"faire\":1,\"faisabilité\":1,\"flexibilité\":1,\"flux\":2,\"fonction\":1,\"fonctionnelle\":1,\"fonctionnels\":1,\"fr\":1,\"gestion\":2,\"globale\":1,\"gouvernance\":2,\"général\":1,\"handicapé\":1,\"hiérarchique\":1,\"identification\":1,\"idéalement\":1,\"impulser\":1,\"indemnitaire\":1,\"indicateurs\":1,\"information\":1,\"infrastructure\":1,\"ingénierie\":1,\"innovants\":1,\"intelligence\":2,\"interface\":1,\"interrogation\":1,\"intervenants\":1,\"it\":1,\"jouerez\":1,\"l\":6,\"langage\":1,\"langages\":1,\"leadership\":1,\"lieux\":1,\"liés\":1,\"logicielle\":1,\"maitriser\":1,\"managériaux\":1,\"manière\":1,\"matière\":1,\"maîtrise\":2,\"maîtriser\":4,\"merise\":1,\"mettre\":1,\"mission\":1,\"missions\":1,\"méthodes\":4,\"méthodologie\":1,\"métiers\":1,\"normes\":1,\"numérique\":2,\"numériques\":1,\"nécessaires\":1,\"object\":1,\"objectif\":1,\"objectifs\":2,\"objet\":1,\"opendata\":1,\"opportunités\":1,\"organisation\":2,\"organisationnels\":1,\"orientés\":1,\"outils\":3,\"participer\":1,\"parties\":1,\"pilotage\":1,\"piloter\":1,\"place\":1,\"plan\":1,\"planning\":1,\"plans\":1,\"prenantes\":1,\"prendre\":1,\"principales\":1,\"principe\":1,\"principes\":2,\"prioriser\":1,\"problématiques\":1,\"procédures\":1,\"professionnels\":1,\"projet\":3,\"projets\":4,\"public\":1,\"publique\":1,\"qu\":1,\"qualité\":2,\"quarantaine\":1,\"quotidiennement\":1,\"rassemble\":1,\"rattaché\":1,\"reconnaissance\":1,\"relationnelle\":1,\"remplissant\":1,\"requises\":1,\"respectant\":1,\"responsabilités\":1,\"responsable\":1,\"restaurant\":1,\"restitution\":1,\"rgpd\":1,\"risques\":1,\"réalisation\":2,\"référentiels\":1,\"régime\":1,\"rémunération\":1,\"rôle\":1,\"rôles\":1,\"sens\":1,\"serez\":1,\"service\":2,\"services\":1,\"soient\":1,\"solution\":1,\"sorte\":1,\"sql\":2,\"standards\":1,\"statutaire\":1,\"statutaires\":1,\"stratégie\":1,\"structuration\":2,\"suivi\":2,\"sujet\":1,\"synthèse\":1,\"systèmes\":1,\"sécurité\":3,\"talend\":1,\"technique\":1,\"technologique\":2,\"territoire\":1,\"territoriale\":1,\"tickets\":1,\"titre\":1,\"traduction\":1,\"traitement\":1,\"transition\":1,\"transversalité\":1,\"travailler\":1,\"travailleur\":1,\"uml\":1,\"utilisateurs\":1,\"valoriser\":1,\"veille\":1,\"veiller\":1,\"vision\":1,\"voie\":1,\"écoute\":1,\"égalité\":1,\"équipe\":1,\"état\":1,\"évaluation\":2,\"œuvrent\":1}\n",
      "{\"abondement\":1,\"accompagnement\":1,\"accompagnons\":1,\"acculturation\":1,\"accès\":1,\"acquis\":1,\"acteurs\":2,\"activités\":1,\"administration\":2,\"adn\":1,\"adopter\":1,\"agir\":1,\"aimer\":1,\"alimentations\":1,\"allez\":1,\"alliance\":2,\"amélioration\":1,\"an\":1,\"anglais\":1,\"appliquées\":1,\"architect\":1,\"architects\":1,\"ascq\":1,\"assurance\":2,\"assurances\":1,\"assurons\":1,\"assurées\":1,\"attend\":1,\"avantageusece\":1,\"avez\":2,\"axes\":1,\"bac\":1,\"bancaires\":1,\"banque\":3,\"besoin\":1,\"bi\":4,\"bien\":1,\"bienvenue\":1,\"billions\":1,\"bo\":2,\"bonnes\":1,\"bout\":2,\"business\":1,\"c\":1,\"cdi\":1,\"cergy\":1,\"charge\":1,\"chef\":1,\"cherchons\":1,\"clients\":2,\"clés\":1,\"coeur\":1,\"collectif\":1,\"commun\":1,\"complémentaires\":1,\"concepteurs\":2,\"concernés\":1,\"conditions\":1,\"conduisez\":1,\"connaissance\":2,\"connecté\":1,\"contact\":1,\"continue\":1,\"contrat\":1,\"conçoit\":1,\"coopérative\":1,\"crédit\":2,\"d\":8,\"data\":17,\"developpements\":2,\"deviennent\":1,\"dijon\":1,\"diversité\":1,\"donnée\":2,\"données\":7,\"droits\":1,\"décision\":1,\"décisionnel\":1,\"décisionnelle\":1,\"dédié\":1,\"département\":1,\"développement\":2,\"e\":4,\"eclairez\":1,\"ecouter\":1,\"employeur\":1,\"engage\":1,\"engineer\":2,\"engineers\":1,\"ensemble\":4,\"entités\":1,\"entreprise\":2,\"environnement\":1,\"environnementales\":1,\"environnements\":1,\"eur\":1,\"euro\":3,\"exploite\":1,\"expression\":1,\"expérience\":1,\"extension\":1,\"f\":1,\"factory\":2,\"filiale\":1,\"finance\":1,\"fixe\":1,\"fontenay\":1,\"force\":1,\"formation\":1,\"fournissons\":1,\"fraude\":1,\"fédérale\":2,\"gardiens\":1,\"gestion\":1,\"goût\":1,\"grande\":1,\"groupe\":5,\"géographiques\":1,\"h\":1,\"hadoop\":1,\"humain\":1,\"humaine\":1,\"ib\":1,\"immobilier\":1,\"information\":6,\"informationsle\":1,\"informatique\":2,\"ingénieur\":1,\"innovant\":1,\"international\":1,\"intimité\":1,\"intéressement\":1,\"investit\":1,\"jours\":1,\"jupyter\":1,\"l\":17,\"lab\":1,\"langage\":1,\"lille\":1,\"lyon\":1,\"maintient\":1,\"maitrisez\":1,\"mathématiques\":1,\"maîtrise\":1,\"menez\":1,\"mission\":1,\"missions\":2,\"missionsvous\":1,\"mobilisez\":1,\"mois\":1,\"monde\":1,\"motivé\":1,\"mutualisez\":1,\"mutualiste\":1,\"mutuel\":2,\"métiers\":5,\"nancy\":1,\"nantes\":3,\"national\":2,\"niveau\":2,\"nouseuro\":1,\"numérique\":1,\"objectifs\":1,\"officer\":2,\"officers\":1,\"organisation\":1,\"organisé\":1,\"orléans\":1,\"outil\":1,\"parentale\":1,\"paris\":1,\"participation\":1,\"participez\":3,\"partiel\":1,\"perco\":1,\"performance\":1,\"peuplons\":1,\"pilotez\":2,\"plaira\":1,\"plan\":1,\"politique\":1,\"portez\":1,\"poste\":2,\"postuler\":1,\"pourvoir\":1,\"pratiques\":1,\"prise\":1,\"prises\":1,\"process\":1,\"production\":1,\"profils\":1,\"projet\":1,\"projets\":6,\"proposition\":1,\"prédictifs\":1,\"préférentielles\":1,\"prévoyance\":1,\"publiée\":1,\"python\":2,\"qualité\":1,\"r\":2,\"raison\":1,\"recruteur\":1,\"regroupons\":1,\"reporting\":1,\"respect\":1,\"responsabilité\":1,\"retraite\":1,\"rtt\":1,\"réalisation\":1,\"réalise\":1,\"réf\":1,\"référence\":1,\"rémunération\":1,\"répartis\":1,\"résumé\":1,\"s\":2,\"salariés\":1,\"santé\":1,\"sap\":2,\"sas\":2,\"science\":2,\"scientists\":2,\"scores\":1,\"sein\":4,\"semaine\":1,\"service\":3,\"site\":1,\"sites\":1,\"sociales\":1,\"solidarité\":1,\"solide\":1,\"sommes\":1,\"souhaitez\":1,\"spss\":2,\"statistiques\":1,\"statut\":1,\"strasbourg\":1,\"suivi\":1,\"supplémentaire\":1,\"système\":1,\"sécurité\":1,\"taille\":1,\"technologie\":2,\"technologique\":1,\"traitement\":1,\"travaillent\":1,\"travailler\":1,\"travaux\":1,\"télétravail\":2,\"universitaire\":1,\"usages\":1,\"utilisation\":1,\"utilisé\":1,\"val\":1,\"valorisation\":2,\"variété\":1,\"vente\":1,\"versée\":1,\"vertica\":1,\"villeneuve\":1,\"vivre\":1,\"vousde\":1,\"webfocus\":1,\"éco\":1,\"école\":1,\"épargne\":1,\"équipe\":2,\"êtes\":1}\n",
      "{\"abondement\":1,\"accompagnement\":1,\"accompagnons\":1,\"acculturation\":1,\"accès\":1,\"acquis\":1,\"acteurs\":2,\"activités\":1,\"administration\":2,\"adn\":1,\"adopter\":1,\"agir\":1,\"aimer\":1,\"alimentations\":1,\"allez\":1,\"alliance\":2,\"amélioration\":1,\"an\":1,\"anglais\":1,\"appliquées\":1,\"architect\":1,\"architects\":1,\"ascq\":2,\"assurance\":2,\"assurances\":1,\"assurons\":1,\"assurées\":1,\"attend\":1,\"avantageusece\":1,\"avez\":2,\"axes\":1,\"bac\":1,\"bancaires\":1,\"banque\":3,\"besoin\":1,\"bi\":4,\"bien\":1,\"bienvenue\":1,\"billions\":1,\"bo\":2,\"bonnes\":1,\"bout\":2,\"business\":1,\"c\":1,\"cdi\":1,\"cergy\":1,\"charge\":1,\"chef\":1,\"cherchons\":1,\"clients\":2,\"clés\":1,\"coeur\":1,\"collectif\":1,\"commun\":1,\"complémentaires\":1,\"concepteurs\":2,\"concernés\":1,\"conditions\":1,\"conduisez\":1,\"connaissance\":2,\"connecté\":1,\"contact\":1,\"continue\":1,\"contrat\":1,\"conçoit\":1,\"coopérative\":1,\"crédit\":2,\"d\":9,\"data\":17,\"developpements\":2,\"deviennent\":1,\"dijon\":1,\"diversité\":1,\"donnée\":2,\"données\":7,\"droits\":1,\"décision\":1,\"décisionnel\":1,\"décisionnelle\":1,\"dédié\":1,\"département\":1,\"développement\":2,\"e\":4,\"eclairez\":1,\"ecouter\":1,\"employeur\":1,\"engage\":1,\"engineer\":2,\"engineers\":1,\"ensemble\":4,\"entités\":1,\"entreprise\":2,\"environnement\":1,\"environnementales\":1,\"environnements\":1,\"eur\":1,\"euro\":3,\"exploite\":1,\"expression\":1,\"expérience\":1,\"extension\":1,\"f\":1,\"factory\":2,\"filiale\":1,\"finance\":1,\"fixe\":1,\"fontenay\":1,\"force\":1,\"formation\":1,\"fournissons\":1,\"fraude\":1,\"fédérale\":2,\"gardiens\":1,\"gestion\":1,\"goût\":1,\"grande\":1,\"groupe\":5,\"géographiques\":1,\"h\":1,\"hadoop\":1,\"humain\":1,\"humaine\":1,\"ib\":1,\"immobilier\":1,\"information\":6,\"informationsle\":1,\"informatique\":2,\"ingénieur\":1,\"innovant\":1,\"international\":1,\"intimité\":1,\"intéressement\":1,\"investit\":1,\"jours\":1,\"jupyter\":1,\"l\":17,\"lab\":1,\"langage\":1,\"lille\":1,\"lyon\":1,\"maintient\":1,\"maitrisez\":1,\"mathématiques\":1,\"maîtrise\":1,\"menez\":1,\"mission\":1,\"missions\":2,\"missionsvous\":1,\"mobilisez\":1,\"mois\":1,\"monde\":1,\"motivé\":1,\"mutualisez\":1,\"mutualiste\":1,\"mutuel\":2,\"métiers\":5,\"nancy\":1,\"nantes\":2,\"national\":2,\"niveau\":2,\"nouseuro\":1,\"numérique\":1,\"objectifs\":1,\"officer\":2,\"officers\":1,\"organisation\":1,\"organisé\":1,\"orléans\":1,\"outil\":1,\"parentale\":1,\"paris\":1,\"participation\":1,\"participez\":3,\"partiel\":1,\"perco\":1,\"performance\":1,\"peuplons\":1,\"pilotez\":2,\"plaira\":1,\"plan\":1,\"politique\":1,\"portez\":1,\"poste\":2,\"pourvoir\":1,\"pratiques\":1,\"prise\":1,\"prises\":1,\"process\":1,\"production\":1,\"profils\":1,\"projet\":1,\"projets\":6,\"proposition\":1,\"prédictifs\":1,\"préférentielles\":1,\"prévoyance\":1,\"publiée\":1,\"python\":2,\"qualité\":1,\"r\":2,\"raison\":1,\"regroupons\":1,\"reporting\":1,\"respect\":1,\"responsabilité\":1,\"retraite\":1,\"rtt\":1,\"réalisation\":1,\"réalise\":1,\"réf\":1,\"référence\":1,\"rémunération\":1,\"répartis\":1,\"résumé\":1,\"s\":2,\"salariés\":1,\"santé\":1,\"sap\":2,\"sas\":2,\"science\":2,\"scientists\":2,\"scores\":1,\"sein\":4,\"semaine\":1,\"service\":3,\"sites\":1,\"sociales\":1,\"solidarité\":1,\"solide\":1,\"sommes\":1,\"souhaitez\":1,\"spss\":2,\"statistiques\":1,\"statut\":1,\"strasbourg\":1,\"suivi\":1,\"supplémentaire\":1,\"système\":1,\"sécurité\":1,\"taille\":1,\"technologie\":2,\"technologique\":1,\"traitement\":1,\"travaillent\":1,\"travailler\":1,\"travaux\":1,\"télétravail\":2,\"universitaire\":1,\"usages\":1,\"utilisation\":1,\"utilisé\":1,\"val\":1,\"valorisation\":2,\"variété\":1,\"vente\":1,\"versée\":1,\"vertica\":1,\"villeneuve\":2,\"vivre\":1,\"vousde\":1,\"webfocus\":1,\"éco\":1,\"école\":1,\"épargne\":1,\"équipe\":2,\"êtes\":1}\n",
      "{\"acquise\":1,\"administration\":1,\"agile\":1,\"aide\":1,\"alturion\":2,\"améliorer\":1,\"an\":3,\"analyse\":1,\"ans\":2,\"api\":1,\"assurer\":1,\"auprès\":1,\"bac\":2,\"bases\":1,\"basse\":1,\"basé\":1,\"basée\":1,\"besoins\":2,\"bord\":2,\"brut\":1,\"bénéficiez\":1,\"cahier\":1,\"cdi\":1,\"charges\":1,\"chiffrage\":1,\"client\":1,\"clients\":2,\"commerce\":1,\"communiqué\":1,\"condition\":1,\"conscient\":1,\"construire\":1,\"d\":6,\"dam\":1,\"data\":3,\"destination\":1,\"dimension\":1,\"diplômé\":1,\"disponibilité\":1,\"disposition\":1,\"distribution\":2,\"donnée\":4,\"données\":13,\"dsi\":2,\"décision\":1,\"e\":5,\"edi\":1,\"emploi\":1,\"enjeux\":1,\"ensemble\":1,\"entreprise\":3,\"entreprises\":1,\"entrepôt\":2,\"estimation\":4,\"estimé\":2,\"etl\":1,\"exactitude\":1,\"exp\":1,\"expertise\":2,\"expérience\":2,\"externes\":1,\"f\":2,\"faire\":2,\"fonction\":1,\"fourchette\":1,\"fournisseurs\":1,\"fraicheur\":2,\"gestion\":1,\"h\":2,\"haute\":1,\"hellowork\":2,\"idéalement\":1,\"inconnues\":1,\"information\":1,\"inhérents\":1,\"insee\":1,\"internes\":1,\"intégrité\":2,\"l\":8,\"liés\":2,\"lyon\":3,\"maintenir\":1,\"maitrisez\":1,\"management\":1,\"manager\":2,\"matériel\":1,\"mdf\":1,\"mettre\":3,\"minimum\":1,\"missions\":2,\"méthode\":1,\"métier\":1,\"n\":1,\"nationale\":1,\"non\":1,\"offre\":1,\"offres\":1,\"opérationnelle\":1,\"oui\":1,\"outils\":3,\"participer\":1,\"pertinence\":1,\"piloter\":1,\"pilotez\":1,\"pim\":1,\"place\":2,\"poste\":2,\"postuler\":1,\"principales\":1,\"profil\":1,\"projets\":2,\"propose\":1,\"publiée\":1,\"qualification\":1,\"rationnaliser\":1,\"rattaché\":1,\"recherché\":1,\"recrute\":1,\"recruteur\":1,\"recueil\":1,\"recueillir\":1,\"rédaction\":1,\"réf\":1,\"résumé\":1,\"salaire\":3,\"secteur\":1,\"sensible\":1,\"services\":3,\"similaires\":1,\"solutions\":1,\"spécialisé\":1,\"stockage\":1,\"sud\":1,\"systèmes\":1,\"sécuriser\":1,\"sécurité\":1,\"t\":1,\"tableaux\":2,\"technique\":1,\"techniques\":1,\"technologies\":1,\"termes\":2,\"utile\":1,\"variable\":1,\"web\":1,\"échanges\":3,\"électrique\":1,\"équivalent\":1,\"évoluer\":2,\"êtes\":1}\n"
     ]
    }
   ],
   "source": [
    "fillDW(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"welcometothejungle\"\n",
    "rootLink = \"https://www.welcometothejungle.com\"\n",
    "service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service) \n",
    "keyword=\"data\"\n",
    "pages=2\n",
    "nb_docs=33\n",
    "corpus = list()\n",
    "try:\n",
    "    for page in range(1,pages+1):    \n",
    "        try:\n",
    "            driver.get(f'{rootLink}/fr/jobs?query=data&aroundQuery=France&page={page}')\n",
    "            # Initial find of elements\n",
    "            try:\n",
    "                accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#axeptio_btn_acceptAll'))\n",
    "                )\n",
    "\n",
    "                accept_cookies_button.click()\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "                \n",
    "            offers = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "            )\n",
    "            \n",
    "            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Getting docs left if last page to query\n",
    "            if nb_docs%30 != 0 and page==pages: \n",
    "                limit = nb_docs%30\n",
    "                div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")[:limit]\n",
    "            else :\n",
    "                div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "\n",
    "            for index in range(len(div_elements_to_click_list)):\n",
    "                try:\n",
    "                    offers = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "                    )\n",
    "                    \n",
    "                    div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "                    div_elements_to_click = div_elements_to_click_list[index]\n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "                    # Check if the index is within the valid range\n",
    "                    div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "                    position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "                    position = position_elem.text\n",
    "                    workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "                    contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "                    main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "                    contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "                    published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "                    published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "                    published_date = published_date.strftime('%d/%m/%Y')\n",
    "                    # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "                    # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                    # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                    # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                    # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                    # Scroll into view\n",
    "                    # actions = ActionChains(driver)\n",
    "                    # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "                    #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "                    # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "                    # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                    # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "                    # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                    # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "                    # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "                    # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "                    driver.execute_script(\"arguments[0].click();\", div_offer)\n",
    "                    WebDriverWait(driver, 20).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-testid='job-section-description']\"))\n",
    "                    )\n",
    "                    description_elem = driver.find_element(By.CSS_SELECTOR,\"div[data-testid='job-section-description']\")\n",
    "\n",
    "                    # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                    \n",
    "                    WebDriverWait(driver, 3)\n",
    "\n",
    "                    try:\n",
    "                        profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "                        profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                        profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "                    except NoSuchElementException:\n",
    "                        # If profile element is not found, set it to \"NULL\"\n",
    "                        profile = \"NULL\"\n",
    "                    # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "                    description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    \n",
    "                    #parsing tags\n",
    "                    description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "                    company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "                    company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "                    long_infos = \" \".join([description,profile])\n",
    "                    # print(long_infos)\n",
    "                    # Wait for the child element to become present in the DOM\n",
    "\n",
    "                    # # Get the page source after the click\n",
    "                    # page_source = driver.page_source\n",
    "\n",
    "                    # Use Beautiful Soup to parse the page source\n",
    "                    # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                    # Example: Retrieve the text of a specific element\n",
    "                    # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                    # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                    # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                    # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                    # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "                    # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                    # long_infos = soup.select(\"section\")\n",
    "                    # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "                    # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "                    # infos = \" \".join([item for item in infos if item != ''])\n",
    "                    # infos = [item for item in infos if '' not in item]\n",
    "                    corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "\n",
    "                finally:\n",
    "                    # Navigate back to the main page\n",
    "                    driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print the exception for debugging purposes\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Intérim\n",
      "Error: name 'description_elem' is not defined\n",
      "Error: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=120.0.6099.130)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0BE59A]\n",
      "\t(No symbol) [0x00007FF75A0B59E9]\n",
      "\t(No symbol) [0x00007FF75A0B5AF3]\n",
      "\t(No symbol) [0x00007FF75A0B4298]\n",
      "\t(No symbol) [0x00007FF75A0B732A]\n",
      "\t(No symbol) [0x00007FF75A12B12B]\n",
      "\t(No symbol) [0x00007FF75A1120AA]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n",
      "Error: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=120.0.6099.130)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0BE59A]\n",
      "\t(No symbol) [0x00007FF75A0B59E9]\n",
      "\t(No symbol) [0x00007FF75A0B5AF3]\n",
      "\t(No symbol) [0x00007FF75A0B4298]\n",
      "\t(No symbol) [0x00007FF75A0B732A]\n",
      "\t(No symbol) [0x00007FF75A12B12B]\n",
      "\t(No symbol) [0x00007FF75A1120AA]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n",
      "Error: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=120.0.6099.130)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0BE59A]\n",
      "\t(No symbol) [0x00007FF75A0B59E9]\n",
      "\t(No symbol) [0x00007FF75A0B5AF3]\n",
      "\t(No symbol) [0x00007FF75A0B4298]\n",
      "\t(No symbol) [0x00007FF75A0B732A]\n",
      "\t(No symbol) [0x00007FF75A12B12B]\n",
      "\t(No symbol) [0x00007FF75A1120AA]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n",
      "Error: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0F175C]\n",
      "\t(No symbol) [0x00007FF75A0F18DC]\n",
      "\t(No symbol) [0x00007FF75A12CBC7]\n",
      "\t(No symbol) [0x00007FF75A1120EF]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(div_elements_to_click_list)):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma[id=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhypViewJob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         div_elements_to_click_list \u001b[38;5;241m=\u001b[39m offers\n\u001b[0;32m     37\u001b[0m         div_elements_to_click \u001b[38;5;241m=\u001b[39m div_elements_to_click_list[index]\n",
      "File \u001b[1;32mc:\\Users\\leogo\\miniconda3\\envs\\text-mining\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:86\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     84\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Problème toutes les entreprises sont Adecco, si autre, redirection sur un autre site\n",
    "# source = \"adecco\"\n",
    "# rootLink = \"https://www.adecco.fr\"\n",
    "# service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=service) \n",
    "# keyword=\"data\"\n",
    "# pages=1\n",
    "# nb_docs=33\n",
    "# corpus = list()\n",
    "# try:\n",
    "#     for page in range(1,pages+1):    \n",
    "#         try:\n",
    "#             print(page)\n",
    "#             driver.get(f'{rootLink}/resultats-offres-emploi/m-{keyword}?pageNum={page}')\n",
    "#             # Initial find of elements\n",
    "                \n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"a[id='hypViewJob']\"))\n",
    "#             )\n",
    "\n",
    "#             offers = driver.find_elements(By.CSS_SELECTOR, \"a[id='hypViewJob']\")\n",
    "\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#             # Getting docs left if last page to query\n",
    "#             if nb_docs%50!= 0 and page==pages: \n",
    "#                 limit = nb_docs%50\n",
    "#                 div_elements_to_click_list = offers[:limit]\n",
    "#             else :\n",
    "#                 div_elements_to_click_list = offers\n",
    "#             # print(len(div_elements_to_click_list))\n",
    "#             for index in range(len(div_elements_to_click_list)):\n",
    "#                 try:\n",
    "#                     WebDriverWait(driver, 10).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"a[id='hypViewJob']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     div_elements_to_click_list = offers\n",
    "#                     div_elements_to_click = div_elements_to_click_list[index]\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "#                     # Check if the index is within the valid range\n",
    "#                     # div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "#                     # position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "#                     # position = position_elem.text\n",
    "#                     # workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "#                     # contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "#                     # main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "#                     # contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "#                     # published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "#                     # published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "#                     # published_date = published_date.strftime('%d/%m/%Y')\n",
    "#                     # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "#                     # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "#                     # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "#                     # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "#                     # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "#                     # Scroll into view\n",
    "#                     # actions = ActionChains(driver)\n",
    "#                     # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "#                     #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "#                     # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "#                     # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "#                     # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "#                     driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "#                     WebDriverWait(driver, 20).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='media-body']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     workplace = driver.find_element(By.CSS_SELECTOR,\"span[id='lblCity']\").text\n",
    "#                     contract_type_elem = driver.find_element(By.CSS_SELECTOR,\"span[id='ltEmploymentType']\")\n",
    "#                     contract_type = contract_type_elem.find_element(By.CSS_SELECTOR,\"a\").text\n",
    "                    \n",
    "#                     # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                    \n",
    "#                     WebDriverWait(driver, 3)\n",
    "\n",
    "#                     try:\n",
    "#                         profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "#                         profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                         profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If profile element is not found, set it to \"NULL\"\n",
    "#                         profile = \"NULL\"\n",
    "#                     # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "#                     description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                     # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    \n",
    "#                     #parsing tags\n",
    "#                     # description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "#                     # company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "#                     # long_infos = \" \".join([description,profile])\n",
    "#                     # print(long_infos)\n",
    "#                     # Wait for the child element to become present in the DOM\n",
    "\n",
    "#                     # # Get the page source after the click\n",
    "#                     # page_source = driver.page_source\n",
    "\n",
    "#                     # Use Beautiful Soup to parse the page source\n",
    "#                     # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "#                     # Example: Retrieve the text of a specific element\n",
    "#                     # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "#                     # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "#                     # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "#                     # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "#                     # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "#                     # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "#                     # long_infos = soup.select(\"section\")\n",
    "#                     # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "#                     # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "#                     # infos = \" \".join([item for item in infos if item != ''])\n",
    "#                     # infos = [item for item in infos if '' not in item]\n",
    "#                     corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "#                 except Exception as e:\n",
    "#                     # Print the exception for debugging purposes\n",
    "#                     print(f\"Error: {e}\")\n",
    "\n",
    "#                 finally:\n",
    "#                     # Navigate back to the main page\n",
    "#                     driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             # Print the exception for debugging purposes\n",
    "#             print(f\"Error: {e}\")\n",
    "\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m         deny_cookies_button \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbutton#pecookies-continue-btn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         deny_cookies_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
      "File \u001b[1;32mc:\\Users\\leogo\\miniconda3\\envs\\text-mining\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:86\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     84\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Problème toutes les entreprises sont Adecco, si autre, redirection sur un autre site\n",
    "# source = \"jobintree\"\n",
    "# rootLink = \"https://www.jobintree.com\"\n",
    "# service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=service) \n",
    "# keyword=\"data\"\n",
    "# pages=1\n",
    "# nb_docs=33\n",
    "# corpus = list()\n",
    "# try:\n",
    "#     for page in range(pages):    \n",
    "#         try:\n",
    "#             driver.get(f'{rootLink}/emploi?keywords={keyword}&page={page}')\n",
    "#             # Initial find of elements\n",
    "                \n",
    "#             offers = WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='annonces_normales']\"))\n",
    "#             )\n",
    "\n",
    "#             offers = offers.find_elements(By.CSS_SELECTOR,\"a\")\n",
    "#             print(len(offers))\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#             # Getting docs left if last page to query\n",
    "#             if nb_docs%20!= 0 and page==pages-1: \n",
    "#                 limit = nb_docs%20\n",
    "#                 div_elements_to_click_list = offers[:limit]\n",
    "#             else :\n",
    "#                 div_elements_to_click_list = offers\n",
    "#             # print(len(div_elements_to_click_list))\n",
    "#             for index in range(len(div_elements_to_click_list)):\n",
    "#                 try:\n",
    "#                     WebDriverWait(driver, 10).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"a[id='hypViewJob']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     div_elements_to_click_list = offers\n",
    "#                     div_elements_to_click = div_elements_to_click_list[index]\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "#                     # Check if the index is within the valid range\n",
    "#                     # div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "#                     # position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "#                     # position = position_elem.text\n",
    "#                     # workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "#                     # contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "#                     # main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "#                     # contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "#                     # published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "#                     # published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "#                     # published_date = published_date.strftime('%d/%m/%Y')\n",
    "#                     # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "#                     # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "#                     # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "#                     # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "#                     # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "#                     # Scroll into view\n",
    "#                     # actions = ActionChains(driver)\n",
    "#                     # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "#                     #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "#                     # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "#                     # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "#                     # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "#                     driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "#                     WebDriverWait(driver, 20).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='media-body']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     workplace = driver.find_element(By.CSS_SELECTOR,\"span[id='lblCity']\").text\n",
    "#                     contract_type_elem = driver.find_element(By.CSS_SELECTOR,\"span[id='ltEmploymentType']\")\n",
    "#                     contract_type = contract_type_elem.find_element(By.CSS_SELECTOR,\"a\").text\n",
    "                    \n",
    "#                     # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                    \n",
    "#                     WebDriverWait(driver, 3)\n",
    "\n",
    "#                     try:\n",
    "#                         profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "#                         profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                         profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If profile element is not found, set it to \"NULL\"\n",
    "#                         profile = \"NULL\"\n",
    "#                     # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "#                     description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                     # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    \n",
    "#                     #parsing tags\n",
    "#                     # description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "#                     # company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "#                     # long_infos = \" \".join([description,profile])\n",
    "#                     # print(long_infos)\n",
    "#                     # Wait for the child element to become present in the DOM\n",
    "\n",
    "#                     # # Get the page source after the click\n",
    "#                     # page_source = driver.page_source\n",
    "\n",
    "#                     # Use Beautiful Soup to parse the page source\n",
    "#                     # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "#                     # Example: Retrieve the text of a specific element\n",
    "#                     # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "#                     # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "#                     # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "#                     # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "#                     # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "#                     # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "#                     # long_infos = soup.select(\"section\")\n",
    "#                     # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "#                     # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "#                     # infos = \" \".join([item for item in infos if item != ''])\n",
    "#                     # infos = [item for item in infos if '' not in item]\n",
    "#                     corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "#                 except Exception as e:\n",
    "#                     # Print the exception for debugging purposes\n",
    "#                     print(f\"Error: {e}\")\n",
    "\n",
    "#                 finally:\n",
    "#                     # Navigate back to the main page\n",
    "#                     driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             # Print the exception for debugging purposes\n",
    "#             print(f\"Error: {e}\")\n",
    "\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = \"pole-emploi\"\n",
    "# rootLink = \"https://candidat.pole-emploi.fr\"\n",
    "# service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=service) \n",
    "# keyword=\"data\"\n",
    "# pages=2\n",
    "# nb_docs=33\n",
    "# corpus = list()\n",
    "# try:\n",
    "#     driver.get(f'{rootLink}/offres/recherche?motsCles={keyword}&range=0-{nb_docs-1}')\n",
    "#     # Initial find of elements\n",
    "#     try:\n",
    "#         accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "#             EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#pecookies-accept-all'))\n",
    "#         )\n",
    "#         print(\"test\")\n",
    "#         accept_cookies_button.click()\n",
    "#     except TimeoutException:\n",
    "#         pass\n",
    "#     WebDriverWait(driver, 10).until(\n",
    "#     EC.presence_of_element_located((By.CSS_SELECTOR, \"a[class='media with-fav']\"))\n",
    "#     )\n",
    "\n",
    "#     offers = driver.find_elements(By.CSS_SELECTOR, \"a[class='media with-fav']\")\n",
    "\n",
    "#     div_elements_to_click_list = offers\n",
    "#     # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#     # Getting docs left if last page to query\n",
    "#     # print(len(div_elements_to_click_list))\n",
    "#     for index in range(len(div_elements_to_click_list)):\n",
    "#         try:\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"a[class='media with-fav']\"))\n",
    "#             )\n",
    "#             offers = driver.find_elements(By.CSS_SELECTOR, \"a[class='media with-fav']\")\n",
    "\n",
    "#             div_elements_to_click_list = offers\n",
    "#             div_elements_to_click = div_elements_to_click_list[index]\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "#             # Check if the index is within the valid range\n",
    "#             # div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "#             # position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "#             # position = position_elem.text\n",
    "#             # workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "#             # contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "#             # main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "#             # contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "#             # published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "#             # published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "#             # published_date = published_date.strftime('%d/%m/%Y')\n",
    "#             # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "#             # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "#             # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "#             # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "#             # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "#             # Scroll into view\n",
    "#             # actions = ActionChains(driver)\n",
    "#             # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "#             #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "#             # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "#             # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#             # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "#             # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#             # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "#             # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "#             # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "#             driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "#             WebDriverWait(driver, 3000).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='media-body']\"))\n",
    "#             )\n",
    "            \n",
    "#             # workplace = driver.find_element(By.CSS_SELECTOR,\"span[id='lblCity']\").text\n",
    "#             # contract_type_elem = driver.find_element(By.CSS_SELECTOR,\"span[id='ltEmploymentType']\")\n",
    "#             # contract_type = contract_type_elem.find_element(By.CSS_SELECTOR,\"a\").text\n",
    "            \n",
    "#             # # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "            \n",
    "#             # WebDriverWait(driver, 3)\n",
    "\n",
    "#             # try:\n",
    "#             #     profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "#             #     profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#             #     profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "#             # except NoSuchElementException:\n",
    "#             #     # If profile element is not found, set it to \"NULL\"\n",
    "#             #     profile = \"NULL\"\n",
    "#             # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "#             # description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#             # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "            \n",
    "#             #parsing tags\n",
    "#             # description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "#             # company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "#             # company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "#             # long_infos = \" \".join([description,profile])\n",
    "#             # print(long_infos)\n",
    "#             # Wait for the child element to become present in the DOM\n",
    "\n",
    "#             # # Get the page source after the click\n",
    "#             # page_source = driver.page_source\n",
    "\n",
    "#             # Use Beautiful Soup to parse the page source\n",
    "#             # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "#             # Example: Retrieve the text of a specific element\n",
    "#             # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "#             # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "#             # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "#             # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "#             # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "#             # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "#             # long_infos = soup.select(\"section\")\n",
    "#             # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "#             # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "#             # infos = \" \".join([item for item in infos if item != ''])\n",
    "#             # infos = [item for item in infos if '' not in item]\n",
    "#             # corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "#         except Exception as e:\n",
    "#             # Print the exception for debugging purposes\n",
    "#             print(f\"Error: {e}\")\n",
    "\n",
    "#         finally:\n",
    "#             # Navigate back to the main page\n",
    "#             driver.execute_script(\"window.history.go(-1);\")\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 404\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"fr\" xml:lang=\"fr\" class=\"no-js\">\n",
      "\t<head> \n",
      " <meta name=\"robots\" content=\"noindex,nofollow\"/>\n",
      " <meta name=\"pragma\" content=\"no-cache\"/>\n",
      "\t\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
      "\t\t<meta charset=\"utf-8\">\n",
      "\t\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "\t\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "\n",
      "\t\t<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->\n",
      "\t\t<meta name=\"description\" content=\"\">\n",
      "\t\t<meta name=\"author\" content=\"\">\n",
      "\t\t<link rel=\"icon\" href=\"favicon.ico\">\n",
      "\n",
      "\t\t<title>Erreur 404 | pole-emploi.fr, fusion des sites anpe.fr et assedic.fr</title>\n",
      "\t\t<link href=\"/css/pages-autonomes.css\" rel=\"stylesheet\">\n",
      "\t\t<!-- JS -->\n",
      "\t\t<script src=\"/js/jquery.min.js\"></script>\n",
      "\t\t<!--[if lt IE 9]>\n",
      "\t\t\t<script src=\"/js/ie8-svg-support.js\"></script>\n",
      "\t\t\t<script src=\"/js/html5shiv.min.js\"></script>\n",
      "\t\t\t<script src=\"/js/respond.min.js\"></script>\n",
      "\t\t<![endif]-->\n",
      "\t</head>\n",
      "\t<body class=\"candidat erreur\"> \n",
      "\t\t<div class=\"container small-container text-center\">\n",
      "\t\t\t<header role=\"banner\" class=\"text-center\">\t\n",
      "\t\t\t\t<a href=\"http://www.pole-emploi.fr\">\n",
      "\t\t\t\t\t<img src=\"/img/logo-pe.svg\" alt=\"Logo de PÃ´le emploi\" /><span class=\"sr-only\">Accueil PÃ´le emploi</span>\n",
      "\t\t\t\t</a>\n",
      "\t\t\t\t<h1 class=\"t2\">Page <span class=\"text-candidat\">introuvable</span></h1>\t\t\n",
      "\t\t\t</header>\n",
      "\t\t\t\n",
      "\t\t\t<hr aria-hidden=\"true\">\n",
      "\t\t\t<p class=\"t5\">La page demandÃ©e n'est malheureusement pas disponible</p>\n",
      "\t\t\t<a class=\"btn btn-primary btn-lg btn-block\" href=\"http://www.pole-emploi.fr\" role=\"button\">Retour</a>\t\n",
      "\t\t\t\t\n",
      "\t\t</div>\n",
      "\t\t<footer role=\"content-info\" class=\"footer center-block text-center\">\n",
      "\t\t\t<div class=\"container-fluid\">\n",
      "\t\t\t\t<small>&#169; 2016 POLE EMPLOI. Tous droits rÃ©servÃ©s.</small>\n",
      "\t\t\t</div>\n",
      "\t\t</footer>\n",
      "\t\t<!-- fin de la page -->\n",
      "\t</body>\t\n",
      "<script type=\"text/javascript\">\n",
      "xtnv = document;\n",
      "xtsd = \"http://logp6\";\n",
      "xtsite = \"475540\";\n",
      "xtn2 = \"10\";\n",
      "xtpage = \"api::Page_indisponible_(Erreur_404-candidat.html)\";\n",
      "xterr = \"\";\n",
      "xtmc = \"\";\n",
      "xtnp = \"\";\n",
      "xt_ac = \"\";\n",
      "xt_an = \"\";\n",
      "xtprm = \"\";\n",
      "roimt = \"\";\n",
      "roitest = false;\n",
      "visiteciblee = false;\n",
      "xtidmod = \"\";\n",
      "xtergo = \"0\";\n",
      "xt_multc = \"&amp;x1=Tapestry&amp;x2=Transverse&amp;x3=0\"; //all the xi indicators (like \"&x1=...&x2=....&x3=...\")\n",
      "</script>\n",
      "<script src=\"/js/xtclicks.js\" type=\"text/javascript\" ></script>\n",
      "<script src=\"/js/xtcore.js\" type=\"text/javascript\" ></script>\n",
      "<noscript>\n",
      "<img alt=\"\" height=\"1\" src=\"http://logp6.xiti.com/hit.xiti?s=475540&amp;s2=10&amp;p=api::Page_indisponible_(Erreur_404-candidat.html)&amp;roimt=&amp;roivc=&amp;x1=Tapestry&amp;x2=Transverse&amp;x3=0\" width=\"1\">\n",
      "</noscript>\n",
      "\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
