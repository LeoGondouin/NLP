{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import math\n",
    "from dateutil import parser\n",
    "import html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait,Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException,NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapCorpus(sources,keyword,nb_docs):\n",
    "\n",
    "    source = \"\"\n",
    "    position = \"\"\n",
    "    company = \"\"\n",
    "    workplace = \"\"\n",
    "    published_date = \"\"\n",
    "    contract_type = \"\"\n",
    "    long_infos = \"\"\n",
    "\n",
    "\n",
    "    if  \"emploi-territorial\" in sources:\n",
    "        pages = math.ceil(nb_docs/20)\n",
    "        source = \"emploi-territorial\"\n",
    "        rootLink = \"https://www.emploi-territorial.fr\"\n",
    "        # 20 offres par pages\n",
    "        # pages = math.ceil(nb_docs/20)\n",
    "        url = f\"{rootLink}/emploi-mobilite/?adv-search={keyword}&page={pages}\"\n",
    "        response = requests.get(url) \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        root = soup.find(\"body\")\n",
    "        offresLinkElems = root.select(\"div[class*='bloc-lien-offre'] > a[class*='lien-details-offre']\")[:nb_docs]    \n",
    "        links = [rootLink+offresLinkElem.get(\"href\") for offresLinkElem in offresLinkElems]\n",
    "\n",
    "        corpus = list(dict())\n",
    "\n",
    "        for link in links:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            root = soup.find(\"body\")\n",
    "\n",
    "            position = root.select(\"h2[class*='set-line-emploi']\")[0].text.strip()\n",
    "            company = root.select(\"div[class*='offre-item-value'] > strong > a\")[0].text.strip() if root.select(\"div[class*='offre-item-value'] > strong > a\") else \"NULL\"\n",
    "            workplace = root.select_one(\"div[class*='offre-item-label']:contains('Lieu de travail') + .offre-item-value\").text.strip() if root.select_one(\"div[class*='offre-item-label']:contains('Lieu de travail') + .offre-item-value\") else \"NULL\"\n",
    "            published_date = root.select_one(\"div[class*='px-3']:contains('Publiée le') > .set-color-green\").text.strip() if root.select_one(\"div[class*='px-3']:contains('Publiée le') > .set-color-green\") else \"NULL\"\n",
    "            contract_type = root.select_one('div[class*=\"offre-item-label\"]:contains(\"Type d\\'emploi\") + .offre-item-value').text.strip() if root.select_one('div[class*=\"offre-item-label\"]:contains(\"Type d\\'emploi\") + .offre-item-value') else \"NULL\"\n",
    "            position_type = root.select_one('div[class*=\"offre-item-label\"]:contains(\"Famille de métiers\") + .offre-item-value').text.split(\">\")[0].strip()\n",
    "            \n",
    "            long_text = root.select('div[class*=\"offre-item-text\"]')\n",
    "\n",
    "            long_infos = \" \".join([long_text[0].text.strip(),long_text[1].text.strip(),long_text[2].text.strip()])\n",
    "            current_offer = {\"source\": source, \"link\": rootLink, \"position\": position, \"position_type\": position_type,\n",
    "                \"company\": company, \"workplace\": workplace, \"published_date\": published_date,\n",
    "                \"contract_type\": contract_type, \"description\": long_infos}\n",
    "\n",
    "            # Check if the offer is already in the corpus based on a frozenset comparison\n",
    "            corpus.append(current_offer)\n",
    "    \n",
    "    if \"apec\" in sources:\n",
    "        pages = math.ceil(nb_docs/20)\n",
    "        source = \"apec\"\n",
    "        service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        rootLink = \"https://www.apec.fr\"\n",
    "        try:\n",
    "            for page in range(pages):    \n",
    "                try:\n",
    "                    driver.get(f'{rootLink}/candidat/recherche-emploi.html/emploi?motsCles={keyword}&page={pages}')\n",
    "                    # Initial find of elements\n",
    "                    try:\n",
    "                        deny_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#onetrust-reject-all-handler'))\n",
    "                        )\n",
    "\n",
    "                        deny_cookies_button.click()\n",
    "                    except TimeoutException:\n",
    "                        pass\n",
    "\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.card-offer\"))\n",
    "                    )\n",
    "\n",
    "                    # Getting docs left if last page to query\n",
    "                    if nb_docs%20 != 0 and page==pages-1: \n",
    "                        limit = nb_docs%20\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, 'div.card-offer')[:limit]\n",
    "                    else :\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, 'div.card-offer')\n",
    "                        \n",
    "                    for index in range(len(div_elements_to_click_list)):\n",
    "                        try:\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.card-offer\"))\n",
    "                            )\n",
    "                            # Re-find elements after navigating back\n",
    "                            div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, 'div.card-offer')\n",
    "\n",
    "                            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.card-offer\"))\n",
    "                            )\n",
    "                            # Check if the index is within the valid range\n",
    "\n",
    "                            div_elements_to_click = div_elements_to_click_list[index]\n",
    "                            company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                            contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                            workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                            published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                            # Scroll into view\n",
    "\n",
    "                            actions = ActionChains(driver)\n",
    "                            actions.move_to_element(div_elements_to_click).click().perform()\n",
    "\n",
    "                            # Wait for the child element to become present in the DOM\n",
    "                            WebDriverWait(driver, 20).until(\n",
    "                                EC.presence_of_element_located((By.XPATH, f\"//h4[text()='Descriptif du poste']\"))\n",
    "                            )\n",
    "                            # Get the page source after the click\n",
    "                            page_source = driver.page_source\n",
    "\n",
    "                            # Use Beautiful Soup to parse the page source\n",
    "                            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                            # Example: Retrieve the text of a specific element\n",
    "                            desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                            descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                            profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                            profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                            position = soup.select_one(\"h4:contains('Métier') + span\").text\n",
    "                            position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                            long_infos = \" \".join([descText,profileText])\n",
    "                            \n",
    "                            corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":position_type,\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "                        except Exception as e:\n",
    "                            # Print the exception for debugging purposes\n",
    "                            print(f\"Error: {e}\")\n",
    "\n",
    "                        finally:\n",
    "                            # Navigate back to the main page\n",
    "                            driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    if \"hellowork\" in sources:\n",
    "        pages = math.ceil(nb_docs/30)\n",
    "        source = \"hellowork\"\n",
    "        rootLink = \"https://www.hellowork.com\"\n",
    "        service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "        driver = webdriver.Chrome(service=service) \n",
    "        try:\n",
    "            for page in range(pages):    \n",
    "                try:\n",
    "                    print(page)\n",
    "                    driver.get(f'{rootLink}/fr-fr/emploi/recherche.html?k={keyword}&k_autocomplete=&l=France&l_autocomplete=&p={page+1}')\n",
    "                    # Initial find of elements\n",
    "                    try:\n",
    "                        accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#hw-cc-notice-accept-btn'))\n",
    "                        )\n",
    "\n",
    "                        accept_cookies_button.click()\n",
    "\n",
    "                        combobox = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, \"select[name='country']\"))\n",
    "                        )\n",
    "\n",
    "                        select = Select(combobox)\n",
    "                        # Select a specific item by visible text\n",
    "                        select.select_by_value(\"FR\")\n",
    "\n",
    "                        form = driver.find_element(By.CSS_SELECTOR,\"form[data-action*='service-adaptation']\")\n",
    "                        next_button = form.find_element(By.CSS_SELECTOR,\"button\")\n",
    "                        next_button.click()\n",
    "                    except TimeoutException:\n",
    "                        pass\n",
    "\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"button[data-cy='seeOffer']\"))\n",
    "                    )\n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    # Getting docs left if last page to query\n",
    "                    if nb_docs%20 != 0 and page==pages-1: \n",
    "                        limit = nb_docs%20\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, \"button[data-cy='seeOffer']\")[:limit]\n",
    "                    else :\n",
    "                        div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, \"button[data-cy='seeOffer']\")\n",
    "                        \n",
    "                    for index in range(len(div_elements_to_click_list)):\n",
    "                        try:\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"button[data-cy='seeOffer']\"))\n",
    "                            )\n",
    "\n",
    "                            # Re-find elements after navigating back\n",
    "                            div_elements_to_click_list = driver.find_elements(By.CSS_SELECTOR, \"button[data-cy='seeOffer']\")\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"button[data-cy='seeOffer']\"))\n",
    "                            )\n",
    "                            # Check if the index is within the valid range\n",
    "\n",
    "                            div_elements_to_click = div_elements_to_click_list[index]\n",
    "                            # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "                            # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                            # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                            # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                            # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                            # Scroll into view\n",
    "\n",
    "                            # actions = ActionChains(driver)\n",
    "                            # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "                            #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "                            div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "                            company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "                            company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "                            workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "                            published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "                            driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "                            # Wait for the child element to become present in the DOM\n",
    "                            WebDriverWait(driver, 20).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"span[data-cy='jobTitle']\"))\n",
    "                            )\n",
    "                            # Get the page source after the click\n",
    "                            page_source = driver.page_source\n",
    "\n",
    "                            # Use Beautiful Soup to parse the page source\n",
    "                            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                            # Example: Retrieve the text of a specific element\n",
    "                            # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                            # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                            # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                            # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                            position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "                            # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                            long_infos = soup.select(\"section\")\n",
    "                            infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "                            infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "                            infos = \" \".join([item for item in infos if item != ''])\n",
    "                            # infos = [item for item in infos if '' not in item]\n",
    "                            corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":infos})\n",
    "                        except Exception as e:\n",
    "                            # Print the exception for debugging purposes\n",
    "                            print(f\"Error: {e}\")\n",
    "\n",
    "                        finally:\n",
    "                            # Navigate back to the main page\n",
    "                            driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    " \n",
    "    if \"welcometothejungle\" in sources:\n",
    "        source = \"welcometothejungle\"\n",
    "        rootLink = \"https://www.welcometothejungle.com\"\n",
    "        service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        try:\n",
    "            for page in range(1,pages+1):    \n",
    "                try:\n",
    "                    driver.get(f'{rootLink}/fr/jobs?query=data&aroundQuery=France&page={page}')\n",
    "                    # Initial find of elements\n",
    "                    try:\n",
    "                        accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#axeptio_btn_acceptAll'))\n",
    "                        )\n",
    "\n",
    "                        accept_cookies_button.click()\n",
    "                    except TimeoutException:\n",
    "                        pass\n",
    "                        \n",
    "                    offers = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "                    )\n",
    "                    \n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    # Getting docs left if last page to query\n",
    "                    if nb_docs%30 != 0 and page==pages: \n",
    "                        limit = nb_docs%30\n",
    "                        div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")[:limit]\n",
    "                    else :\n",
    "                        div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "\n",
    "                    for index in range(len(div_elements_to_click_list)):\n",
    "                        try:\n",
    "                            offers = WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "                            )\n",
    "                            \n",
    "                            div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "                            div_elements_to_click = div_elements_to_click_list[index]\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "                            # Check if the index is within the valid range\n",
    "                            div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "                            position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "                            position = position_elem.text\n",
    "                            workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "                            contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "                            main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "                            contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "                            published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "                            published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "                            published_date = published_date.strftime('%d/%m/%Y')\n",
    "                            # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "                            # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                            # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                            # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                            # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                            # Scroll into view\n",
    "                            # actions = ActionChains(driver)\n",
    "                            # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "                            #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "                            # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "                            # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "                            # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                            # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "                            # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "                            # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "                            driver.execute_script(\"arguments[0].click();\", div_offer)\n",
    "                            WebDriverWait(driver, 20).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-testid='job-section-description']\"))\n",
    "                            )\n",
    "                            description_elem = driver.find_element(By.CSS_SELECTOR,\"div[data-testid='job-section-description']\")\n",
    "\n",
    "                            # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                            \n",
    "                            WebDriverWait(driver, 3)\n",
    "\n",
    "                            try:\n",
    "                                profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "                                profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                                profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "                            except NoSuchElementException:\n",
    "                                # If profile element is not found, set it to \"NULL\"\n",
    "                                profile = \"NULL\"\n",
    "                            # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "                            description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                            # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                            \n",
    "                            #parsing tags\n",
    "                            description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "                            company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "                            company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "                            long_infos = \" \".join([description,profile])\n",
    "                            # print(long_infos)\n",
    "                            # Wait for the child element to become present in the DOM\n",
    "\n",
    "                            # # Get the page source after the click\n",
    "                            # page_source = driver.page_source\n",
    "\n",
    "                            # Use Beautiful Soup to parse the page source\n",
    "                            # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                            # Example: Retrieve the text of a specific element\n",
    "                            # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                            # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                            # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                            # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                            # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "                            # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                            # long_infos = soup.select(\"section\")\n",
    "                            # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "                            # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "                            # infos = \" \".join([item for item in infos if item != ''])\n",
    "                            # infos = [item for item in infos if '' not in item]\n",
    "                            corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "                        except Exception as e:\n",
    "                            # Print the exception for debugging purposes\n",
    "                            print(f\"Error: {e}\")\n",
    "\n",
    "                        finally:\n",
    "                            # Navigate back to the main page\n",
    "                            driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    return(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "corpus = scrapCorpus(sources=[\"emploi-territorial\",\"hellowork\",\"welcometothejungle\"],keyword=\"data\",nb_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Officer - Data Factory H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'EURO-INFORMATION DEVELOPPEMENTS',\n",
       "  'workplace': 'Nantes - 44',\n",
       "  'published_date': '27/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Notre raison d'être : Ensemble, Ecouter et Agir.Vos missionsVous souhaitez travailler dans un environnement innovant et au sein d'une équipe à taille humaine ? La Data Factory est le département dédié à la donnée au sein d'Euro-Information au service de l'ensemble des entités du groupe et la diversité de leurs métiers pour la valorisation des données.- Nous fournissons les environnements (Vertica, Hadoop) et la technologie (Jupyter Lab pour python, R, SPSS, SAS, SAP BO ou IB Webfocus).- Nous les peuplons de données (plus de 90 billions de données et en extension).- Nous en assurons la sécurité et le respect de l'intimité numérique des clients.- Nous accompagnons les métiers du groupe dans l'utilisation des données au service de leurs clients, de la BI à la Data science. Nous regroupons des profils complémentaires qui travaillent ensemble. Data engineer, Data architect, Data scientists, Concepteurs BI, Data officers... Nous cherchons un(e) Data Officer.Nous avons une grande variété de projets, tant sur les métiers concernés (vente, gestion, fraude sur la banque, l'assurance, l'immobilier...) que sur les objectifs (scores prédictifs, reporting, connaissance ou administration de la donnée...). Vous pilotez et menez à bien des projets de valorisation de données :- Conduisez les projets de bout en bout : de l'expression du besoin jusqu'à la réalisation et au suivi de sa performance.- Mobilisez l'ensemble des acteurs : Business, Data scientists et/ou concepteurs BI, Data engineer...- Eclairez et participez aux prises de décision.- Mutualisez les travaux et les bonnes pratiques entre les acteurs et les différents métiers du groupe.Vous êtes l'un des gardiens des données :- Participez à l'administration des alimentations, usages et droits d'accès en contact avec les Data engineers, data architects et les métiers.- Portez une amélioration continue de ces process.- Pilotez des projets de connaissance et de qualité des données.- Participez à l'acculturation Data au sein du groupe, au sein de vos projets et au-delà.Ce que vous allez vivre chez nous- Télétravail (2 jours par semaine)- Rémunération fixe versée sur 13 mois- RTT- Intéressement, participation et abondement- Plan épargne entreprise et PERCO- Contrat de santé collectif- Prévoyance- Retraite supplémentaire prise en charge à 100% par l'employeur- Conditions bancaires et assurances préférentielles- Politique parentale avantageuseCe que nous allons aimer chez vousDe formation école d'ingénieur ou universitaire en statistiques, traitement de l'information, mathématiques appliquées et/ou informatique décisionnelle vous maitrisez un outil/langage décisionnel (Python, R, SAS, SPSS, SAP BO...). Vous avez le goût de la Data. Vous avez acquis une solide expérience dans les projets Data, BI ou Data science en tant que chef de projet. Une maîtrise de l'anglais sera un plus. Ce qui nous plaira le plus chez vous, c'est vous ! Organisé(e), motivé(e), ouvert(e) et force de proposition ? L'équipe vous attend.Autres informationsLe poste est à pourvoir à Villeneuve d'Ascq ou Nantes dès que possible. Bienvenue chez EURO-INFORMATION DEVELOPPEMENTS Qui sommes nousEuro-Information, filiale technologique de Crédit Mutuel Alliance Fédérale, conçoit, réalise, maintient et exploite un système d'information commun utilisé par le Groupe. Les activités de développement et de production informatique au niveau national et international sont assurées par environ 4000 salariés répartis sur plusieurs sites géographiques au niveau national : Strasbourg, Nancy, Dijon, Orléans, Lyon, Lille, Cergy, Val de Fontenay, Paris et Nantes. Première Banque à adopter le statut d'entreprise à mission, le Crédit Mutuel Alliance Fédérale s'investit et s'engage dans différentes missions sociales et environnementales :- L'accompagnement de tous par notre organisation coopérative et mutualiste reste au coeur de notre ADN.- La technologie au service de l'humain est une référence dans notre monde connecté.- La solidarité et l'éco-responsabilité deviennent des axes clés dans notre développement. En résumé : Data Officer - Data Factory H/F EURO-INFORMATION DEVELOPPEMENTS Postuler sur le site du recruteur Nantes - 44 CDI 40 000 - 60 000 EUR par an 🏠 Télétravail partiel Bac +5 Banque • Assurance • Finance 2 de plus 2 de moins Publiée le 27/12/2023 - Réf : 74238_3\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Officer - Data Factory H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'EURO-INFORMATION DEVELOPPEMENTS',\n",
       "  'workplace': \"Villeneuve-d'Ascq - 59\",\n",
       "  'published_date': '27/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Notre raison d'être : Ensemble, Ecouter et Agir.Vos missionsVous souhaitez travailler dans un environnement innovant et au sein d'une équipe à taille humaine ? La Data Factory est le département dédié à la donnée au sein d'Euro-Information au service de l'ensemble des entités du groupe et la diversité de leurs métiers pour la valorisation des données.- Nous fournissons les environnements (Vertica, Hadoop) et la technologie (Jupyter Lab pour python, R, SPSS, SAS, SAP BO ou IB Webfocus).- Nous les peuplons de données (plus de 90 billions de données et en extension).- Nous en assurons la sécurité et le respect de l'intimité numérique des clients.- Nous accompagnons les métiers du groupe dans l'utilisation des données au service de leurs clients, de la BI à la Data science. Nous regroupons des profils complémentaires qui travaillent ensemble. Data engineer, Data architect, Data scientists, Concepteurs BI, Data officers... Nous cherchons un(e) Data Officer.Nous avons une grande variété de projets, tant sur les métiers concernés (vente, gestion, fraude sur la banque, l'assurance, l'immobilier...) que sur les objectifs (scores prédictifs, reporting, connaissance ou administration de la donnée...). Vous pilotez et menez à bien des projets de valorisation de données :- Conduisez les projets de bout en bout : de l'expression du besoin jusqu'à la réalisation et au suivi de sa performance.- Mobilisez l'ensemble des acteurs : Business, Data scientists et/ou concepteurs BI, Data engineer...- Eclairez et participez aux prises de décision.- Mutualisez les travaux et les bonnes pratiques entre les acteurs et les différents métiers du groupe.Vous êtes l'un des gardiens des données :- Participez à l'administration des alimentations, usages et droits d'accès en contact avec les Data engineers, data architects et les métiers.- Portez une amélioration continue de ces process.- Pilotez des projets de connaissance et de qualité des données.- Participez à l'acculturation Data au sein du groupe, au sein de vos projets et au-delà.Ce que vous allez vivre chez nous- Télétravail (2 jours par semaine)- Rémunération fixe versée sur 13 mois- RTT- Intéressement, participation et abondement- Plan épargne entreprise et PERCO- Contrat de santé collectif- Prévoyance- Retraite supplémentaire prise en charge à 100% par l'employeur- Conditions bancaires et assurances préférentielles- Politique parentale avantageuseCe que nous allons aimer chez vousDe formation école d'ingénieur ou universitaire en statistiques, traitement de l'information, mathématiques appliquées et/ou informatique décisionnelle vous maitrisez un outil/langage décisionnel (Python, R, SAS, SPSS, SAP BO...). Vous avez le goût de la Data. Vous avez acquis une solide expérience dans les projets Data, BI ou Data science en tant que chef de projet. Une maîtrise de l'anglais sera un plus. Ce qui nous plaira le plus chez vous, c'est vous ! Organisé(e), motivé(e), ouvert(e) et force de proposition ? L'équipe vous attend.Autres informationsLe poste est à pourvoir à Villeneuve d'Ascq ou Nantes dès que possible. Bienvenue chez EURO-INFORMATION DEVELOPPEMENTS Qui sommes nousEuro-Information, filiale technologique de Crédit Mutuel Alliance Fédérale, conçoit, réalise, maintient et exploite un système d'information commun utilisé par le Groupe. Les activités de développement et de production informatique au niveau national et international sont assurées par environ 4000 salariés répartis sur plusieurs sites géographiques au niveau national : Strasbourg, Nancy, Dijon, Orléans, Lyon, Lille, Cergy, Val de Fontenay, Paris et Nantes. Première Banque à adopter le statut d'entreprise à mission, le Crédit Mutuel Alliance Fédérale s'investit et s'engage dans différentes missions sociales et environnementales :- L'accompagnement de tous par notre organisation coopérative et mutualiste reste au coeur de notre ADN.- La technologie au service de l'humain est une référence dans notre monde connecté.- La solidarité et l'éco-responsabilité deviennent des axes clés dans notre développement. En résumé : Data Officer - Data Factory H/F EURO-INFORMATION DEVELOPPEMENTS Villeneuve-d'Ascq - 59 CDI 40 000 - 60 000 EUR par an 🏠 Télétravail partiel Bac +5 Banque • Assurance • Finance 2 de plus 2 de moins Publiée le 27/12/2023 - Réf : 74238_70\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Technique H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'Utigroup',\n",
       "  'workplace': 'Dijon - 21',\n",
       "  'published_date': '25/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Vous intervenez activement sur la montée en puissance du Système d'Information Géographique interne de notre client en Banque, votre mission constiste à :- Réalisez les développements Python de nouvelles fonctionnalités en utilisant les librairies adaptées (Folium, GeoPanda, OpenStreetMap) et proposez de nouveaux usages.- Etudiez les différentes sources open data et qualification des données.- Intégrez de nouvelles sources de données (ETL Stambia ou script SQL) et construction d'automatisme d'intégration de données depuis un fichier open vers une BDD interne.- Réalisez des POC avec les utilisateurs sur notre notebook Jupyter Lab.- Participez à la réalisation de projet avec les utilisateurs métiers (atelier métier avec les utilisateurs, expression de besoin, étude de faisabilité, développement et suivi projet). Le profil recherché - Vous êtes titulaire d'un Bac +5.- Vous mâitrisez : Python, HP Vertica, Stambia, l'environnement SIG (Système d'Information Géographique).- Vous êtes autonome.- Vous avez de bonnes capacités d'analyse et êtes capables de gérer plusieurs sujets en parallèle.- Vous avez l'esprit d'équipe, des aptitudes à communiquer et le sens du service rendu. L'entreprise Rejoindre UTIGroup, c'est rejoindre une Société à taille humaine qui investit sur l'accompagnement de ses Consultants, et où la prise de décision est rapide.Certifiée ISO 9001/2015, UTIGroup a eu 35 ans en 2021.Solidité et transparence financière (notre société est cotée en Bourse depuis plus de 20 ansNotation ECO-Vadis exceptionnelle (79/100) : niveau PLATINIUM (RSE, éthique, social et achats responsables)UTIGroup respecte les textes internationaux, européens et la législation Française en matière de discrimination. Mais aussi... - Statut Cadre.- RTT.- Ticket restaurant.- Mutuelle.- Participation au transport.- Assurance décès - invalidité. En résumé : Data Technique H/F Utigroup Super recruteur Postuler Dijon - 21 CDI 40 000 - 46 000 EUR par an Bac +5 Média • Internet • Communication Distribution • Commerce de gros Secteur informatique • ESN Exp. 1 à 7 ans Exp. + 7 ans 6 de plus 6 de moins Publiée le 25/12/2023 - Réf : 617708/2022925 DT/21D\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Modeler H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'VOLT',\n",
       "  'workplace': 'Alpes-Maritimes - 06',\n",
       "  'published_date': '22/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Nous recherchons pour l'un de nos clients un Data modeler. Le poste est à pourvoir aux alentours de Nice.Vos missions :- Soutien renforcé sur la gestion des données- Modélisation du Data Warehouse (DWH)- Collaboration au sein de l'équipe de data engineering- Travaux axés sur Spotfire pour la Business Intelligence (BI)Votre environnement technique :- Data warehouses et la modélisation dimensionnelle (en étoile)- Méthode spécifique de modélisation et de conception de systèmes décisionnels Kimball- Avoir une bonne maîtrise du langage SQL- Idéalement, avoir des compétences en Sas Designer, Python et Excel- Méthodes Agiles notamment Kanban/Scrum Le profil recherché - Vous justifiez de minimum de 4 ans d'expérience professionnelle en tant que Data modeler.- Vous êtes un bon communiquant et appréciez travailler en équipe.- Vous êtes force de proposition et autonome.- Vous parlez couramment françaisDémarrage : ASAPLieu du poste : Alentours de NiceContrat : CDI Bienvenue chez VOLT A la fois Cabinet de recrutement & Société de conseil, VOLT (Innova Solutions) a une couverture nationale. Nous sommes basés à Sophia Antipolis, au coeur de la French Riviera, nous accompagnons nos clients dans toute la France et offrons à nos candidats des postes dans les secteurs de l'IT, des Télécoms et de l'Engineering.Nos clients ont accès à plus de 85 bureaux à travers le monde :- Aux États-Unis- En Europe (Belgique et Nice)- Aux Royaume-Uni- A Singapour- En Inde En résumé : Data Modeler H/F VOLT Postuler Alpes-Maritimes - 06 CDI Bac +2 Bac +3, Bac +4 Bac +5 Secteur informatique • ESN Exp. 1 à 7 ans Exp. + 7 ans 6 de plus 6 de moins Publiée le 22/12/2023 - Réf : 1838682/10659445 DM/06A\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Manager Data H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'Interima',\n",
       "  'workplace': 'Cagnes-sur-Mer - 06',\n",
       "  'published_date': 'il y a 7 heures',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Nous recrutons pour l'un de nos client un(e) Manager Data H/F en CDI,Vos missions :Votre responsabilité, en tant que Manager Data s'étendra aux missions suivantes :- Mettre en place les process et workflow « Master Data » en étroite collaboration avec des keys users- Être garant de la qualité de la donnée tout au long de son cycle de vie, notamment grâce aux outils appropriés- Construire le reporting et KPI dans PowerBI (Microsoft). Elaborer le plan des priorités, mettre en place le RACI des droits utilisateurs/destinataires. Gestion des actifs, espace de travail...- Construire la Data Warehouse / Data Lake (agrégation des données) - Mettre en place un ETL (gestion des flux des données)- Construire la vision de la donnée IA (Intelligence Artificielle, big data, simulations...) - Fournir les requêtes SQL selon le besoin des métiers Le profil recherché De formation Bac +3/+5 de type Business Intelligence, Data Analyste, PowerBI.Vous disposez d'une expérience réussie de d'environ 5 ans sur un poste similaire ou en ESN spécialisée- Vous disposez de bases solides en connaissance SQL. Connaissance de l'outil PowerBI est un prérequis.- Vous avez construit un environnement de stockage de données de type Data Warehouse, Data Lake...- Vous avez géré la diffusion des reporting/KPI avec l'aide de pack office365.- Vous êtes rigoureux·se, autonome, doté·e d'une vraie capacité d'analyse et de bonnes capacités rédactionnelles. Vous aimez travailler en équipe et savez faire preuve de coopération.Contrat 38h du Lundi au VendrediType d'emploi : Temps plein, CDI L'entreprise Le Groupe Interima est un réseau régional indépendant de travail temporaire, recrutement, formation et compétences.Nos équipes vous proposent des postes en intérim, CDD, CDI et CDI Intérimaires via nos sept bureaux Interima répartis sur l'ensemble des Alpes-Maritimes. En résumé : Manager Data H/F Interima Postuler Cagnes-sur-Mer - 06 CDI 60 000 - 65 000 EUR par an Bac +3, Bac +4 Services aux Personnes • Particuliers Services aux Entreprises Exp. 1 à 7 ans 4 de plus 4 de moins Publiée le 28/12/2023 - Réf : 1787875/10169130 MD/06C\"}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': 'Ingénieur data F/H',\n",
       "  'position_type': \"Informatique et système d'information\",\n",
       "  'company': 'CONSEIL DEPARTEMENTAL DU NORD',\n",
       "  'workplace': 'Lille cedex',\n",
       "  'published_date': '19/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Le Département du Nord place les systèmes d'information au cœur du projet de transformation digitale au service de l'usager et de ses partenaires. La Direction des Systèmes d'Information (environ 110 collaborateurs, 7000 postes de travail, 200 sites) poursuit le développement de projets innovants :\\n-\\tLa sécurisation de ses infrastructures, le développement du haut débit pour ses établissements, des plateformes téléphoniques, des systèmes de GED et de numérisation.\\n-\\tDe nombreux chantiers autour du poste de travail comme la mobilité en autres\\n-\\tLa mise en œuvre d'applications au service des compétences départementales, la solidarité, la mobilité, la jeunesse, le sport, la culture, le développement des territoires.\\n\\nAu sein de la DSI, vous êtes rattaché-e au service Pilotage et Sécurisation SI, qui anime les Consultants Internes SI, les ingénieur-es Sécurité et une cellule Data (Données SI). La cellule DATA initie la structuration de la donnée dans ses missions de définition et de pilotage d'un écosystème Data de la collectivité.\\nLes missions de la cellule Data (Données SI) :\\n*\\tSTRUCTURER : définir et piloter la mise en œuvre de l'écosystème Data de la collectivité, autour de la collecte, du stockage, du référencement, de la qualité, de la visualisation et la diffusion des données.\\n*\\tACCOMPAGNER l'usage : aider à faire parler les données et être un support aux équipes de pilotage des directions métiers. \\n*\\tCAPITALISER notre patrimoine de données : œuvrer au référencement et à la collecte des données à des fins de réemploi ; en premier lieu pour l'analyse des données (dont décisionnel) puis pour la circulation des données entre les systèmes.\\n\\nEn qualité d'ingénieur-e data, vous êtes le/la référent-e technique de la collectivité pour les projets d'intégration et de stockage des données et vous apportez l'expertise technique nécessaire pour le développement des solutions data appropriées. Pour ce faire, vous contribuez activement à :\\n*\\tDéfinir l'architecture décisionnelle cible et les besoins d'infrastructures nécessaires au stockage et à la valorisation des données.\\n*\\tConcevoir les projets d'industrialisation de traitement des données et superviser les équipes opérationnelles dans les projets d'intégration (process ETL, entrepôts de données...). \\n*\\tParticiper aux chantiers data structurants (qualité, référentiels et cartographie des données, urbanisation...).\\n*\\tGarantir la cohérence et la sécurité de l'écosystème décisionnel départemental.\\n*\\tAccompagner les directions métiers dans les projets décisionnels (recueil des besoins, modélisation des données...) et fournir aux équipes (data analystes) un appui technique à l'exploration complexe des données. \\nRelations professionnelles : \\nInterne : Relations directes avec l'équipe de direction et les services de la DSI, les secrétariats généraux et les autres directions de la collectivité (responsables et collaborateurs des dites directions).\\nExterne : Prestataires, éditeurs, autres collectivités, ministères. Savoir faire \\n Assistance à la maîtrise d'ouvrage opérationnelle \\n*\\tParticiper à la conduite du changement \\n*\\tDéfinir des spécifications fonctionnelles à partir de l'expression des besoins \\n\\nPilotage et conduite de projet d'informatisation \\n*\\tÉvaluer les enjeux et les risques (techniques, financiers, organisationnels) d'un projet informatique \\n*\\tÉlaborer le cahier des charges et le calendrier de réalisation \\n*\\tOpérer des choix techniques en matière de logiciels \\n*\\tOrganiser le déroulement du projet et planifier les travaux de développement \\n\\nMaintien en conditions opérationnelles des applications et plateformes (MCO) \\n*\\tAssurer l'assistance de niveau 3 (expertise, problèmes complexes, etc.) \\n*\\tAssurer la maintenance corrective \\n*\\tAssurer la maintenance évolutive et la gestion des changements (au sens ITIL)\\n\\nConception et intégration d'applications \\n*\\tRéaliser les spécifications fonctionnelles et techniques \\n*\\tMettre en œuvre des progiciels (paramétrage, reprise de données, interfaces, développements spécifiques, etc.) \\n*\\tRéaliser des tests des programmes et des prototypes \\n*\\tRédiger la documentation (guides, modes opératoires, etc.) \\n\\nAssistance et appui technique auprès des services de la collectivité \\n*\\tSensibiliser les services et diffuser des supports d'information \\n*\\tConduire une action de formation en interne \\n\\nVeille et observation sectorielle \\n*\\tEnrichir des bases documentaires et d'information \\n\\nGestion de la commande publique \\n*\\tÉlaborer les cahiers des charges et pièces du marché public * Définir des critères de sélection des offres \\n*\\tPréparer les dossiers des commissions d'appels d'offres \\n*\\tAnalyser les propositions techniques et financières des fournisseurs et entreprises \\n*\\tNégocier avec les fournisseurs et les entreprises \\n*\\tAttester le service fait \\n\\nElaboration et suivi du budget \\n*\\tPlanifier les besoins budgétaires et élaborer un budget prévisionnel \\n*\\tSuivre et contrôler l'exécution du budget \\n*\\tRenseigner des outils de pilotage et de suivi (tableaux de bord) \\n\\nContrôle de la qualité des services rendus \\n*\\tVérifier la conformité des prestations des entreprises avec les clauses techniques définies dans les pièces du marché \\n*\\tÉtablir des rapports et bilans d'activités \\n\\nContrôle et suivi des prestations effectuées par des tiers \\n*\\tRéceptionner et contrôler les projets, travaux et prestations fournis par des tiers \\n\\nConduite de projet \\n*\\tOrganiser et animer des groupes projet et des comités de pilotage \\n*\\tIdentifier et mobiliser les acteurs et les compétences nécessaires à la conduite d'un projet Savoirs  \\n*\\t Méthodes et outils de la planification \\n*\\tArchitecture et fonctionnalités des SI \\n*\\tRègles et aspects légaux des SI \\n*\\tTechniques de conception, modélisation et architecture d'applications \\n*\\tMarché de l'offre informatique \\n*\\tNormes et procédures de sécurité \\nFonctionnement de la collectivité et des services \\n*\\tTechniques d'élaboration de cahier des charges et de planification d'études  \\n*\\tTechniques d'analyses comparatives (benchmarking) \\n*\\tCode des marchés publics et modalités d'application \\n*\\tRègles et techniques d'expression écrite et rédactionnelles (notes, compte-rendu, rapports, etc.) \\n*\\tRègles relatives à l'accès aux documents administratifs \\n*\\tTechniques rédactionnelles de rapports et bilans d'activité \\n*\\tMéthodes et techniques de réception des travaux et prestations \\n\\n   Savoir - être \\n*\\tDisposer d'une bonne aisance relationnelle \\n*\\tDisposer de bonnes capacités d'analyse et de synthèse \\n*\\tSavoir travailler en transversalité \\n*\\tFaire preuve d'esprit d'équipe et de capacité à coopérer  \\n*\\tFaire preuve d'autonomie \\n*\\tAvoir de bonnes capacités d'écoute  \\n\\n Obligations du poste : \\n- Formation supérieure, idéalement bac+5 en informatique ou école d'ingénieurs, \\n- Une ou plusieurs expériences dans le développement de projets d'intégration de données\\n\\nConditions particulières : \\n - Déplacements possibles sur les sites \\n-  Fonction Télétravaillable\"},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': 'Ingénieur DATA H/F',\n",
       "  'position_type': 'Pilotage',\n",
       "  'company': 'CONSEIL DEPARTEMENTAL DE HAUTE-SAVOIE',\n",
       "  'workplace': 'Annecy',\n",
       "  'published_date': '15/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': 'Sous l\\'autorité du responsable de l\\'Unité Urbanisation et Intelligence de la donnée, l\\'Ingénieur data assure la disponibilité des différentes briques DATA de notre système d\\'information. En étroite collaboration avec les différents services de la collectivité, vous contribuez à la collecte, nettoyage et organisation des données provenant de différentes sources. Vous utilisez des outils et des techniques d\\'analyse de données pour extraire des informations pertinentes, identifier des tendances et des modèles, et effectuer des analyses approfondies. L\\'Ingénieur data travaille en étroite collaboration avec les équipes métier et les autres services de la DSI pour comprendre leurs besoins et proposer des solutions basées sur les données. Il surveille également les performances des modèles et des analyses, en effectuant des ajustements et des améliorations régulières.\\n\\nL\\'objectif global de l\\'Ingénieur data sera de s\\'assurer en priorité de la disponibilité des infrastructure et moteur de base de données, puis ensuite d\\'exploiter les données pour générer des connaissances et des informations exploitables, afin de soutenir la prise de décisions éclairées et d\\'améliorer la performance globale de la collectivité. Construire et faire évoluer le socle DATA\\nParticiper activement, en tant qu\\'expert, à la construction des entrepôts de donnée du CD74 qui contiendra une plateforme d\\'intégration, des conteneurs de données, un moteur d\\'indexation et de recherche, un bus de message, un gestionnaire de workflow, un API manager et des ETL (Talend) ;\\nParticiper activement à la modélisation de l\\'architecture Data global du CD74 (modélisation des flux et des ressources techniques associées) visant à rendre interopérables des volumes importants et hétérogènes de données de manière sécurisée ;\\nMaintenir en condition opérationnelle ces mêmes briques ;\\nFaire évoluer ce socle d\\'application orienté DATA.\\nGarantir la disponibilité des données du SI\\nAdministrer les bases de données du SI (Oracle, PostgreSQL, MySQL) ;\\nSauvegarder, restaurer ;\\nSuivre le versioning et le licensing des moteurs de bases de données ;\\nÊtre force de proposition d\\'axes d\\'amélioration (processus et technique) visant à professionnaliser/industrialiser la gestion des données du SI.\\nTransformer la performance en excellence, en contribuant aux projets d\\'efficience\\nComprendre les besoins métiers en s\\'appuyant sur les équipes et utilisateurs clés ;\\nParticiper à la réalisation des cahiers des charges ;\\nÊtre le point de contact privilégié des interlocuteurs métiers sur les sujets data ;\\nMener les tests en interactions avec les équipes de la DSI et les utilisateurs clés ;\\nTenir le bon niveau de discours apte à rendre les sujets Data compréhensibles par les différentes populations d\\'utilisateurs clés.\\nMesurer la performance, avec un état d\\'esprit \" data \" avec les compétences et connaissances appréciés suivantes\\nMaîtrise des solutions de bases de données (SQL, NoSQL...) ;\\nConnaissances en langages structurés (Javascript, Java, Python...) ;\\nAppétence pour l\\'IA et capacité à construire ou participer à construire des modèles de machine learning, Deep Learning et l\\'analyse de texte ;\\nForte expertise le stockage de données et les outils ETL tels que Talend, Business Objects ;\\nConnaissance en technologies du Big Data permettant le traitement et la manipulation de données (Hadoop, Spark, Kafka...) ;\\nConnaissance en Master Data Management, DataWarehouses et Data Lakes. Maitrise des moteurs de bases de données\\nUne bonne connaissance d\\'Oracle, de PostgreSQL et du langage SQL (requête et Scripting)\\nRédaction de la documentation sur les architectures déployées et les procédures techniques associées pour le MCO\\nCapacité à résoudre les problèmes techniques et à diagnostiquer les pannes liées aux bases de données\\nCapacité à documenter les procédures et les meilleures pratiques\\nNous recherchons une personne ayant le sens du travail en équipe, le sens du service et de l\\'organisation, et le sens du respect des procédures.\\n\\nVous êtes curieux(se), autonome et rigoureux(se) ? Vous savez relever les incohérences et faites attention au détail ?Vous avez su faire preuve d\\'organisation et de fiabilité au cours de vos précédentes expériences ? Vous êtes motivé(e) pour apprendre et acquérir de nouvelles compétences ? Alors ce poste est fait pour vous et le Département attend votre candidature avant le 15/01/2024'},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': 'Chef(fe) de projet DATA',\n",
       "  'position_type': \"Informatique et système d'information\",\n",
       "  'company': \"Conseil Départemental d'Eure-et-Loir\",\n",
       "  'workplace': 'Chartres cedex',\n",
       "  'published_date': '19/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Rémunération statutaire, régime indemnitaire, collectivité affiliée au cnas, tickets restaurant. Le Conseil départemental, c'est un collectif d'environ 2 000 collaborateurs qui œuvrent quotidiennement au service des Euréliens.\\n\\nLa Direction du numérique rassemble une quarantaine de collaborateurs et a pour objectif d'impulser et d'accompagner les changements professionnels, organisationnels et managériaux nécessaires à la transition numérique de la collectivité et du territoire.\\n\\nDans ce cadre, vous serez rattaché(e) au Service ingénierie des projets et encadrez l'ensemble  des projets de Big data, Data Intelligence et/ou Data Gouvernance.\\n\\nEn tant que responsable de tous les projets innovants dans le domaine de la donnée et de l'intelligence artificielle et de leur bonne réalisation, vous jouerez un rôle d'interface entre les toutes les Directions métiers mais aussi de toutes les parties prenantes du projet. En tant que Chef de projet DATA, vos principales missions seront de :\\n\\n    Déployer la stratégie de la Collectivité en matière d'analyse et de traitement de données ;\\n    Assurer la cohérence des actions des différents intervenants dans le cadre des plans d'actions liés à la gestion de la donnée ;\\n    Veiller à la conformité et à la bonne organisation des données dans les systèmes d'information, piloter et accompagner le déploiement de la solution auprès des utilisateurs ;\\n     Assurer une veille technologique ;\\n    Apporter des conseils au sujet de la restitution de données,\\n    Participer activement à l'activité Opendata\\n    Définir les rôles et responsabilités autour de la donnée IT et des référentiels attenants\\n    Mettre en place un contrôle qualité et une gouvernance sur ces données\\n    Valoriser ces données, faire en sorte qu'elles soient compréhensibles et exploitables\\n    Faciliter l'accès à ces données en respectant les exigences et standards de sécurité et conformité.\\n    Assurer le pilotage, la coordination et le suivi de projets numériques (état des lieux, définition des objectifs et suivi des délais, plan d'actions, coordination des acteurs, évaluation...),\\n    Contribuer à la traduction technique des besoins fonctionnels (analyse fonctionnelle, identification des opportunités et de la faisabilité technologique, évaluation des risques, sécurité du SI)\\n    Assurer une mission de conception et de développement (architecture logicielle, réalisation d'applications, cartographie de flux, structuration et documentation des bases de données)\"},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': \"UN(E) CHEF(FE) DE PROJET RESPONSABLE D'APPLICATIONS SI DATA ET DOCUMENTATION\",\n",
       "  'position_type': 'Mobilité, déplacements et transports',\n",
       "  'company': 'SYNDICAT MIXTE (SM) SYTRAL',\n",
       "  'workplace': 'LYON CEDEX 03',\n",
       "  'published_date': '15/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Au sein de la Direction des Expertises Techniques et du Patrimoine et du service systèmes transverses\\nPlacé(e) sous la responsabilité hiérarchique de la responsable du pôle informatique\\n\\nMISSIONS\\n* Piloter les activités de mise en œuvre, de maintenance et d'évolution des systèmes de collecte et d'exploitation des données des réseaux de transport en réponse aux besoins fonctionnels identifiés.\\n* Chargé(e) du bon fonctionnement quotidien des outils mis à la disposition des utilisateurs SYTRAL Mobilités, opérateurs et partenaires, ainsi que du suivi des usages de la plateforme Cloud hébergeant les données.\\n* Être le/la référent(e) technique sur les projets data et documentation en lien avec les métiers. 1- Mise en œuvre des solutions Data et Documentation,\\n- Piloter la mise en œuvre des projets sur son périmètre : cadrage des besoins, chiffrage, rédaction des cahiers des charges, pilotage des prestations, suivi des risques, validation des livrables et recettes en lien avec les acteurs métier,\\n- Veiller au respect des règles d'urbanisation et de sécurité SI sur les solutions mises en place,\\n- Garantir la qualité de la documentation des solutions et assurer la mise à jour de la cartographie des données et des flux d'alimentation sur son périmètre,\\n- Garantir le respect du budget des solutions Cloud par une surveillance régulière des consommations et le suivi d'alertes.\\n2- Fonctionnement quotidien des outils mis à disposition,\\n- Assurer la gestion des contrats de maintenance des applications du périmètre\\n- Être l'interface avec les éditeurs, intégrateurs ou équipe en charge d'une tierce maintenance applicative (TMA),\\n- Gérer les authentifications et droits sur les données et sur les infrastructures de la plateforme data (création de ressources, stockages, machines...),\\n- Gérer l'administration fonctionnelle et la maintenance évolutive des outils de restitution (PowerBi, BO etc...),\\n- Suivre les anomalies et prendre en charge des adaptations sur les applications du périmètre,\\n- Animer un plan d'amélioration continue et identifier des pistes d'optimisation,\\n- Suivre la qualité de service,\\n- Rédiger des procédures et animer les communautés d'utilisateurs.\\n\\n3- Contact technique sur les projets Data en lien avec les métiers,\\n- Contribuer aux études sur les nouvelles données à collecter, en lien avec les opérateurs et partenaires,\\n- Réaliser les analyses d'impacts (faisabilité, coûts, délais...) sur les demandes relatives à des fonctionnalités ou à de nouveaux algorithmes de calculs, en s'appuyant au besoin sur des expertises techniques spécifiques data/IA. Les savoirs\\n\\tIngénieur, Bac + 5 avec une expérience dans les domaines de la conduite de projets informatiques ou en tant que responsable d'applications, \\n\\tExpertise technico-fonctionnelle de solutions de Gestion Electronique Documentaire, de modélisation de workflow, de reporting et d'outils de manipulation des données (ETL),\\n\\tConnaissances des systèmes de gestion de bases de données, modèles relationnels ou objets et formats de stockages,\\n\\tConnaissances du fonctionnement des clouds publics,\\n\\tConnaissances en matière de marchés publics appréciées.\\n\\nLes savoir-être\\n\\tPragmatisme,\\n\\tRigueur,\\n\\tEsprit d'analyse et de synthèse,\\n\\tAisance relationnelle, facultés de communication, \\n\\tSens du service client.\\n\\nCONDITIONS DE TRAVAIL\\n\\n- Lieu : 21 Boulevard Vivier Merle 69003 LYON - à proximité de la gare Part-Dieu,\\n- Temps de travail : 35h (7h/j), 37h30 (7h30/j + 15 jours de RTT) ou 38h45 (7h45/j + 22 jours de RTT),\\n- Poste ouvert au télétravail jusqu'à 2 jours par semaine sans conditions d'ancienneté,\\n- Rémunération statutaire, RIFSEEP (IFSE et CIA), prime de fin d'année, tickets-restaurant, adhésion au Comité Social de la Métropole de Lyon (voyages, billetterie, culture, loisirs, aides...),\\n- Poste ouvert aux contractuels : il est possible de recruter sur ce poste une personne qui n'est pas lauréate d'un concours de la fonction publique.\"},\n",
       " {'source': 'emploi-territorial',\n",
       "  'link': 'https://www.emploi-territorial.fr',\n",
       "  'position': \"Un(e) chef(fe) de projet responsable d'applications SI Data et documentation\",\n",
       "  'position_type': \"Informatique et système d'information\",\n",
       "  'company': 'SYNDICAT MIXTE (SM) SYTRAL',\n",
       "  'workplace': 'LYON CEDEX 03',\n",
       "  'published_date': '15/12/2023',\n",
       "  'contract_type': \"Emploi permanent - création d'emploi\",\n",
       "  'description': \"Au sein de la Direction des Expertises Techniques et du Patrimoine et du service systèmes transverses\\nPlacé(e) sous la responsabilité hiérarchique de la responsable du pôle informatique\\n\\nMISSIONS\\n* Piloter les activités de mise en œuvre, de maintenance et d'évolution des systèmes de collecte et d'exploitation des données des réseaux de transport en réponse aux besoins fonctionnels identifiés.\\n* Chargé(e) du bon fonctionnement quotidien des outils mis à la disposition des utilisateurs SYTRAL Mobilités, opérateurs et partenaires, ainsi que du suivi des usages de la plateforme Cloud hébergeant les données.\\n* Être le/la référent(e) technique sur les projets data et documentation en lien avec les métiers. ACTIVITES PRINCIPALES\\n1- Mise en œuvre des solutions Data et Documentation,\\n- Piloter la mise en œuvre des projets sur son périmètre : cadrage des besoins, chiffrage, rédaction des cahiers des charges, pilotage des prestations, suivi des risques, validation des livrables et recettes en lien avec les acteurs métier,\\n- Veiller au respect des règles d'urbanisation et de sécurité SI sur les solutions mises en place,\\n- Garantir la qualité de la documentation des solutions et assurer la mise à jour de la cartographie des données et des flux d'alimentation sur son périmètre,\\n- Garantir le respect du budget des solutions Cloud par une surveillance régulière des consommations et le suivi d'alertes.\\n2- Fonctionnement quotidien des outils mis à disposition,\\n- Assurer la gestion des contrats de maintenance des applications du périmètre\\n- Être l'interface avec les éditeurs, intégrateurs ou équipe en charge d'une tierce maintenance applicative (TMA),\\n- Gérer les authentifications et droits sur les données et sur les infrastructures de la plateforme data (création de ressources, stockages, machines...),\\n- Gérer l'administration fonctionnelle et la maintenance évolutive des outils de restitution (PowerBi, BO etc...),\\n- Suivre les anomalies et prendre en charge des adaptations sur les applications du périmètre,\\n- Animer un plan d'amélioration continue et identifier des pistes d'optimisation,\\n- Suivre la qualité de service,\\n- Rédiger des procédures et animer les communautés d'utilisateurs.\\n3- Contact technique sur les projets Data en lien avec les métiers,\\n- Contribuer aux études sur les nouvelles données à collecter, en lien avec les opérateurs et partenaires,\\n- Réaliser les analyses d'impacts (faisabilité, coûts, délais...) sur les demandes relatives à des fonctionnalités ou à de nouveaux algorithmes de calculs, en s'appuyant au besoin sur des expertises techniques spécifiques data/IA. Les savoirs\\n\\tIngénieur, Bac + 5 avec une expérience dans les domaines de la conduite de projets informatiques ou en tant que responsable d'applications, \\n\\tExpertise technico-fonctionnelle de solutions de Gestion Electronique Documentaire, de modélisation de workflow, de reporting et d'outils de manipulation des données (ETL),\\n\\tConnaissances des systèmes de gestion de bases de données, modèles relationnels ou objets et formats de stockages,\\n\\tConnaissances du fonctionnement des clouds publics,\\n\\tConnaissances en matière de marchés publics appréciées.\\n\\nLes savoir-être\\n\\tPragmatisme,\\n\\tRigueur,\\n\\tEsprit d'analyse et de synthèse,\\n\\tAisance relationnelle, facultés de communication, \\n\\tSens du service client.\\nCONDITIONS DE TRAVAIL\\n\\n- Lieu : 21 Boulevard Vivier Merle 69003 LYON - à proximité de la gare Part-Dieu,\\n- Temps de travail : 35h (7h/j), 37h30 (7h30/j + 15 jours de RTT) ou 38h45 (7h45/j + 22 jours de RTT),\\n- Poste ouvert au télétravail jusqu'à 2 jours par semaine sans conditions d'ancienneté,\\n- Rémunération statutaire, RIFSEEP (IFSE et CIA), prime de fin d'année, tickets-restaurant, adhésion au Comité Social de la Métropole de Lyon (voyages, billetterie, culture, loisirs, aides...),\\n- Poste ouvert aux contractuels : il est possible de recruter sur ce poste une personne qui n'est pas lauréate d'un concours de la fonction publique.\"}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in corpus if item[\"source\"]==\"emploi-territorial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Modeler H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'VOLT',\n",
       "  'workplace': 'Alpes-Maritimes - 06',\n",
       "  'published_date': '22/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Nous recherchons pour l'un de nos clients un Data modeler. Le poste est à pourvoir aux alentours de Nice.Vos missions :- Soutien renforcé sur la gestion des données- Modélisation du Data Warehouse (DWH)- Collaboration au sein de l'équipe de data engineering- Travaux axés sur Spotfire pour la Business Intelligence (BI)Votre environnement technique :- Data warehouses et la modélisation dimensionnelle (en étoile)- Méthode spécifique de modélisation et de conception de systèmes décisionnels Kimball- Avoir une bonne maîtrise du langage SQL- Idéalement, avoir des compétences en Sas Designer, Python et Excel- Méthodes Agiles notamment Kanban/Scrum Le profil recherché - Vous justifiez de minimum de 4 ans d'expérience professionnelle en tant que Data modeler.- Vous êtes un bon communiquant et appréciez travailler en équipe.- Vous êtes force de proposition et autonome.- Vous parlez couramment françaisDémarrage : ASAPLieu du poste : Alentours de NiceContrat : CDI Bienvenue chez VOLT A la fois Cabinet de recrutement & Société de conseil, VOLT (Innova Solutions) a une couverture nationale. Nous sommes basés à Sophia Antipolis, au coeur de la French Riviera, nous accompagnons nos clients dans toute la France et offrons à nos candidats des postes dans les secteurs de l'IT, des Télécoms et de l'Engineering.Nos clients ont accès à plus de 85 bureaux à travers le monde :- Aux États-Unis- En Europe (Belgique et Nice)- Aux Royaume-Uni- A Singapour- En Inde En résumé : Data Modeler H/F VOLT Postuler Alpes-Maritimes - 06 CDI Bac +2 Bac +3, Bac +4 Bac +5 Secteur informatique • ESN Exp. 1 à 7 ans Exp. + 7 ans 6 de plus 6 de moins Publiée le 22/12/2023 - Réf : 1838682/10659445 DM/06A\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Technique H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'Utigroup',\n",
       "  'workplace': 'Dijon - 21',\n",
       "  'published_date': 'il y a 13 heures',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Vous intervenez activement sur la montée en puissance du Système d'Information Géographique interne de notre client en Banque, votre mission constiste à :- Réalisez les développements Python de nouvelles fonctionnalités en utilisant les librairies adaptées (Folium, GeoPanda, OpenStreetMap) et proposez de nouveaux usages.- Etudiez les différentes sources open data et qualification des données.- Intégrez de nouvelles sources de données (ETL Stambia ou script SQL) et construction d'automatisme d'intégration de données depuis un fichier open vers une BDD interne.- Réalisez des POC avec les utilisateurs sur notre notebook Jupyter Lab.- Participez à la réalisation de projet avec les utilisateurs métiers (atelier métier avec les utilisateurs, expression de besoin, étude de faisabilité, développement et suivi projet). Le profil recherché - Vous êtes titulaire d'un Bac +5.- Vous mâitrisez : Python, HP Vertica, Stambia, l'environnement SIG (Système d'Information Géographique).- Vous êtes autonome.- Vous avez de bonnes capacités d'analyse et êtes capables de gérer plusieurs sujets en parallèle.- Vous avez l'esprit d'équipe, des aptitudes à communiquer et le sens du service rendu. L'entreprise Rejoindre UTIGroup, c'est rejoindre une Société à taille humaine qui investit sur l'accompagnement de ses Consultants, et où la prise de décision est rapide.Certifiée ISO 9001/2015, UTIGroup a eu 35 ans en 2021.Solidité et transparence financière (notre société est cotée en Bourse depuis plus de 20 ansNotation ECO-Vadis exceptionnelle (79/100) : niveau PLATINIUM (RSE, éthique, social et achats responsables)UTIGroup respecte les textes internationaux, européens et la législation Française en matière de discrimination. Mais aussi... - Statut Cadre.- RTT.- Ticket restaurant.- Mutuelle.- Participation au transport.- Assurance décès - invalidité. En résumé : Data Technique H/F Utigroup Super recruteur Postuler Dijon - 21 CDI 40 000 - 46 000 EUR par an Bac +5 Média • Internet • Communication Distribution • Commerce de gros Secteur informatique • ESN Exp. 1 à 7 ans Exp. + 7 ans 6 de plus 6 de moins Publiée le 25/12/2023 - Réf : 617708/2022925 DT/21D\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Engineer - Big Data - Data Factory - Nantes H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'Sopra Steria',\n",
       "  'workplace': 'Nantes - 44',\n",
       "  'published_date': '23/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Votre environnement de travail :La division « Banque » s'est développée autour des métiers de la banque de détail, de la banque privée et des services financiers spécialisés. Nous participons à la révolution digitale grâce à notre expertise en automatisation des processus, Big Data, IA, Cloud. Nous accompagnons la transformation de nos clients en y associant nos compétences dans les domaines fonctionnels des Crédits, des Risques/Conformité et des Moyens de Paiement.Si vous êtes passionné(e) par la valorisation de la donnée, rejoignez notre Data Factory localisée à Nantes et les quelques 100 Data Ingénieurs qui la composent. Vous y rencontrerez des experts de la mise en oeuvre de Plateforme de Données, des Data Architectes ou autres experts solution autour des problématiques de valorisation de la donnée.Vous êtes accompagné(e) au développement de vos connaissances aux travers de différents parcours Data que ce soit pour l'ingestion, la construction de datahub, la transformation et valorisation de la donnée, la modélisation et mise à disposition. Rejoindre la Data Factory Sopra Steria, c'est rejoindre une communauté de Data Ingénieurs fiers de partager leur savoir et ouverts aux nouvelles expériences et expérimentations de la donnée.Votre rôle et vos missions :Dans le cadre de la mise en place de plateforme Data pour nos clients et selon votre expérience et votre appétence pour l'un de nos chapitres Data ci-dessous, vous participez à :- La compréhension des besoins métiers et la traduction solution de data ingénierie et ou data analysis ;- La mise en oeuvre de solution d'ingestion des données quelles soit en batch et/ou en streaming dans un contexte Cloud ;- La structuration du DataLake, la mise en place des processus de gouvernance et de sécurisation des données ;- Le traitement de la donnée jusqu'à l'exposition au métier ;- La mise en place de la chaine CI/CD et de sa supervision ;- La veille technologie avec nos partenaires éditeurs et la mise en place de Prototype Design, Proof of Concept ou encore MVP dans un objectif d'idéation pour nos clients.Environnement technique : Spark, Hadoop, Hive, Kafka, Elastic, Cloudera, Azure HD Insight, Informatica, Talend, Stambia, DataStage, SAS, DataIku, DataBricks, Qlik, SAP BI (BO), Power BI, Java, Scala, Python, R Le profil recherché Votre profil :Diplômé(e) d'une Ecole d'ingénieur ou formation équivalente, vous avez déjà participé à un projet Data (Big Data, BI) et vous avez une expérience de minimum 2 ans.Vous accordez une importance particulière au développement de vos compétences sur plusieurs technologies. Vous souhaitez une évolution réelle de carrière à travers l'expérience projet. Vous êtes soucieux de l'apport de valeur pour vos clients. Et vous voulez transmettre votre savoir auprès de collaborateurs moins expérimentés. Alors, n'attendez-plus, ce poste est fait pour vous ! Bienvenue chez Sopra Steria Sopra Steria, l'un des leaders européens de la Tech reconnu pour ses activités de conseil, de services numériques et d'édition de logiciels, aide ses clients à mener leur transformation digitale et à obtenir des bénéfices concrets et durables. Il apporte une réponse globale aux enjeux de compétitivité des grandes entreprises et organisations, combinant une connaissance approfondie des secteurs d'activité et des technologies innovantes à une approche résolument collaborative.Sopra Steria place l'humain au centre de son action et s'engage auprès de ses clients à tirer le meilleur parti du digital pour construire un avenir positif. Fort de 50 000 collaborateurs dans près de 30 pays, le Groupe a réalisé un chiffre d'affaires de 5, 1 milliards d'Euros en 2022.The world is how we shape IT. Mais aussi... - Un accord télétravail pour télétravailler jusqu'à 2 jours par semaine selon vos missions.- Un package avantages intéressant : une mutuelle, un CSE, des titres restaurants, un accord d'intéressement, des primes vacances et cooptation.- Un accompagnement individualisé avec un mentor.- Des opportunités de carrières multiples : plus de 30 familles de métiers, autant de passerelles à imaginer ensemble.- Plusieurs centaines de formations accessibles en toute autonomie depuis l'app mobile avec Sopra Steria Academy.- La possibilité de s'engager auprès de notre fondation ou de notre partenaire « Vendredi ».- L'opportunité de rejoindre le collectif Tech'Me UP (formations, conférences, veille, et bien plus encore...).Employeur inclusif et engagé, notre société oeuvre chaque jour pour lutter contre toute forme de discrimination et favoriser un environnement de travail respectueux. C'est pourquoi, attachés à la mixité et à la diversité, nous encourageons toutes les candidatures et tous les profils.https://www.soprasteria.fr/nous-connaitre/nos-engagements HelloWork a estimé le salaire pour ce métier à Nantes Le recruteur n'a pas communiqué le salaire de cette offre mais HelloWork vous propose une estimation (fourchette variable selon l'expérience). Estimation basée sur les données INSEE et les offres d’emploi similaires. Estimation basse 38 000 € par an Salaire brut estimé 42 500 € par an Estimation haute 47 500 € par an Cette information vous semble-t-elle utile ? Oui Non Merci pour votre retour ! En résumé : Data Engineer - Big Data - Data Factory - Nantes H/F Sopra Steria Postuler Nantes - 44 CDI 🏠 Télétravail occasionnel Bac +5 Secteur informatique • ESN Exp. + 7 ans 3 de plus 3 de moins Publiée le 23/12/2023 - Réf : 5abd22dc-057d-4f51-ab91-4defe9ed3bed\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Officer H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'SYNERGIE',\n",
       "  'workplace': \"Villeneuve-d'Ascq - 59\",\n",
       "  'published_date': '23/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Vos missionsEn tant que Data Officer, vous êtes l'un(e) des gardien(ne)s des données.Au sein du Data Office de la Direction de la Donnée de Synergie, vous rejoindrez une équipe de 5 personnes en charge de la gouvernance des données et des projets Data en lien avec les entités de Cofidis Group, la Data Factory (filière Data au sein d'Euro-Information) et le Crédit Mutuel Alliance Fédérale.Vous êtes un(e) acteur(trice) important(e) dans le développement de la culture des données.- Concernant la gouvernance et les outils, vous contribuez à l'élaboration et la diffusion des politiques de gouvernance des données au sein du groupe, en collaboration étroite avec le DPO. Vous assurez la cohérence avec la stratégie data, notamment sur l'exploitation responsable des données et la mutualisation de la préparation des données. Vous êtes force de proposition sur la démarche de data management (documentation du data catalogue & dictionnaire métier, supervision de la qualité des données).Pour nourrir les réflexions prospectives, vous êtes en veille sur les nouveaux outils du marché et vous portez les besoins d'évolution des outils de data management et de business intelligence au regard des enjeux métier.Vous animez et contrôlez les entités sur le respect des principes de gouvernance des données (connaissance du patrimoine de données, sécurité, usage économe et responsable...).- En tant que chef de projet, vous pilotez des projets pour les filiales de Cofidis Group. Ces projets peuvent concerner notamment la collecte de nouvelles sources de données, la mise en place d'outils de gouvernance des données, l'accompagnement sur des migrations...Vous coordonnez les différents acteurs et les actions du projet, du cadrage à la mise en production.Ce que vous allez vivre chez nousEn plus de votre rôle de Data Officer, vous pourrez participer à des projets métier, collaboratifs ou caritatifs qui mettront votre engagement au service de notre projet d'entreprise. Certifié « Great Place to Work » en 2022, en rejoignant Synergie, vous intégrerez un environnement où il fait bon travailler, et bénéficierez des avantages suivants :- Rémunération fixe sur 13 mois- Intéressement, participation et abondement- Plan d'Epargne Groupe et PERECOL- Mutuelle et prévoyance- Télétravail jusqu'à 2 jours par semaine- Chèques restaurant et restaurant d'entreprise- CESU et chèques vacances- Parking assuré et gratuit- CSE dynamiqueEt si vous souhaitez poursuivre votre carrière chez nous, et évoluer vers un autre métier, ou sur l'une des autres marques de Cofidis Group, c'est également possible.Ce que nous allons aimer chez vous- Vous êtes diplômé(e) d'une école d'ingénieur ou équivalent universitaire ou un autodidacte reconnu par ses pairs.- Vous justifiez de 3 ans ou plus d'exercice de missions dans la Data, en collaboration avec des experts Data Analyst ou Data Scientists.- Vous avez l'expérience du management de projets.- Vous maîtrisez l'Anglais (compréhension écrite et orale niveau B2).- Vous êtes autonome, force de proposition et vous avez un grand sens de l'organisation et une capacité à traiter plusieurs sujets en même temps avec persévérance et dynamisme.- Vous faîtes preuve de curiosité et vous appréciez contribuer à l'acculturation et l'accompagnement au changement.- Vos qualités de communication orale et écrite vous permettent de diffuser un message clair, synthétique et pédagogique.- Vous faîtes preuve d'un bon leadership et vous collaborez activement avec les autres membres du service et vos clients internes, en favorisant l'esprit d'équipe et le partage.CE QUI SERAIT UN VRAI PLUS !- Vous avez des compétences avancées en statistique ou informatique décisionnelle (idéalement dans le domaine bancaire ou assurance)40- Les langages de programmation tels que SQL, SAS, Python ne vous sont pas inconnus et vous avez une connaissance des principes d'architecture (SI décisionnel).Autres informationsPour postuler, merci de joindre votre CVManager recruteur : Anne-Laure TARTAR Bienvenue chez SYNERGIE Qui sommes nousCofidis Group crée, vend et gère une large gamme de services financiers pour les particuliers et les commerçants partenaires. Implanté dans 9 pays, pionnier du crédit à distance, Cofidis Group est depuis 40 ans l'un des principaux acteurs du crédit à la consommation en Europe (prêts personnels et crédits à la consommation, solutions de paiement, services bancaires, assurance, rachat de créances et partenariats). En France, Cofidis Group regroupe 3 enseignes commerciales, spécialistes dans la vente de produits et services financiers, et son GEIE Synergie : - Cofidis, spécialiste européen du crédit en ligne - Monabanq, la banque en ligne nouvelle génération - Creatis, spécialiste du regroupement de crédits à la consommation - Le GEIE Synergie qui apporte aux enseignes son expertise dans les domaines de la gestion du risque, du recouvrement /contentieux, des ressources humaines et de la communication, du juridique, du risk management, des fonctions financières/ comptables et organisationnelles.Pourquoi nous recrutonsChez Cofidis Group, nous avons entamé une transformation, sur notre organisation et notre business, afin de devenir un acteur complet et leader de l'expérience client, partenaire et collaborateur. Synergie, à travers ses métiers d'expertise, se positionne en tant qu'accélérateur de la transformation du groupe. Nous avons pour mission de stimuler la collaboration entre nos 3 entités commerciales et nos filiales à l'international, de fixer le cadre et de nous assurer de son respect et d'impulser et stimuler l'innovation. En parallèle, nous continuons à réaliser et développer des activités opérationnelles pour le compte de nos entités partenaires, nous accompagnons, challengeons les filiales et veillons à la mise en oeuvre des actions. C'est ensemble que nous voulons créer l'entreprise de demain et c'est pour cela que nous avons besoin de vous. En résumé : Data Officer H/F SYNERGIE Villeneuve-d'Ascq - 59 CDI 40 000 - 50 000 EUR par an 🏠 Télétravail partiel Bac +5 Banque • Assurance • Finance 2 de plus 2 de moins Publiée le 23/12/2023 - Réf : 80466_70\"},\n",
       " {'source': 'hellowork',\n",
       "  'link': 'https://www.hellowork.com',\n",
       "  'position': 'Data Officer H/F',\n",
       "  'position_type': 'NULL',\n",
       "  'company': 'ENGIE',\n",
       "  'workplace': 'Courbevoie - 92',\n",
       "  'published_date': '22/12/2023',\n",
       "  'contract_type': 'CDI',\n",
       "  'description': \"Les missions du poste Rejoins ENGIE IT et fais vivre l'IT au coeur de la transition énergétique en tant que :Data Officer (H/F)Poste basé à La Défense (92)Description du posteNous recherchons un data Officer pour accompagner les directions du Corporate sur des sujets innovants autour de l'Analytics, de la Datascience, de la data en général avec les qualifications ci-dessous.Interlocuteur Data privilégié du Corporate vous pourrez accompagner les directions du Corporate sur l'idéation et la réalisation de use case Data.Qualifications- Formation Bac +5 (école d'ingénieur ou master) dans un domaine relatif à la Data & à l'Analytics- Au moins 3 ans d'expérience en Analytics- Expérience sur un ou plusieurs cloud provider (AWS fortement apprécié, Azure, GCP)- Bonnes connaissances sur les architectures de données.- Expériences en Data Management- Expériences en Power BI- Maîtrise de l'anglais à l'écrit comme à l'oral- Nice to have :- A déjà travaillé sur des projets en mode agile- Expériences avec Datiku- Connaissance de Databricks- Connaissance de Palantir- Connaissance de CollibraCompétences comportementales- Aisance relationnelle- Rigueur et organisation- Savoir prendre des initiatives- Sens du résultat et de l'engagement- Pro activité- Esprit de Synthèse- Force de conviction- Pragmatisme- Curiosité et volonté d'apprendre Bienvenue chez ENGIE ENGIE IT apporte les meilleures solutions IT à l'ensemble des BU du Groupe ENGIE et les aide à relever tous les défis énergétiques d'aujourd'hui et de demain.Avec nous, tu pourras avoir à la fois l'agilité et la bienveillance d'une PME et la solidité et l'envergure d'un grand groupe, l'expertise et l'engagement, l'audace et l'excellence. En résumé : Data Officer H/F ENGIE Courbevoie - 92 CDI Bac +5 Secteur Energie • Environnement 2 de plus 2 de moins Publiée le 22/12/2023 - Réf : 10497\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, Date, ForeignKey,text,String\n",
    "from sqlalchemy.orm import declarative_base, Session, relationship\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Modele de bdd\n",
    "class HPositionType(Base):\n",
    "    __tablename__ = 'h_position_type'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    position_type = Column(String)\n",
    "    d_positions = relationship('DPosition', back_populates='h_position_type')\n",
    "\n",
    "class DPosition(Base):\n",
    "    __tablename__ = 'd_position'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    position = Column(String)\n",
    "    position_type_id = Column(Integer, ForeignKey('h_position_type.id'))\n",
    "    h_position_type = relationship('HPositionType', back_populates='d_positions')\n",
    "\n",
    "class DWebsite(Base):\n",
    "    __tablename__ = 'd_website'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    label = Column(String)\n",
    "    link = Column(String)\n",
    "\n",
    "class DCompany(Base):\n",
    "    __tablename__ = 'd_company'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    label = Column(String)\n",
    "\n",
    "class DCity(Base):\n",
    "    __tablename__ = 'd_city'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    city = Column(String)\n",
    "\n",
    "class DContractType(Base):\n",
    "    __tablename__ = 'd_contract_type'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    contract_type = Column(String)\n",
    "\n",
    "class DCalendar(Base):\n",
    "    __tablename__ = 'd_calendar'\n",
    "    date = Column(Date, primary_key=True)\n",
    "    day = Column(Integer)\n",
    "    month = Column(Integer)\n",
    "    year = Column(Integer)\n",
    "\n",
    "\n",
    "class FJobAdvertisements(Base):\n",
    "    __tablename__ = 'f_job_advertisements'\n",
    "    nb_occurences = Column(Integer)\n",
    "    contract_type_id = Column(Integer, ForeignKey('d_contract_type.id'),primary_key=True)\n",
    "    position_id = Column(Integer, ForeignKey('d_position.id'),primary_key=True)\n",
    "    website_id = Column(Integer, ForeignKey('d_website.id'),primary_key=True)\n",
    "    city_id = Column(Integer, ForeignKey('d_city.id'),primary_key=True)\n",
    "    company_id = Column(Integer, ForeignKey('d_company.id'),primary_key=True)\n",
    "    published_date = Column(Date, ForeignKey('d_calendar.date'),primary_key=True)\n",
    "\n",
    "    position = relationship('DPosition', back_populates='job_advertisements')\n",
    "    website = relationship('DWebsite', back_populates='job_advertisements')\n",
    "    company = relationship('DCompany', back_populates='job_advertisements')\n",
    "    city = relationship('DCity', back_populates='job_advertisements')\n",
    "    contract_type = relationship('DContractType', back_populates='job_advertisements')\n",
    "\n",
    "# Définition des relations\n",
    "DPosition.job_advertisements = relationship('FJobAdvertisements', back_populates='position')\n",
    "DWebsite.job_advertisements = relationship('FJobAdvertisements', back_populates='website')\n",
    "DCompany.job_advertisements = relationship('FJobAdvertisements', back_populates='company')\n",
    "DCity.job_advertisements = relationship('FJobAdvertisements', back_populates='city')\n",
    "DContractType.job_advertisements = relationship('FJobAdvertisements', back_populates='contract_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "#Seulement à executer une fois (contrainte avec la table de fait)\n",
    "def insertCalendar(yearsRange):\n",
    "    engine = create_engine('mysql+mysqlconnector://root:@localhost/job_scrapping')\n",
    "    Base.metadata.create_all(engine)\n",
    "    session = Session(bind=engine)\n",
    "    session.commit()\n",
    "    for year in yearsRange:\n",
    "        for month in range(1, 13):\n",
    "            num_days_in_month = (datetime(year, month % 12 + 1, 1) - timedelta(days=1)).day\n",
    "            for day in range(1, num_days_in_month + 1):\n",
    "                date_details = datetime(year, month, day)\n",
    "                date = DCalendar(date=date_details.strftime('%Y-%m-%d'),day=date_details.day,month=date_details.month,year=date_details.year)\n",
    "                session.add(date)\n",
    "                session.commit()\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertCalendar(range(2015,2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alimentation du DW\n",
    "def fillDW(corpus):\n",
    "    engine = create_engine('mysql+mysqlconnector://root:@localhost/job_scrapping')\n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    duplicates = False\n",
    "    #Vidage du DW\n",
    "    session = Session()\n",
    "    session.execute(text(\"CALL pTruncateDW()\"))\n",
    "    session.commit()\n",
    "    i=0\n",
    "    #Itération sur les éléments du corpus récupérés\n",
    "    for item in corpus:\n",
    "        corpus_position = item[\"position\"]\n",
    "        corpus_website = item[\"source\"]\n",
    "        print(corpus_position,corpus_website)\n",
    "        corpus_link = item[\"link\"]\n",
    "        corpus_company = item[\"company\"]\n",
    "        corpus_city = item[\"workplace\"]\n",
    "        corpus_contract_type = item[\"contract_type\"]\n",
    "        corpus_position_type = item['position_type']\n",
    "        corpus_published_date = item['published_date']\n",
    "        try:\n",
    "                date_object = datetime.strptime(corpus_published_date, '%d/%m/%Y')\n",
    "                # If successful, use the formatted date\n",
    "                corpus_published_date = date_object.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            corpus_published_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        # print(corpus_position)\n",
    "        # print(corpus_position_type)\n",
    "        \n",
    "        position_type = HPositionType(position_type=corpus_position_type)\n",
    "        position = DPosition(position=corpus_position)\n",
    "        website = DWebsite(label=corpus_website,link=corpus_link)\n",
    "        company = DCompany(label=corpus_company)\n",
    "        city = DCity(city=corpus_city)\n",
    "        contract_type = DContractType(contract_type=corpus_contract_type)\n",
    "        session = Session()\n",
    "\n",
    "        # Recherche des élements de chaque dimensions pour éviter les doublons\n",
    "        existing_position_type = session.query(HPositionType).filter_by(position_type=corpus_position_type).first()\n",
    "        existing_position = session.query(DPosition).filter_by(position=corpus_position).first()\n",
    "        existing_website = session.query(DWebsite).filter_by(label=corpus_website).first()\n",
    "        existing_company = session.query(DCompany).filter_by(label=corpus_company).first()\n",
    "        existing_city = session.query(DCity).filter_by(city=corpus_city).first()\n",
    "        existing_contract_type = session.query(DContractType).filter_by(contract_type=corpus_contract_type).first()\n",
    "\n",
    "        #Si elle existe je récupère la ligne existante, sinon j'insert la nouvelle ligne\n",
    "        if existing_position_type:\n",
    "            position_type = existing_position_type\n",
    "        else:\n",
    "            session.add(position_type)\n",
    "            session.commit()\n",
    "        if existing_position:\n",
    "            position = existing_position\n",
    "        else:\n",
    "            position.position_type_id = position_type.id\n",
    "            session.add(position)\n",
    "\n",
    "        if existing_website:\n",
    "            website = existing_website\n",
    "        else:\n",
    "            session.add(website)\n",
    "\n",
    "        if existing_company:\n",
    "            company = existing_company\n",
    "        else:\n",
    "            session.add(company)\n",
    "\n",
    "        if existing_city:\n",
    "            city = existing_city\n",
    "        else:\n",
    "            session.add(city)\n",
    "\n",
    "        if existing_contract_type:\n",
    "            contract_type = existing_contract_type\n",
    "        else:\n",
    "            session.add(contract_type)\n",
    "        session.commit()\n",
    "        \n",
    "        # J'insert les données dans la table de fait (id des dimensions + KPI)\n",
    "        job_advertisement = FJobAdvertisements(\n",
    "            nb_occurences=random.randint(3,10),\n",
    "            position=position,\n",
    "            website=website,\n",
    "            company=company,\n",
    "            city=city,\n",
    "            contract_type=contract_type,\n",
    "            published_date = corpus_published_date\n",
    "        )\n",
    "\n",
    "        session.add(job_advertisement)\n",
    "        try:\n",
    "            session.add(job_advertisement)\n",
    "            session.commit()\n",
    "            i = i + 1\n",
    "        except IntegrityError as e:\n",
    "            duplicates = True\n",
    "            session.rollback()\n",
    "        session.commit()\n",
    "    session.close()\n",
    "    if duplicates:\n",
    "        print(f\"Some exact duplicates have been detected in the job scrapping process, less documents have been saved then asked : {i} documents saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chef de projet Data (H/F) welcometothejungle\n",
      "Data Project Coordinator / Chargé(e) de données PIM DAM (F/H) welcometothejungle\n",
      "Data Analytics Engineer H/F/X welcometothejungle\n",
      "Manageur (euse) Data et Analytics welcometothejungle\n",
      "Data Center Security Manager, DC Security welcometothejungle\n",
      "Senior Data Scientist welcometothejungle\n",
      "Consultant.e Data Marketing Confirmé.e welcometothejungle\n",
      "Data Analyst confirmé - (F/H) welcometothejungle\n",
      "Scrum Master Data F/H welcometothejungle\n",
      "Data Privacy Analyst F/H welcometothejungle\n",
      "DATA ANALYST EXPERT POWER BI welcometothejungle\n",
      "Data Engineer - Expert Big Data H/F welcometothejungle\n",
      "Auditeur expérimenté Outils, Data & Méthodologie F/H welcometothejungle\n",
      "Ingénieur Big Data - Banque - Bordeaux welcometothejungle\n",
      "Consultant junior Data x Business Consulting H/F - Stage de fin d'études welcometothejungle\n",
      "Data Analyst (Stage de fin d'étude) welcometothejungle\n",
      "Product Owner Data Marketing H/F/X welcometothejungle\n",
      "Data engineer F/H welcometothejungle\n",
      "Ingénieur / Ingénieure Océano-météo / Data science welcometothejungle\n",
      "Data Engineer Intern welcometothejungle\n",
      "Data Science Intern welcometothejungle\n",
      "Data Scientist Finance, Risque et Performance (H/F) – CDI – Paris welcometothejungle\n",
      "STAGE - Ingénieur - Data Scientist - F/H welcometothejungle\n",
      "Senior Data Scientist welcometothejungle\n",
      "Data scientist chez OrdoSafe start-up incubée chez Matrice welcometothejungle\n",
      "ARCHITECTE DATA - H/F welcometothejungle\n",
      "Business unit manager- SaaS/Digital/Data welcometothejungle\n",
      "Stage - Data analyst risques opérationnels F/H welcometothejungle\n",
      "DATA Analyst H/F welcometothejungle\n",
      "Team Lead Data Science (F/M) welcometothejungle\n",
      "Data Analyst Supply Chain F/H welcometothejungle\n",
      "Senior Data Engineer welcometothejungle\n",
      "Intern Data Analyst welcometothejungle\n"
     ]
    }
   ],
   "source": [
    "fillDW(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"welcometothejungle\"\n",
    "rootLink = \"https://www.welcometothejungle.com\"\n",
    "service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service) \n",
    "keyword=\"data\"\n",
    "pages=2\n",
    "nb_docs=33\n",
    "corpus = list()\n",
    "try:\n",
    "    for page in range(1,pages+1):    \n",
    "        try:\n",
    "            driver.get(f'{rootLink}/fr/jobs?query=data&aroundQuery=France&page={page}')\n",
    "            # Initial find of elements\n",
    "            try:\n",
    "                accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#axeptio_btn_acceptAll'))\n",
    "                )\n",
    "\n",
    "                accept_cookies_button.click()\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "                \n",
    "            offers = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "            )\n",
    "            \n",
    "            # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Getting docs left if last page to query\n",
    "            if nb_docs%30 != 0 and page==pages: \n",
    "                limit = nb_docs%30\n",
    "                div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")[:limit]\n",
    "            else :\n",
    "                div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "\n",
    "            for index in range(len(div_elements_to_click_list)):\n",
    "                try:\n",
    "                    offers = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"ol[data-testid='search-results']\"))\n",
    "                    )\n",
    "                    \n",
    "                    div_elements_to_click_list = offers.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "                    div_elements_to_click = div_elements_to_click_list[index]\n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "                    # Check if the index is within the valid range\n",
    "                    div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "                    position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "                    position = position_elem.text\n",
    "                    workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "                    contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "                    main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "                    contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "                    published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "                    published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "                    published_date = published_date.strftime('%d/%m/%Y')\n",
    "                    # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "                    # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "                    # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "                    # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "                    # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "                    # Scroll into view\n",
    "                    # actions = ActionChains(driver)\n",
    "                    # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "                    #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "                    # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "                    # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                    # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "                    # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "                    # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "                    # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "                    # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "                    driver.execute_script(\"arguments[0].click();\", div_offer)\n",
    "                    WebDriverWait(driver, 20).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-testid='job-section-description']\"))\n",
    "                    )\n",
    "                    description_elem = driver.find_element(By.CSS_SELECTOR,\"div[data-testid='job-section-description']\")\n",
    "\n",
    "                    # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                    \n",
    "                    WebDriverWait(driver, 3)\n",
    "\n",
    "                    try:\n",
    "                        profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "                        profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                        profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "                    except NoSuchElementException:\n",
    "                        # If profile element is not found, set it to \"NULL\"\n",
    "                        profile = \"NULL\"\n",
    "                    # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "                    description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    \n",
    "                    #parsing tags\n",
    "                    description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "                    company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "                    company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "                    long_infos = \" \".join([description,profile])\n",
    "                    # print(long_infos)\n",
    "                    # Wait for the child element to become present in the DOM\n",
    "\n",
    "                    # # Get the page source after the click\n",
    "                    # page_source = driver.page_source\n",
    "\n",
    "                    # Use Beautiful Soup to parse the page source\n",
    "                    # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                    # Example: Retrieve the text of a specific element\n",
    "                    # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "                    # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "                    # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "                    # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "                    # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "                    # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "                    # long_infos = soup.select(\"section\")\n",
    "                    # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "                    # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "                    # infos = \" \".join([item for item in infos if item != ''])\n",
    "                    # infos = [item for item in infos if '' not in item]\n",
    "                    corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "                except Exception as e:\n",
    "                    # Print the exception for debugging purposes\n",
    "                    print(f\"Error: {e}\")\n",
    "\n",
    "                finally:\n",
    "                    # Navigate back to the main page\n",
    "                    driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print the exception for debugging purposes\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Intérim\n",
      "Error: name 'description_elem' is not defined\n",
      "Error: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=120.0.6099.130)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0BE59A]\n",
      "\t(No symbol) [0x00007FF75A0B59E9]\n",
      "\t(No symbol) [0x00007FF75A0B5AF3]\n",
      "\t(No symbol) [0x00007FF75A0B4298]\n",
      "\t(No symbol) [0x00007FF75A0B732A]\n",
      "\t(No symbol) [0x00007FF75A12B12B]\n",
      "\t(No symbol) [0x00007FF75A1120AA]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n",
      "Error: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=120.0.6099.130)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0BE59A]\n",
      "\t(No symbol) [0x00007FF75A0B59E9]\n",
      "\t(No symbol) [0x00007FF75A0B5AF3]\n",
      "\t(No symbol) [0x00007FF75A0B4298]\n",
      "\t(No symbol) [0x00007FF75A0B732A]\n",
      "\t(No symbol) [0x00007FF75A12B12B]\n",
      "\t(No symbol) [0x00007FF75A1120AA]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n",
      "Error: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=120.0.6099.130)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0BE59A]\n",
      "\t(No symbol) [0x00007FF75A0B59E9]\n",
      "\t(No symbol) [0x00007FF75A0B5AF3]\n",
      "\t(No symbol) [0x00007FF75A0B4298]\n",
      "\t(No symbol) [0x00007FF75A0B732A]\n",
      "\t(No symbol) [0x00007FF75A12B12B]\n",
      "\t(No symbol) [0x00007FF75A1120AA]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n",
      "Error: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF75A2882B2+55298]\n",
      "\t(No symbol) [0x00007FF75A1F5E02]\n",
      "\t(No symbol) [0x00007FF75A0B05AB]\n",
      "\t(No symbol) [0x00007FF75A0F175C]\n",
      "\t(No symbol) [0x00007FF75A0F18DC]\n",
      "\t(No symbol) [0x00007FF75A12CBC7]\n",
      "\t(No symbol) [0x00007FF75A1120EF]\n",
      "\t(No symbol) [0x00007FF75A12AAA4]\n",
      "\t(No symbol) [0x00007FF75A111E83]\n",
      "\t(No symbol) [0x00007FF75A0E670A]\n",
      "\t(No symbol) [0x00007FF75A0E7964]\n",
      "\tGetHandleVerifier [0x00007FF75A600AAB+3694587]\n",
      "\tGetHandleVerifier [0x00007FF75A65728E+4048862]\n",
      "\tGetHandleVerifier [0x00007FF75A64F173+4015811]\n",
      "\tGetHandleVerifier [0x00007FF75A3247D6+695590]\n",
      "\t(No symbol) [0x00007FF75A200CE8]\n",
      "\t(No symbol) [0x00007FF75A1FCF34]\n",
      "\t(No symbol) [0x00007FF75A1FD062]\n",
      "\t(No symbol) [0x00007FF75A1ED3A3]\n",
      "\tBaseThreadInitThunk [0x00007FF90948257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF90A8CAA58+40]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(div_elements_to_click_list)):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma[id=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhypViewJob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         div_elements_to_click_list \u001b[38;5;241m=\u001b[39m offers\n\u001b[0;32m     37\u001b[0m         div_elements_to_click \u001b[38;5;241m=\u001b[39m div_elements_to_click_list[index]\n",
      "File \u001b[1;32mc:\\Users\\leogo\\miniconda3\\envs\\text-mining\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:86\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     84\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Problème toutes les entreprises sont Adecco, si autre, redirection sur un autre site\n",
    "# source = \"adecco\"\n",
    "# rootLink = \"https://www.adecco.fr\"\n",
    "# service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=service) \n",
    "# keyword=\"data\"\n",
    "# pages=1\n",
    "# nb_docs=33\n",
    "# corpus = list()\n",
    "# try:\n",
    "#     for page in range(1,pages+1):    \n",
    "#         try:\n",
    "#             print(page)\n",
    "#             driver.get(f'{rootLink}/resultats-offres-emploi/m-{keyword}?pageNum={page}')\n",
    "#             # Initial find of elements\n",
    "                \n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"a[id='hypViewJob']\"))\n",
    "#             )\n",
    "\n",
    "#             offers = driver.find_elements(By.CSS_SELECTOR, \"a[id='hypViewJob']\")\n",
    "\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#             # Getting docs left if last page to query\n",
    "#             if nb_docs%50!= 0 and page==pages: \n",
    "#                 limit = nb_docs%50\n",
    "#                 div_elements_to_click_list = offers[:limit]\n",
    "#             else :\n",
    "#                 div_elements_to_click_list = offers\n",
    "#             # print(len(div_elements_to_click_list))\n",
    "#             for index in range(len(div_elements_to_click_list)):\n",
    "#                 try:\n",
    "#                     WebDriverWait(driver, 10).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"a[id='hypViewJob']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     div_elements_to_click_list = offers\n",
    "#                     div_elements_to_click = div_elements_to_click_list[index]\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "#                     # Check if the index is within the valid range\n",
    "#                     # div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "#                     # position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "#                     # position = position_elem.text\n",
    "#                     # workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "#                     # contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "#                     # main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "#                     # contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "#                     # published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "#                     # published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "#                     # published_date = published_date.strftime('%d/%m/%Y')\n",
    "#                     # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "#                     # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "#                     # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "#                     # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "#                     # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "#                     # Scroll into view\n",
    "#                     # actions = ActionChains(driver)\n",
    "#                     # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "#                     #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "#                     # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "#                     # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "#                     # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "#                     driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "#                     WebDriverWait(driver, 20).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='media-body']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     workplace = driver.find_element(By.CSS_SELECTOR,\"span[id='lblCity']\").text\n",
    "#                     contract_type_elem = driver.find_element(By.CSS_SELECTOR,\"span[id='ltEmploymentType']\")\n",
    "#                     contract_type = contract_type_elem.find_element(By.CSS_SELECTOR,\"a\").text\n",
    "                    \n",
    "#                     # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                    \n",
    "#                     WebDriverWait(driver, 3)\n",
    "\n",
    "#                     try:\n",
    "#                         profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "#                         profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                         profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If profile element is not found, set it to \"NULL\"\n",
    "#                         profile = \"NULL\"\n",
    "#                     # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "#                     description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                     # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    \n",
    "#                     #parsing tags\n",
    "#                     # description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "#                     # company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "#                     # long_infos = \" \".join([description,profile])\n",
    "#                     # print(long_infos)\n",
    "#                     # Wait for the child element to become present in the DOM\n",
    "\n",
    "#                     # # Get the page source after the click\n",
    "#                     # page_source = driver.page_source\n",
    "\n",
    "#                     # Use Beautiful Soup to parse the page source\n",
    "#                     # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "#                     # Example: Retrieve the text of a specific element\n",
    "#                     # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "#                     # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "#                     # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "#                     # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "#                     # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "#                     # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "#                     # long_infos = soup.select(\"section\")\n",
    "#                     # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "#                     # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "#                     # infos = \" \".join([item for item in infos if item != ''])\n",
    "#                     # infos = [item for item in infos if '' not in item]\n",
    "#                     corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "#                 except Exception as e:\n",
    "#                     # Print the exception for debugging purposes\n",
    "#                     print(f\"Error: {e}\")\n",
    "\n",
    "#                 finally:\n",
    "#                     # Navigate back to the main page\n",
    "#                     driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             # Print the exception for debugging purposes\n",
    "#             print(f\"Error: {e}\")\n",
    "\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m         deny_cookies_button \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbutton#pecookies-continue-btn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         deny_cookies_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
      "File \u001b[1;32mc:\\Users\\leogo\\miniconda3\\envs\\text-mining\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:86\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     84\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Problème toutes les entreprises sont Adecco, si autre, redirection sur un autre site\n",
    "# source = \"jobintree\"\n",
    "# rootLink = \"https://www.jobintree.com\"\n",
    "# service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=service) \n",
    "# keyword=\"data\"\n",
    "# pages=1\n",
    "# nb_docs=33\n",
    "# corpus = list()\n",
    "# try:\n",
    "#     for page in range(pages):    \n",
    "#         try:\n",
    "#             driver.get(f'{rootLink}/emploi?keywords={keyword}&page={page}')\n",
    "#             # Initial find of elements\n",
    "                \n",
    "#             offers = WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='annonces_normales']\"))\n",
    "#             )\n",
    "\n",
    "#             offers = offers.find_elements(By.CSS_SELECTOR,\"a\")\n",
    "#             print(len(offers))\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#             # Getting docs left if last page to query\n",
    "#             if nb_docs%20!= 0 and page==pages-1: \n",
    "#                 limit = nb_docs%20\n",
    "#                 div_elements_to_click_list = offers[:limit]\n",
    "#             else :\n",
    "#                 div_elements_to_click_list = offers\n",
    "#             # print(len(div_elements_to_click_list))\n",
    "#             for index in range(len(div_elements_to_click_list)):\n",
    "#                 try:\n",
    "#                     WebDriverWait(driver, 10).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"a[id='hypViewJob']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     div_elements_to_click_list = offers\n",
    "#                     div_elements_to_click = div_elements_to_click_list[index]\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                     # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "#                     # Check if the index is within the valid range\n",
    "#                     # div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "#                     # position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "#                     # position = position_elem.text\n",
    "#                     # workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "#                     # contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "#                     # main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "#                     # contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "#                     # published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "#                     # published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "#                     # published_date = published_date.strftime('%d/%m/%Y')\n",
    "#                     # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "#                     # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "#                     # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "#                     # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "#                     # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "#                     # Scroll into view\n",
    "#                     # actions = ActionChains(driver)\n",
    "#                     # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "#                     #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "#                     # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "#                     # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#                     # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "#                     # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "#                     # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "#                     driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "#                     WebDriverWait(driver, 20).until(\n",
    "#                         EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='media-body']\"))\n",
    "#                     )\n",
    "                    \n",
    "#                     workplace = driver.find_element(By.CSS_SELECTOR,\"span[id='lblCity']\").text\n",
    "#                     contract_type_elem = driver.find_element(By.CSS_SELECTOR,\"span[id='ltEmploymentType']\")\n",
    "#                     contract_type = contract_type_elem.find_element(By.CSS_SELECTOR,\"a\").text\n",
    "                    \n",
    "#                     # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "                    \n",
    "#                     WebDriverWait(driver, 3)\n",
    "\n",
    "#                     try:\n",
    "#                         profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "#                         profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                         profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If profile element is not found, set it to \"NULL\"\n",
    "#                         profile = \"NULL\"\n",
    "#                     # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "#                     description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#                     # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "                    \n",
    "#                     #parsing tags\n",
    "#                     # description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "#                     # company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "#                     # company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "#                     # long_infos = \" \".join([description,profile])\n",
    "#                     # print(long_infos)\n",
    "#                     # Wait for the child element to become present in the DOM\n",
    "\n",
    "#                     # # Get the page source after the click\n",
    "#                     # page_source = driver.page_source\n",
    "\n",
    "#                     # Use Beautiful Soup to parse the page source\n",
    "#                     # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "#                     # Example: Retrieve the text of a specific element\n",
    "#                     # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "#                     # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "#                     # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "#                     # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "#                     # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "#                     # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "#                     # long_infos = soup.select(\"section\")\n",
    "#                     # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "#                     # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "#                     # infos = \" \".join([item for item in infos if item != ''])\n",
    "#                     # infos = [item for item in infos if '' not in item]\n",
    "#                     corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "#                 except Exception as e:\n",
    "#                     # Print the exception for debugging purposes\n",
    "#                     print(f\"Error: {e}\")\n",
    "\n",
    "#                 finally:\n",
    "#                     # Navigate back to the main page\n",
    "#                     driver.execute_script(\"window.history.go(-1);\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             # Print the exception for debugging purposes\n",
    "#             print(f\"Error: {e}\")\n",
    "\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = \"pole-emploi\"\n",
    "# rootLink = \"https://candidat.pole-emploi.fr\"\n",
    "# service = ChromeService(\"C:/Users/leogo/Documents/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "# driver = webdriver.Chrome(service=service) \n",
    "# keyword=\"data\"\n",
    "# pages=2\n",
    "# nb_docs=33\n",
    "# corpus = list()\n",
    "# try:\n",
    "#     driver.get(f'{rootLink}/offres/recherche?motsCles={keyword}&range=0-{nb_docs-1}')\n",
    "#     # Initial find of elements\n",
    "#     try:\n",
    "#         accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "#             EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#pecookies-accept-all'))\n",
    "#         )\n",
    "#         print(\"test\")\n",
    "#         accept_cookies_button.click()\n",
    "#     except TimeoutException:\n",
    "#         pass\n",
    "#     WebDriverWait(driver, 10).until(\n",
    "#     EC.presence_of_element_located((By.CSS_SELECTOR, \"a[class='media with-fav']\"))\n",
    "#     )\n",
    "\n",
    "#     offers = driver.find_elements(By.CSS_SELECTOR, \"a[class='media with-fav']\")\n",
    "\n",
    "#     div_elements_to_click_list = offers\n",
    "#     # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#     # Getting docs left if last page to query\n",
    "#     # print(len(div_elements_to_click_list))\n",
    "#     for index in range(len(div_elements_to_click_list)):\n",
    "#         try:\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"a[class='media with-fav']\"))\n",
    "#             )\n",
    "#             offers = driver.find_elements(By.CSS_SELECTOR, \"a[class='media with-fav']\")\n",
    "\n",
    "#             div_elements_to_click_list = offers\n",
    "#             div_elements_to_click = div_elements_to_click_list[index]\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#             # driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "#             # Check if the index is within the valid range\n",
    "#             # div_offer = div_elements_to_click.find_element(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "#             # position_elem = div_offer.find_element(By.CSS_SELECTOR,\"h4\")\n",
    "#             # position = position_elem.text\n",
    "#             # workplace = position_elem.find_elements(By.XPATH, \"./following::*//span\")[1].text\n",
    "#             # contract_type_icon = div_offer.find_element(By.CSS_SELECTOR,\"i[name='contract']\")\n",
    "#             # main_contract_div = driver.execute_script(\"return arguments[0].parentNode;\", contract_type_icon)\n",
    "#             # contract_type = main_contract_div.find_element(By.CSS_SELECTOR,\"span\").text\n",
    "#             # published_date = div_offer.find_element(By.CSS_SELECTOR,\"time\").get_attribute(\"datetime\").split(\"T\")[0]\n",
    "#             # published_date = datetime.strptime(published_date, '%Y-%m-%d')\n",
    "#             # published_date = published_date.strftime('%d/%m/%Y')\n",
    "#             # driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_elements_to_click)\n",
    "#             # company = div_elements_to_click.find_element(By.CSS_SELECTOR, 'p.card-offer__company').text\n",
    "#             # contract_type = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[0].text\n",
    "#             # workplace = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[1].text\n",
    "#             # published_date = div_elements_to_click.find_elements(By.CSS_SELECTOR, 'ul.important-list > li')[2].text\n",
    "#             # Scroll into view\n",
    "#             # actions = ActionChains(driver)\n",
    "#             # actions.move_to_element(div_elements_to_click).click().perform()\n",
    "#             #On remote de deux niveau du bouton \"Voir offre\" pour pouvoir récupérer les informations appropriées\n",
    "#             # div_offer = driver.execute_script(\"return arguments[0].parentNode.parentNode;\", div_elements_to_click)\n",
    "#             # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#             # company = company_elem.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "#             # company_elem = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='companyName']\")\n",
    "#             # contract_type = div_offer.find_element(By.CSS_SELECTOR, \"span[data-cy='contract']\").text\n",
    "#             # workplace = div_offer.find_elements(By.CSS_SELECTOR, \"div[data-cy='loc']\")[-1].text\n",
    "#             # published_date = div_offer.find_elements(By.CSS_SELECTOR, \"span[data-cy='publishDate']\")[-1].text\n",
    "\n",
    "#             driver.execute_script(\"arguments[0].click();\", div_elements_to_click)\n",
    "\n",
    "#             WebDriverWait(driver, 3000).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class='media-body']\"))\n",
    "#             )\n",
    "            \n",
    "#             # workplace = driver.find_element(By.CSS_SELECTOR,\"span[id='lblCity']\").text\n",
    "#             # contract_type_elem = driver.find_element(By.CSS_SELECTOR,\"span[id='ltEmploymentType']\")\n",
    "#             # contract_type = contract_type_elem.find_element(By.CSS_SELECTOR,\"a\").text\n",
    "            \n",
    "#             # # driver.execute_script(\"arguments[0].click();\",description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "            \n",
    "#             # WebDriverWait(driver, 3)\n",
    "\n",
    "#             # try:\n",
    "#             #     profile_elem = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='job-section-experience']\")\n",
    "#             #     profile_html = profile_elem.find_elements(By.CSS_SELECTOR, \"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#             #     profile = BeautifulSoup(profile_html, 'html.parser').get_text()\n",
    "#             # except NoSuchElementException:\n",
    "#             #     # If profile element is not found, set it to \"NULL\"\n",
    "#             #     profile = \"NULL\"\n",
    "#             # driver.execute_script(\"arguments[0].click();\",profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-1].find_element(By.CSS_SELECTOR,\"span\"))\n",
    "\n",
    "#             # description = description_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "#             # profile = profile_elem.find_elements(By.CSS_SELECTOR,\"div\")[-2].get_attribute(\"innerHTML\")\n",
    "            \n",
    "#             #parsing tags\n",
    "#             # description = BeautifulSoup(description, 'html.parser').get_text()\n",
    "#             # company_elem = driver.find_element(By.CSS_SELECTOR,\"a[href*='/fr/companies/']\")\n",
    "#             # company = company_elem.find_element(By.CSS_SELECTOR,\"img\").get_attribute(\"alt\")\n",
    "#             # long_infos = \" \".join([description,profile])\n",
    "#             # print(long_infos)\n",
    "#             # Wait for the child element to become present in the DOM\n",
    "\n",
    "#             # # Get the page source after the click\n",
    "#             # page_source = driver.page_source\n",
    "\n",
    "#             # Use Beautiful Soup to parse the page source\n",
    "#             # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "#             # Example: Retrieve the text of a specific element\n",
    "#             # desc = soup.select(\"h4:contains('Descriptif du poste') + p\")\n",
    "#             # descText = [\"\".join(elem.text) for elem in desc][0]\n",
    "\n",
    "#             # profile = soup.select(\"h4:contains('Profil recherché') + p\")\n",
    "#             # profileText = [\"\".join(elem.text) for elem in profile][0]                  \n",
    "\n",
    "#             # position = soup.select_one(\"span[data-cy='jobTitle']\").text\n",
    "#             # position_type = soup.select_one('h4:contains(\"Secteur d’activité du poste\")+span').text\n",
    "#             # long_infos = soup.select(\"section\")\n",
    "#             # infos = [item.text.replace(\"\\n\",\" \").strip() for item in long_infos]\n",
    "#             # infos = [re.sub(r'\\s+', ' ',item) for item in infos]\n",
    "#             # infos = \" \".join([item for item in infos if item != ''])\n",
    "#             # infos = [item for item in infos if '' not in item]\n",
    "#             # corpus.append({\"source\":source,\"link\":rootLink,\"position\":position,\"position_type\":\"NULL\",\"company\":company,\"workplace\":workplace,\"published_date\":published_date,\"contract_type\":contract_type,\"description\":long_infos})\n",
    "#         except Exception as e:\n",
    "#             # Print the exception for debugging purposes\n",
    "#             print(f\"Error: {e}\")\n",
    "\n",
    "#         finally:\n",
    "#             # Navigate back to the main page\n",
    "#             driver.execute_script(\"window.history.go(-1);\")\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 404\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"fr\" xml:lang=\"fr\" class=\"no-js\">\n",
      "\t<head> \n",
      " <meta name=\"robots\" content=\"noindex,nofollow\"/>\n",
      " <meta name=\"pragma\" content=\"no-cache\"/>\n",
      "\t\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
      "\t\t<meta charset=\"utf-8\">\n",
      "\t\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "\t\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "\n",
      "\t\t<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->\n",
      "\t\t<meta name=\"description\" content=\"\">\n",
      "\t\t<meta name=\"author\" content=\"\">\n",
      "\t\t<link rel=\"icon\" href=\"favicon.ico\">\n",
      "\n",
      "\t\t<title>Erreur 404 | pole-emploi.fr, fusion des sites anpe.fr et assedic.fr</title>\n",
      "\t\t<link href=\"/css/pages-autonomes.css\" rel=\"stylesheet\">\n",
      "\t\t<!-- JS -->\n",
      "\t\t<script src=\"/js/jquery.min.js\"></script>\n",
      "\t\t<!--[if lt IE 9]>\n",
      "\t\t\t<script src=\"/js/ie8-svg-support.js\"></script>\n",
      "\t\t\t<script src=\"/js/html5shiv.min.js\"></script>\n",
      "\t\t\t<script src=\"/js/respond.min.js\"></script>\n",
      "\t\t<![endif]-->\n",
      "\t</head>\n",
      "\t<body class=\"candidat erreur\"> \n",
      "\t\t<div class=\"container small-container text-center\">\n",
      "\t\t\t<header role=\"banner\" class=\"text-center\">\t\n",
      "\t\t\t\t<a href=\"http://www.pole-emploi.fr\">\n",
      "\t\t\t\t\t<img src=\"/img/logo-pe.svg\" alt=\"Logo de PÃ´le emploi\" /><span class=\"sr-only\">Accueil PÃ´le emploi</span>\n",
      "\t\t\t\t</a>\n",
      "\t\t\t\t<h1 class=\"t2\">Page <span class=\"text-candidat\">introuvable</span></h1>\t\t\n",
      "\t\t\t</header>\n",
      "\t\t\t\n",
      "\t\t\t<hr aria-hidden=\"true\">\n",
      "\t\t\t<p class=\"t5\">La page demandÃ©e n'est malheureusement pas disponible</p>\n",
      "\t\t\t<a class=\"btn btn-primary btn-lg btn-block\" href=\"http://www.pole-emploi.fr\" role=\"button\">Retour</a>\t\n",
      "\t\t\t\t\n",
      "\t\t</div>\n",
      "\t\t<footer role=\"content-info\" class=\"footer center-block text-center\">\n",
      "\t\t\t<div class=\"container-fluid\">\n",
      "\t\t\t\t<small>&#169; 2016 POLE EMPLOI. Tous droits rÃ©servÃ©s.</small>\n",
      "\t\t\t</div>\n",
      "\t\t</footer>\n",
      "\t\t<!-- fin de la page -->\n",
      "\t</body>\t\n",
      "<script type=\"text/javascript\">\n",
      "xtnv = document;\n",
      "xtsd = \"http://logp6\";\n",
      "xtsite = \"475540\";\n",
      "xtn2 = \"10\";\n",
      "xtpage = \"api::Page_indisponible_(Erreur_404-candidat.html)\";\n",
      "xterr = \"\";\n",
      "xtmc = \"\";\n",
      "xtnp = \"\";\n",
      "xt_ac = \"\";\n",
      "xt_an = \"\";\n",
      "xtprm = \"\";\n",
      "roimt = \"\";\n",
      "roitest = false;\n",
      "visiteciblee = false;\n",
      "xtidmod = \"\";\n",
      "xtergo = \"0\";\n",
      "xt_multc = \"&amp;x1=Tapestry&amp;x2=Transverse&amp;x3=0\"; //all the xi indicators (like \"&x1=...&x2=....&x3=...\")\n",
      "</script>\n",
      "<script src=\"/js/xtclicks.js\" type=\"text/javascript\" ></script>\n",
      "<script src=\"/js/xtcore.js\" type=\"text/javascript\" ></script>\n",
      "<noscript>\n",
      "<img alt=\"\" height=\"1\" src=\"http://logp6.xiti.com/hit.xiti?s=475540&amp;s2=10&amp;p=api::Page_indisponible_(Erreur_404-candidat.html)&amp;roimt=&amp;roivc=&amp;x1=Tapestry&amp;x2=Transverse&amp;x3=0\" width=\"1\">\n",
      "</noscript>\n",
      "\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
